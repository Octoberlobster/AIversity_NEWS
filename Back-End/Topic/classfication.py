import os, json, re
from supabase import create_client
from dotenv import load_dotenv
from google import genai
from google.genai import types
from pydantic import BaseModel, Field
from typing import Optional
import random, hashlib
from pathlib import Path
from datetime import datetime

# ========================================
# ç³»çµ±åˆå§‹åŒ–èˆ‡è³‡æ–™ç²å–
# ========================================

def initialize_services():
    """åˆå§‹åŒ– Supabase å’Œ Gemini æœå‹™é€£æ¥"""
    load_dotenv()
    supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))
    gemini = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
    return supabase, gemini

def fetch_data_from_database(supabase):
    """å¾è³‡æ–™åº«ç²å–å°ˆé¡Œå’Œæ–°èè³‡æ–™"""
    topic = supabase.table("topic").select("topic_id, topic_title").eq("alive", 1).execute()
    print(topic.data)
    
    news = []
    batch_size = 1000
    start = 0
    while True:
        temp = supabase.table("single_news").select("story_id, news_title, short").order("generated_date", desc=True).range(start, start + batch_size - 1).execute()
        if not temp.data:
            break
        news.extend(temp.data)
        break
        start += batch_size
    return topic, news

def get_classified_news_ids(supabase) -> set[str]:
    """ç²å–å·²ç¶“åˆ†é¡çš„æ–°è ID é›†åˆ"""
    try:
        result = supabase.table("topic_news_map").select("story_id").execute()
        if result.data:
            classified_story_ids = {str(item["story_id"]) for item in result.data}
            print(f"æ‰¾åˆ° {len(classified_story_ids)} å€‹å·²åˆ†é¡çš„æ–°è")
            return classified_story_ids
        else:
            print("æ²’æœ‰æ‰¾åˆ°å·²åˆ†é¡çš„æ–°è")
            return set()
    except Exception as e:
        print(f"ç²å–å·²åˆ†é¡æ–°èå¤±æ•—: {e}")
        return set()

def filter_new_news_only(news_data, classified_story_ids: set[str]) -> list[dict]:
    """éæ¿¾å‡ºåªæœ‰æœªåˆ†é¡çš„æ–°è"""
    new_news = []
    already_classified = []
    
    for item in news_data:
        story_id = str(item["story_id"])
        news_title = item.get("news_title", "")
        
        if story_id in classified_story_ids:
            already_classified.append({"story_id": story_id, "news_title": news_title})
        else:
            new_news.append(item)
    
    print(f"\nğŸ“° æ–°èåˆ†æçµ±è¨ˆ:")
    print(f"   æœªåˆ†é¡æ–°è: {len(new_news)} ç¯‡")
    print(f"   å·²åˆ†é¡æ–°è: {len(already_classified)} ç¯‡")
    print(f"   ç¸½æ–°èæ•¸: {len(news_data)} ç¯‡")
    
    if not new_news:
        print("\nâš ï¸  æ‰€æœ‰æ–°èéƒ½å·²åˆ†é¡ï¼Œç„¡éœ€è™•ç†")
        return []
    
    return new_news

# ========================================
# è³‡æ–™æ¨¡å‹
# ========================================

class TopicBrief(BaseModel):
    short_description: str = Field(max_length=120)
    aliases: list[str] = Field(default_factory=list)
    keywords: list[str] = Field(default_factory=list, description="ä»£è¡¨æ­¤å°ˆé¡Œçš„5-8å€‹æ ¸å¿ƒé—œéµè©")
    positive_examples: list[str] = Field(default_factory=list, description="1-2å€‹æœ€èƒ½ä»£è¡¨æ­¤å°ˆé¡Œçš„ã€æ–°èæ¨™é¡Œã€ç¯„ä¾‹")
    negative_examples: list[str] = Field(default_factory=list, description="1-2å€‹å®¹æ˜“æ··æ·†ä½†ã€ä¸å±¬æ–¼ã€æ­¤å°ˆé¡Œçš„ã€æ–°èæ¨™é¡Œã€ç¯„ä¾‹")

class LMLabel(BaseModel):
    topic_id: Optional[str] = None
    topic_title: Optional[str] = None

# ========================================
# AI æç¤ºè©å»ºæ§‹
# ========================================
def build_topic_description_prompt(topic_title: str) -> str:
    """å»ºæ§‹å°ˆé¡Œæè¿°ç”Ÿæˆçš„ AI æç¤ºè©"""
    return f"""å°ˆé¡Œæ¨™é¡Œï¼š{topic_title}

ä»»å‹™ï¼šè«‹ç‚ºé€™å€‹å°ˆé¡Œç”Ÿæˆä¸€ä»½è©³ç´°çš„ JSON æ ¼å¼æª”æ¡ˆï¼Œä»¥åˆ©å¾ŒçºŒçš„AIæ¨¡å‹èƒ½ç²¾æº–åœ°å°‡æ–°èæ­¸é¡åˆ°æ­¤å°ˆé¡Œã€‚

è¦å‰‡ï¼š
- å…è¨±ä¸Šç¶²æª¢ç´¢è¿‘ 3â€“5 å¹´å…¬é–‹è³‡æ–™ä»¥é‡æ¸…å®šç¾©èˆ‡å¸¸è¦‹ç¯„åœã€‚
- **åªè¼¸å‡º JSON**ï¼Œä¸å¾—æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚JSON æ¬„ä½èˆ‡èªªæ˜å¦‚ä¸‹ï¼š
  - short_description (å­—ä¸²): æ§åˆ¶åœ¨ 100â€“110 å­—ï¼ˆæœ€å¤š 120 å­—ï¼‰ï¼Œ1â€“2 å¥ï¼Œèªªæ˜æœ¬å°ˆé¡Œçš„ä¸»é¡Œé‚Šç•Œèˆ‡å¸¸è¦‹ç¯„åœã€‚
  - aliases (å­—ä¸²é™£åˆ—): å¸¸è¦‹åˆ¥åæˆ–ç¸®å¯«ã€‚
  - keywords (å­—ä¸²é™£åˆ—): æä¾› 5-8 å€‹æœ€èƒ½ä»£è¡¨æ­¤å°ˆé¡Œçš„æ ¸å¿ƒé—œéµè©ã€‚
  - positive_examples (å­—ä¸²é™£åˆ—): æä¾› 1-2 å€‹ã€Œçµ•å°å±¬æ–¼ã€æ­¤å°ˆé¡Œçš„ã€æ¨¡æ“¬æ–°èæ¨™é¡Œã€ç¯„ä¾‹ã€‚
  - negative_examples (å­—ä¸²é™£åˆ—): æä¾› 1-2 å€‹ã€Œå®¹æ˜“æ··æ·†ï¼Œä½†**ä¸å±¬æ–¼**ã€æ­¤å°ˆé¡Œçš„ã€æ¨¡æ“¬æ–°èæ¨™é¡Œã€ç¯„ä¾‹ï¼Œä»¥å¹«åŠ© AI é‡æ¸…é‚Šç•Œã€‚
- é¿å…å¯«å…¥å–®ä¸€äº‹ä»¶æˆ–å–®ä¸€å…¬å¸åç¨±ï¼Œé™¤éè©²åç¨±å·²æˆç‚ºä¸»é¡Œçš„ä»£åè©ã€‚
- è‹¥åˆç¨¿è¶…é•·ï¼Œè«‹è‡ªè¡Œåˆªæ¸›è‡³ç¬¦åˆå­—æ•¸é™åˆ¶ï¼›æ‰€æœ‰æ¬„ä½ä¸å¾—ç‚ºç©ºå€¼ï¼Œè‹¥ç„¡å…§å®¹å‰‡å›å‚³ç©ºé™£åˆ— `[]`ã€‚
"""

def build_classification_prompt(story_title: str, story_short: str, topics_payload: list[dict]) -> str:
    """å»ºæ§‹æ–°èåˆ†é¡çš„ AI æç¤ºè©"""
    NEWS_MAX_CHARS = 1200
    
    article = (story_title or "").strip()
    if story_short:
        article += "\n\n" + (story_short or "").strip()
    article = article[:NEWS_MAX_CHARS]

    topics_json = json.dumps(topics_payload, ensure_ascii=False)

    return (
        "ä»»å‹™ï¼šæ ¹æ“šä¸‹æ–¹æä¾›çš„ã€å€™é¸å°ˆé¡Œæ¸…å–®ã€èˆ‡ã€æœ¬æ–‡ã€ï¼Œè«‹åœ¨å€™é¸æ¸…å–®ä¸­é¸å‡ºæœ€åˆé©çš„ä¸€å€‹å°ˆé¡Œï¼›è‹¥å…¨éƒ¨ä¸åˆé©ï¼Œè«‹å› nullã€‚\n"
        "è¼¸å‡ºï¼šåƒ…è¼¸å‡º JSONï¼Œä¸”**å¿…é ˆå®Œå…¨ç¬¦åˆ**æ­¤çµæ§‹ï¼ˆä¸å¯æœ‰å¤šé¤˜æ¬„ä½æˆ–æ–‡å­—ï¼‰ï¼š\n"
        '{\"topic_id\": <UUIDæˆ–null>, \"topic_title\": <å­—ä¸²æˆ–null>}\n\n'
        "è¦å‰‡ï¼š\n"
        "1) **è«‹ä»”ç´°è©•ä¼°æ¯å€‹å°ˆé¡Œçš„æè¿°(desc)ã€é—œéµè©(keywords)èˆ‡åˆ†é¡æŒ‡å°(guidelines)ã€‚**\n"
        "2) **`guidelines` ä¸­çš„ `includes_examples` æ˜¯æ­£é¢ç¯„ä¾‹ï¼Œ`excludes_examples` æ˜¯åé¢ç¯„ä¾‹ï¼Œé€™å°æ–¼é‡æ¸…å°ˆé¡Œé‚Šç•Œè‡³é—œé‡è¦ã€‚**\n"
        "3) åªèƒ½å¾å€™é¸æ¸…å–®ä¸­é¸ï¼›ä¸å¯ç™¼æ˜æ¸…å–®å¤–çš„æ¨™ç±¤ã€‚\n"
        "4) è‹¥çš†ä¸åˆé©ï¼štopic_id = nullã€topic_title = nullã€‚\n"
        "5) topic_title è«‹å°æ‡‰æ‰€é¸ topic_id çš„ titleã€‚\n\n"
        f"ã€å€™é¸å°ˆé¡Œæ¸…å–®ï¼ˆJSON é™£åˆ—ï¼‰ã€‘\n{topics_json}\n\n"
        f"ã€æœ¬æ–‡ã€‘\næ¨™é¡Œï¼š{story_title}\nå…§å®¹æ‘˜éŒ„ï¼š\n{article}\n"
    )

# ========================================
# æ–‡å­—è™•ç†å·¥å…·å‡½æ•¸
# ========================================
def clamp_description(s: str, min_len: int = 100, max_len: int = 110, hard_max: int = 120) -> str:
    """é™åˆ¶æè¿°é•·åº¦ä¸¦å„ªåŒ–æˆªæ–·ä½ç½®"""
    s = re.sub(r"\s+", " ", (s or "").strip())
    if len(s) <= max_len:
        return s
    cut = s[:max_len]
    punct = "ï¼Œ,ã€;ï¼›ã€‚.!?ï¼Ÿ)]ã€ã€ã€‘ï¼‰"
    tail = cut[-15:]
    idx = max(tail.rfind(ch) for ch in punct)
    if idx >= 0 and (len(cut) - (15 - idx)) >= min_len:
        cut = cut[: len(cut) - (15 - idx)]
    out = cut.rstrip()
    if len(s) > len(out):
        out += "â€¦"
    return out[:hard_max]

def extract_json_candidate(text: str) -> Optional[str]:
    """å¾æ–‡å­—ä¸­æå– JSON å€™é¸å­—ä¸²"""
    if not text:
        return None
    s = text.strip()

    fence = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", s, flags=re.S)
    if fence:
        return fence.group(1)

    if s.startswith("```"):
        s = re.sub(r"^```(?:json)?\s*", "", s)
        s = re.sub(r"\s*```$", "", s)
        s = s.strip()

    try:
        json.loads(s)
        return s
    except Exception:
        pass

    start = s.find("{")
    if start == -1:
        return None
    depth = 0
    for i, ch in enumerate(s[start:], start=start):
        if ch == "{":
            depth += 1
        elif ch == "}":
            depth -= 1
            if depth == 0:
                candidate = s[start:i+1]
                try:
                    json.loads(candidate)
                    return candidate
                except Exception:
                    continue
    return None

# ========================================
# AI å›æ‡‰è™•ç†èˆ‡å°ˆé¡Œæè¿°ç”Ÿæˆ
# ========================================
def repair_response_to_json(gemini_client, bad_text: str) -> TopicBrief:
    """ä¿®å¾©æ ¼å¼éŒ¯èª¤çš„å›æ‡‰ç‚ºåˆæ³• JSON"""
    resp2 = gemini_client.models.generate_content(
        model="gemini-2.5-flash-lite",
        contents=f"è«‹å°‡ä¸‹åˆ—å…§å®¹åªè½‰ç‚ºåˆæ³• JSONï¼ˆæ¬„ä½ï¼šshort_description, aliasesï¼‰ã€‚ä¸å¾—åŠ å…¥ä»»ä½•å¤šé¤˜æ–‡å­—ï¼š\n{bad_text}",
        config=types.GenerateContentConfig(
            system_instruction="ä½ æ˜¯æ ¼å¼åŒ–å·¥å…·ï¼Œåªè¼¸å‡ºåˆæ³• JSONã€‚",
            response_mime_type="application/json",
            response_schema=TopicBrief,
            temperature=0,
        ),
    )
    fixed: TopicBrief = resp2.parsed
    fixed.short_description = clamp_description(fixed.short_description, 100, 110, 120)
    return fixed

def safe_label_from_response(resp) -> LMLabel:
    """å®‰å…¨åœ°å¾ AI å›æ‡‰ä¸­è§£ææ¨™ç±¤ï¼Œç¢ºä¿æ°¸é å›å‚³ LMLabel"""
    # é è¨­
    label = LMLabel(topic_id=None, topic_title=None)

    # 1) å…ˆè©¦ parsed
    parsed = getattr(resp, "parsed", None)
    if parsed is not None:
        if isinstance(parsed, LMLabel):
            return parsed
        try:
            return LMLabel.model_validate(parsed)
        except Exception:
            pass

    # 2) é€€å› text -> JSON
    txt = (getattr(resp, "text", "") or "").strip()
    if txt:
        cand = extract_json_candidate(txt) or txt
        try:
            data = json.loads(cand)
            return LMLabel.model_validate(data)
        except Exception:
            pass

    # 3) å…¨å¤±æ•— â†’ é è¨­
    return label

# ========= å°ˆé¡Œæè¿°ç”Ÿæˆå‡½æ•¸ =========
def generate_topic_description(gemini_client, topic_title: str) -> TopicBrief:
    """ç‚ºå–®å€‹å°ˆé¡Œç”Ÿæˆ AI æè¿°"""
    # ç¬¬ä¸€æ¬¡ï¼šé–‹å·¥å…·ï¼ˆå…è¨±ä¸Šç¶²ï¼‰ï¼Œä½†ä¸é– JSON MIME
    resp = gemini_client.models.generate_content(
        model="gemini-2.5-flash-lite",
        contents=build_topic_description_prompt(topic_title),
        config=types.GenerateContentConfig(
            tools=[types.Tool(google_search=types.GoogleSearch())],
            system_instruction="ä½ æ˜¯æ–°èå°ˆé¡Œæè¿°åŠ©ç†ã€‚è¼¸å‡ºç¹é«”ä¸­æ–‡ã€å®¢è§€ã€ç²¾ç°¡ï¼Œä¾›å‰ç«¯ tooltip ä½¿ç”¨ã€‚",
            temperature=0.2,
        )
    )

    brief: Optional[TopicBrief] = None
    raw = (resp.text or "").strip()

    cand = extract_json_candidate(raw)
    if cand:
        try:
            data = json.loads(cand)
            desc = clamp_description(data.get("short_description", ""), 100, 110, 120)
            aliases = data.get("aliases", []) or []
            brief = TopicBrief(short_description=desc, aliases=aliases)
        except Exception:
            brief = None

    if brief is None:
        try:
            brief = repair_response_to_json(gemini_client, raw if raw else f'{{"short_description": "æœ¬å°ˆé¡Œï¼š{topic_title}", "aliases": []}}')
        except Exception:
            brief = None

    if brief is None:
        fallback = f"æœ¬å°ˆé¡Œèšç„¦ã€Œ{topic_title}ã€ï¼Œå½™æ•´è¿‘å¹´ç›¸é—œæ”¿ç­–ã€äº‹ä»¶ã€ç”¢æ¥­èˆ‡ç¤¾æœƒå½±éŸ¿ï¼Œæä¾›è„ˆçµ¡è¦é»èˆ‡è¶¨å‹¢è§€å¯Ÿï¼Œå”åŠ©è®€è€…å¿«é€ŸæŒæ¡é‡é»èˆ‡å»¶ä¼¸è¨è«–ã€‚"
        brief = TopicBrief(short_description=clamp_description(fallback, 100, 110, 120), aliases=[])

    return brief

def build_topic_profiles(gemini_client, topics_to_process) -> dict[str, dict]:
    """å»ºç«‹å°ˆé¡Œçš„ AI æè¿°æª”æ¡ˆ"""
    topic_profiles: dict[str, dict] = {}
    
    if not topics_to_process:
        print("æ²’æœ‰éœ€è¦è™•ç†çš„å°ˆé¡Œï¼Œè·³éå»ºç«‹æè¿°æª”æ¡ˆ")
        return topic_profiles
    
    print(f"é–‹å§‹ç‚º {len(topics_to_process)} å€‹å°ˆé¡Œå»ºç«‹æè¿°æª”æ¡ˆ...")
    
    for item in topics_to_process:
        topic_title = item["topic_title"]
        brief = generate_topic_description(gemini_client, topic_title)
        
        print(f"[{topic_title}] {brief.short_description} (len={len(brief.short_description)}) | aliases={brief.aliases}")
        
        tid = str(item["topic_id"])
        topic_profiles[tid] = {
            "title": topic_title,
            "desc": brief.short_description,
            "aliases": brief.aliases,
            "keywords": brief.keywords,
            "positive_examples": brief.positive_examples,
            "negative_examples": brief.negative_examples,
        }
    
    return topic_profiles

# ========================================
# å°ˆé¡Œè³‡æ–™æº–å‚™
# ========================================
def build_topics_payload(topic_profiles: dict[str, dict], max_aliases: int = 6) -> list[dict]:
    """å»ºç«‹å°ˆé¡Œå€™é¸æ¸…å–®"""
    items = []
    for tid, prof in topic_profiles.items():
        aliases = [a.strip() for a in (prof.get("aliases") or []) if a and a.strip()]
        aliases = aliases[:max_aliases]

        guidelines = {
            "includes_examples": prof.get("positive_examples", []),
            "excludes_examples": prof.get("negative_examples", [])
        }

        items.append({
            "topic_id": tid,
            "title": prof.get("title", ""),
            "desc": prof.get("desc", ""),
            "aliases": aliases,
            "keywords": prof.get("keywords", []),
            "guidelines": guidelines,
        })
    return items

def shuffle_topics_for_story(topics_payload: list[dict], story_id: str) -> list[dict]:
    """ç‚ºç‰¹å®šæ–°èç”¢ç”Ÿå¯é‡ç¾çš„éš¨æ©Ÿé †åºå°ˆé¡Œåˆ—è¡¨"""
    seed = int(hashlib.md5(story_id.encode("utf-8")).hexdigest(), 16) % (10**8)
    rnd = random.Random(seed)
    copied = topics_payload[:]
    rnd.shuffle(copied)
    return copied

# ========================================
# æ–°èåˆ†é¡è™•ç†
# ========================================
def classify_single_news(gemini_client, story_id: str, story_title: str, story_short: str, topics_payload: list[dict], topic_profiles: dict) -> dict:
    """åˆ†é¡å–®å‰‡æ–°è"""
    SKIP_NONE = False
    
    if not (story_title or story_short):
        if not SKIP_NONE:
            return {
                "topic_id": None,
                "topic_title": None,
                "source_story": {"story_id": story_id, "news_title": story_title, "short": story_short},
            }
        print(f"[story_id={story_id}]ï¼ˆç©ºæ–‡æœ¬ï¼‰ â†’ topic_id=NONE")
        return None

    candidates = shuffle_topics_for_story(topics_payload, story_id)
    prompt = build_classification_prompt(story_title, story_short, candidates)

    # ä¸é–‹å·¥å…·ï¼Œæ‰èƒ½å®‰å…¨ä½¿ç”¨ JSON schema
    try:
        resp = gemini_client.models.generate_content(
            model="gemini-2.5-flash-lite",
            contents=prompt,
            config=types.GenerateContentConfig(
                system_instruction="ä½ æ˜¯æ–°èæ­¸é¡åŠ©ç†ã€‚åƒ…ä¾ç…§æŒ‡ç¤ºè¼¸å‡º JSONï¼ˆtopic_id, topic_titleï¼‰ã€‚",
                response_mime_type="application/json",
                response_schema=LMLabel,
                temperature=0.1,
            )
        )
        label = safe_label_from_response(resp)
    except Exception as e:
        label = LMLabel(topic_id=None, topic_title=None)
        print(f"[story_id={story_id}] LLM å‘¼å«å¤±æ•—ï¼š{e} â†’ topic_id=NONE")

    # ä»¥ topic_id ç‚ºæº–ï¼Œé¿å…æ¨¡å‹æŠŠ title æ‰“éŒ¯ï¼›ç”¨ä½ çš„ canonical title è¦†å¯«
    tid = label.topic_id if (label.topic_id and label.topic_id in topic_profiles) else None
    ttitle = topic_profiles[tid]["title"] if tid else None

    result_obj = {
        "topic_id": tid,
        "topic_title": ttitle,
        "source_story": {
            "story_id": story_id,
            "news_title": story_title,
            "short": story_short,
        }
    }

    shown = tid if tid else "NONE"
    print(f"[story_id={story_id}] {story_title}\n  â†’ topic_id={shown}")
    
    return result_obj

def classify_all_news(gemini_client, news_data, topics_payload: list[dict], topic_profiles: dict) -> list[dict]:
    """åˆ†é¡æ‰€æœ‰æ–°è"""
    classified_results: list[dict] = []
    SKIP_NONE = False

    for doc in news_data.data:
        sid = str(doc["story_id"])
        s_title = (doc.get("news_title") or "").strip()
        s_short = (doc.get("short") or "").strip()
        
        result = classify_single_news(gemini_client, sid, s_title, s_short, topics_payload, topic_profiles)
        
        if result is not None:
            if not ((result.get("topic_id") is None) and SKIP_NONE):
                classified_results.append(result)

    return classified_results

# ========================================
# è³‡æ–™åº«æ“ä½œ
# ========================================
def append_topic_news_mappings(supabase, topic_id: str, story_ids: list[str]) -> bool:
    """å°‡æ–°çš„å°ˆé¡Œå’Œæ–°èæ˜ å°„é—œä¿‚è¿½åŠ åˆ°è³‡æ–™åº«ï¼ˆå¢é‡æ¨¡å¼ï¼‰"""
    try:
        # å…ˆæª¢æŸ¥å“ªäº› story_id å·²ç¶“å­˜åœ¨
        existing_result = supabase.table("topic_news_map").select("story_id").eq("topic_id", topic_id).execute()
        existing_story_ids = {str(item["story_id"]) for item in existing_result.data} if existing_result.data else set()
        
        # éæ¿¾å‡ºçœŸæ­£éœ€è¦æ–°å¢çš„ story_id
        new_story_ids = [sid for sid in story_ids if sid not in existing_story_ids]
        
        if not new_story_ids:
            print(f"å°ˆé¡Œ {topic_id} æ²’æœ‰éœ€è¦æ–°å¢çš„æ–°èæ˜ å°„")
            return True
        
        # æº–å‚™è¦æ’å…¥çš„è³‡æ–™
        mappings = [
            {
                "topic_id": topic_id,
                "story_id": story_id
            }
            for story_id in new_story_ids
        ]
        
        # æ‰¹é‡æ’å…¥
        result = supabase.table("topic_news_map").insert(mappings).execute()
        
        if result.data:
            print(f"æˆåŠŸç‚ºå°ˆé¡Œ {topic_id} æ–°å¢ {len(new_story_ids)} ç­†æ–°èæ˜ å°„")
            return True
        else:
            print(f"ç‚ºå°ˆé¡Œ {topic_id} æ–°å¢æ˜ å°„å¤±æ•—ï¼šç„¡è³‡æ–™è¿”å›")
            return False
            
    except Exception as e:
        print(f"ç‚ºå°ˆé¡Œ {topic_id} æ–°å¢æ˜ å°„å¤±æ•—: {e}")
        return False

def save_incremental_results_to_database(supabase, grouped_output: dict) -> dict:
    """å°‡åˆ†é¡çµæœå¢é‡å­˜å…¥è³‡æ–™åº«ï¼ˆä¸æ¸…é™¤ç¾æœ‰è³‡æ–™ï¼Œåªæ–°å¢ï¼‰"""
    saved_topics = []
    failed_topics = []
    
    print(f"\né–‹å§‹å¢é‡å­˜å…¥è³‡æ–™åº«...")
    
    for topic in grouped_output["topics"]:
        topic_id = topic["topic_id"]
        topic_title = topic["topic_title"]
        stories = topic["stories"]
        news_count = len(stories)
        
        # å¦‚æœé€™å€‹å°ˆé¡Œæ²’æœ‰æ–°æ–°èï¼Œè·³é
        if news_count == 0:
            continue
        
        # æº–å‚™æ–°è ID åˆ—è¡¨
        story_ids = [story["story_id"] for story in stories]
        
        # å¢é‡æ’å…¥æ–°çš„æ˜ å°„ï¼ˆä¸æ¸…é™¤ç¾æœ‰çš„ï¼‰
        if append_topic_news_mappings(supabase, topic_id, story_ids):
            saved_topics.append({
                "topic_id": topic_id,
                "topic_title": topic_title,
                "new_news_count": news_count
            })
            print(f"âœ… æˆåŠŸç‚ºå°ˆé¡Œ '{topic_title}' æ–°å¢: {news_count} ç¯‡æ–°è")
        else:
            failed_topics.append({
                "topic_id": topic_id,
                "topic_title": topic_title,
                "new_news_count": news_count,
                "reason": "æ–°å¢æ˜ å°„å¤±æ•—"
            })
            print(f"âŒ ç‚ºå°ˆé¡Œ '{topic_title}' æ–°å¢æ–°èå¤±æ•—")
    
    # çµ±è¨ˆçµæœ
    summary = {
        "saved_count": len(saved_topics),
        "failed_count": len(failed_topics),
        "total_topics": len(grouped_output["topics"]),
        "saved_topics": saved_topics,
        "failed_topics": failed_topics,
        "mode": "incremental"
    }
    
    print(f"\nğŸ“Š å¢é‡è³‡æ–™åº«å­˜å…¥æ‘˜è¦:")
    print(f"   æˆåŠŸæ–°å¢: {summary['saved_count']} å€‹å°ˆé¡Œçš„æ–°è")
    print(f"   å¤±æ•—: {summary['failed_count']} å€‹å°ˆé¡Œ")
    print(f"   è™•ç†çš„å°ˆé¡Œæ•¸: {summary['total_topics']} å€‹å°ˆé¡Œ")
    
    return summary

# ========================================
# çµæœè™•ç†èˆ‡æª”æ¡ˆè¼¸å‡º
# ========================================
def group_results_by_topic(classified_results: list[dict], topic_profiles: dict) -> dict:
    """ä¾å°ˆé¡Œåˆ†çµ„çµæœ"""
    groups_map: dict[str, dict] = {
        tid: {"topic_id": tid, "topic_title": prof["title"], "stories": []}
        for tid, prof in topic_profiles.items()
    }
    unassigned: list[dict] = []

    for rec in classified_results:
        tid = rec["topic_id"]
        story = rec["source_story"]
        if tid is None:
            unassigned.append(story)
        else:
            bucket = groups_map.setdefault(
                tid,
                {"topic_id": tid, "topic_title": rec.get("topic_title") or topic_profiles.get(tid, {}).get("title"), "stories": []}
            )
            bucket["stories"].append(story)

    grouped_output = {
        "topics": list(groups_map.values()),
        "unassigned": unassigned
    }
    
    return grouped_output

def save_results_to_file(grouped_output: dict) -> Path:
    """å„²å­˜çµæœåˆ°æª”æ¡ˆ"""
    out_dir = Path("out")
    out_dir.mkdir(exist_ok=True)
    out_path = out_dir / f"classified_grouped_{datetime.now():%Y%m%d_%H%M%S}.json"
    
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(grouped_output, f, ensure_ascii=False, indent=2)
    
    return out_path

def print_summary(grouped_output: dict, out_path: Path):
    """å°å‡ºè™•ç†æ‘˜è¦"""
    topics_with_stories = sum(1 for g in grouped_output["topics"] if g["stories"])
    print(f"\nâœ… å·²è¼¸å‡ºåˆ†çµ„çµæœï¼šå…± {len(grouped_output['topics'])} å€‹ä¸»é¡Œï¼ˆå…¶ä¸­ {topics_with_stories} å€‹æœ‰åˆ†åˆ°æ–‡ç« ï¼‰ï¼Œæœªåˆ†é¡ {len(grouped_output['unassigned'])} å‰‡ â†’ {out_path}")

# ========================================
# ä¸»æµç¨‹
# ========================================
def main():
    """ä¸»æµç¨‹ï¼šæ‹‰å–æ‰€æœ‰å°ˆé¡Œï¼Œåªåˆ†é¡æœªåˆ†é¡çš„æ–°è"""
    print("ğŸš€ å•Ÿå‹•æ–°èåˆ†é¡ç³»çµ±")
    
    # åˆå§‹åŒ–æœå‹™
    supabase, gemini = initialize_services()
    
    # ç²å–è³‡æ–™
    topic_data, news_data = fetch_data_from_database(supabase)
    
    # æ‰¾å‡ºå·²åˆ†é¡çš„æ–°è
    classified_story_ids = get_classified_news_ids(supabase)
    
    # éæ¿¾å‡ºæœªåˆ†é¡çš„æ–°è
    unclassified_news = filter_new_news_only(news_data, classified_story_ids)
    
    # å¦‚æœæ²’æœ‰æœªåˆ†é¡çš„æ–°èï¼Œç›´æ¥è¿”å›
    if not unclassified_news:
        print("\nğŸ¯ æ‰€æœ‰æ–°èéƒ½å·²åˆ†é¡ï¼Œç„¡éœ€åŸ·è¡Œåˆ†é¡")
        return {
            "message": "æ‰€æœ‰æ–°èéƒ½å·²åˆ†é¡ï¼Œç„¡éœ€è™•ç†",
            "total_news": len(news_data.data),
            "unclassified_news": 0,
            "total_topics": len(topic_data.data)
        }
    
    # ç‚ºæ‰€æœ‰å°ˆé¡Œå»ºç«‹æè¿°æª”æ¡ˆï¼ˆå› ç‚ºæœªåˆ†é¡çš„æ–°èå¯èƒ½åˆ†åˆ°ä»»ä½•å°ˆé¡Œï¼‰
    print(f"\né–‹å§‹ç‚ºæ‰€æœ‰ {len(topic_data.data)} å€‹å°ˆé¡Œå»ºç«‹æè¿°æª”æ¡ˆ...")
    topic_profiles = build_topic_profiles(gemini, topic_data.data)
    
    # æº–å‚™å°ˆé¡Œå€™é¸æ¸…å–®
    print("æº–å‚™å°ˆé¡Œå€™é¸æ¸…å–®...")
    topics_payload = build_topics_payload(topic_profiles, max_aliases=6)
    
    # åˆ†é¡æœªåˆ†é¡çš„æ–°è
    print(f"é–‹å§‹åˆ†é¡ {len(unclassified_news)} ç¯‡æœªåˆ†é¡æ–°è...")
    fake_news_data = type('obj', (object,), {'data': unclassified_news})
    classified_results = classify_all_news(gemini, fake_news_data, topics_payload, topic_profiles)
    
    # åˆ†çµ„çµæœ
    print("åˆ†çµ„çµæœ...")
    grouped_output = group_results_by_topic(classified_results, topic_profiles)
    
    # å„²å­˜çµæœåˆ°æª”æ¡ˆ
    out_path = save_results_to_file(grouped_output)
    print_summary(grouped_output, out_path)
    
    # å¢é‡å­˜å…¥è³‡æ–™åº«ï¼ˆä¸æ¸…é™¤ç¾æœ‰åˆ†é¡ï¼Œåªæ–°å¢ï¼‰
    db_summary = save_incremental_results_to_database(supabase, grouped_output)
    
    # æº–å‚™åŸ·è¡Œæ‘˜è¦
    execution_summary = {
        "total_topics": len(topic_data.data),
        "total_news": len(news_data),
        "unclassified_news": len(unclassified_news),
        "newly_classified": sum(len(t["stories"]) for t in grouped_output["topics"]),
        "topics_with_new_news": sum(1 for g in grouped_output["topics"] if g["stories"]),
        "unassigned_news": len(grouped_output["unassigned"]),
        "database_saved": db_summary["saved_count"],
        "database_failed": db_summary["failed_count"]
    }
    
    # æ•´åˆæ‰€æœ‰çµæœä¸¦ä¿å­˜ç‚º JSON æª”æ¡ˆ
    complete_results = {
        "timestamp": datetime.now().isoformat(),
        "execution_summary": execution_summary,
        "grouped_output": grouped_output,
        "file_path": str(out_path),
        "database_summary": db_summary
    }
    
    # ä¿å­˜å®Œæ•´çµæœåˆ° JSON æª”æ¡ˆ
    out_dir = Path("out")
    out_dir.mkdir(exist_ok=True)
    complete_results_path = out_dir / f"complete_results_{datetime.now():%Y%m%d_%H%M%S}.json"
    
    try:
        with open(complete_results_path, "w", encoding="utf-8") as f:
            json.dump(complete_results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å®Œæ•´çµæœå·²ä¿å­˜è‡³: {complete_results_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å®Œæ•´çµæœå¤±æ•—: {e}")
    
    return complete_results

# ========================================
# åŸ·è¡Œé»
# ========================================
if __name__ == "__main__":
    main()
