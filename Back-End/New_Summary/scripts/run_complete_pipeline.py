"""
å®Œæ•´æ–°èè™•ç†æµæ°´ç·š
å°‡è³‡æ–™è™•ç†å’Œå ±å°ç”Ÿæˆä¸²è¯åŸ·è¡Œï¼Œç›´æ¥ç”¢ç”Ÿæœ€çµ‚çµæœ
"""

import os
import sys
import json
from datetime import datetime
import logging

# ç¢ºä¿è¼‰å…¥ .env æª”æ¡ˆ
try:
    from dotenv import load_dotenv
    load_dotenv(os.path.join(os.path.dirname(__file__), "../.env"))
except ImportError:
    pass

# æ·»åŠ çˆ¶ç›®éŒ„åˆ° Python è·¯å¾‘ï¼Œä»¥ä¾¿å¼•ç”¨ core æ¨¡çµ„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.news_processor import NewsProcessor
from core.config import NewsProcessorConfig
from core.report_generator import ReportGenerator
from core.report_config import ReportGeneratorConfig

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('complete_pipeline.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CompletePipeline:
    """å®Œæ•´çš„æ–°èè™•ç†æµæ°´ç·š"""
    
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–æµæ°´ç·š"""
        self.api_key = api_key or NewsProcessorConfig.get_gemini_api_key()
        if not self.api_key:
            raise ValueError("æœªè¨­å®š GEMINI_API_KEY")
        
        logger.info("ğŸš€ åˆå§‹åŒ–å®Œæ•´æ–°èè™•ç†æµæ°´ç·š")
        
    def run_complete_pipeline(self, 
                            input_file: str = "cleaned_final_news.json",
                            output_prefix: str = "final_reports",
                            process_all: bool = True,
                            start_index: int = 0,
                            max_stories: int = None):
        """
        åŸ·è¡Œå®Œæ•´æµæ°´ç·š
        
        Args:
            input_file: åŸå§‹æ–°èæª”æ¡ˆ
            output_prefix: è¼¸å‡ºæª”æ¡ˆå‰ç¶´
            process_all: æ˜¯å¦è™•ç†æ‰€æœ‰ stories
            start_index: é–‹å§‹ç´¢å¼•
            max_stories: æœ€å¤§è™•ç†æ•¸é‡
        """
        
        start_time = datetime.now()
        logger.info(f"â° æµæ°´ç·šé–‹å§‹æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ–°èè³‡æ–™è™•ç†
            logger.info("\n" + "="*60)
            logger.info("ğŸ”„ ç¬¬ä¸€æ­¥ï¼šåŸ·è¡Œæ–°èè³‡æ–™è™•ç†")
            logger.info("="*60)
            
            processed_file = self._run_news_processing(
                input_file=input_file,
                start_index=start_index,
                max_stories=max_stories
            )
            
            if not processed_file:
                logger.error("âŒ æ–°èè™•ç†å¤±æ•—ï¼Œæµæ°´ç·šçµ‚æ­¢")
                return None
            
            logger.info(f"âœ… æ–°èè™•ç†å®Œæˆï¼š{processed_file}")
            
            # ç¬¬äºŒæ­¥ï¼šå ±å°ç”Ÿæˆ
            logger.info("\n" + "="*60)
            logger.info("ğŸ“ ç¬¬äºŒæ­¥ï¼šåŸ·è¡Œå ±å°ç”Ÿæˆ")
            logger.info("="*60)
            
            final_reports_file = self._run_report_generation(
                processed_file=processed_file,
                output_prefix=output_prefix
            )
            
            if not final_reports_file:
                logger.error("âŒ å ±å°ç”Ÿæˆå¤±æ•—ï¼Œæµæ°´ç·šçµ‚æ­¢")
                return None
            
            logger.info(f"âœ… å ±å°ç”Ÿæˆå®Œæˆï¼š{final_reports_file}")
            
            # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæœ€çµ‚æ‘˜è¦
            self._generate_final_summary(processed_file, final_reports_file, start_time)
            
            return final_reports_file
            
        except Exception as e:
            logger.error(f"âŒ æµæ°´ç·šåŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return None
    
    def _run_news_processing(self, input_file: str, start_index: int, max_stories: int):
        """åŸ·è¡Œæ–°èè³‡æ–™è™•ç†"""
        try:
            # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
            if not os.path.exists(input_file):
                logger.error(f"âŒ è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨ï¼š{input_file}")
                return None
            
            # åˆå§‹åŒ–æ–°èè™•ç†å™¨
            processor = NewsProcessor(
                api_key=self.api_key, 
                model_name=NewsProcessorConfig.GEMINI_MODEL
            )
            
            # ç”Ÿæˆè¼¸å‡ºæª”å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"../outputs/processed/processed_articles_{timestamp}.json"
            
            # åŸ·è¡Œè™•ç†
            processor.process_all_stories(
                input_file=input_file,
                output_file=output_file,
                start_index=start_index,
                max_stories=max_stories
            )
            
            # æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆæ˜¯å¦ç”Ÿæˆ
            if os.path.exists(output_file):
                return output_file
            else:
                logger.error("âŒ æ–°èè™•ç†è¼¸å‡ºæª”æ¡ˆæœªç”Ÿæˆ")
                return None
                
        except Exception as e:
            logger.error(f"âŒ æ–°èè™•ç†å¤±æ•—ï¼š{e}")
            return None
    
    def _run_report_generation(self, processed_file: str, output_prefix: str):
        """åŸ·è¡Œå ±å°ç”Ÿæˆ"""
        try:
            # åˆå§‹åŒ–å ±å°ç”Ÿæˆå™¨
            generator = ReportGenerator(
                api_key=self.api_key,
                model_name=ReportGeneratorConfig.GEMINI_MODEL
            )
            
            # ç”Ÿæˆè¼¸å‡ºæª”å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"../outputs/reports/{output_prefix}_{timestamp}.json"
            
            # åŸ·è¡Œå ±å°ç”Ÿæˆï¼ˆåªç”Ÿæˆç¶œåˆå ±å°ï¼‰
            generator.generate_reports_for_all_stories(
                input_file=processed_file,
                output_file=output_file,
                generate_individual=False  # åªç”Ÿæˆç¶œåˆå ±å°
            )
            
            # æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆæ˜¯å¦ç”Ÿæˆ
            if os.path.exists(output_file):
                return output_file
            else:
                logger.error("âŒ å ±å°ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆæœªç”Ÿæˆ")
                return None
                
        except Exception as e:
            logger.error(f"âŒ å ±å°ç”Ÿæˆå¤±æ•—ï¼š{e}")
            return None
    
    def _generate_final_summary(self, processed_file: str, reports_file: str, start_time: datetime):
        """ç”Ÿæˆæœ€çµ‚æ‘˜è¦å ±å‘Š"""
        try:
            end_time = datetime.now()
            duration = end_time - start_time
            
            # è®€å–çµ±è¨ˆè³‡è¨Š
            processed_stats = self._get_file_stats(processed_file)
            reports_stats = self._get_file_stats(reports_file)
            
            summary_content = f"""
å®Œæ•´æ–°èè™•ç†æµæ°´ç·šåŸ·è¡Œå ±å‘Š
===============================
åŸ·è¡Œæ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')} - {end_time.strftime('%Y-%m-%d %H:%M:%S')}
ç¸½è€—æ™‚: {duration}

æª”æ¡ˆè³‡è¨Š:
- åŸå§‹æª”æ¡ˆ: cleaned_final_news.json
- è™•ç†çµæœ: {os.path.basename(processed_file)}
- æœ€çµ‚å ±å°: {os.path.basename(reports_file)}

è™•ç†çµ±è¨ˆ:
{processed_stats}

å ±å°çµ±è¨ˆ:
{reports_stats}

ğŸ‰ æµæ°´ç·šåŸ·è¡Œå®Œæˆï¼
æœ€çµ‚è¼¸å‡ºæª”æ¡ˆ: {reports_file}
"""
            
            # ä¿å­˜æ‘˜è¦å ±å‘Š
            summary_file = reports_file.replace('.json', '_pipeline_summary.txt')
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(summary_content)
            
            logger.info(f"ğŸ“Š æœ€çµ‚æ‘˜è¦å ±å‘Šå·²ä¿å­˜ï¼š{summary_file}")
            logger.info("\n" + "="*60)
            logger.info("ğŸ‰ å®Œæ•´æµæ°´ç·šåŸ·è¡Œå®Œæˆï¼")
            logger.info(f"â° ç¸½è€—æ™‚ï¼š{duration}")
            logger.info(f"ğŸ“„ æœ€çµ‚è¼¸å‡ºï¼š{reports_file}")
            logger.info("="*60)
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæœ€çµ‚æ‘˜è¦å¤±æ•—ï¼š{e}")
    
    def _get_file_stats(self, file_path: str):
        """ç²å–æª”æ¡ˆçµ±è¨ˆè³‡è¨Š"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if 'story_info' in data[0]:  # å ±å°æª”æ¡ˆ
                total_stories = len(data)
                total_articles = sum(item.get('story_info', {}).get('total_articles', 0) for item in data)
                successful_reports = sum(1 for item in data if item.get('comprehensive_report', {}).get('title'))
                
                return f"""- Stories ç¸½æ•¸: {total_stories}
- æ–‡ç« ç¸½æ•¸: {total_articles}
- æˆåŠŸç”Ÿæˆç¶œåˆå ±å°: {successful_reports}"""
            
            else:  # è™•ç†æª”æ¡ˆ
                total_stories = len(data)
                total_articles = sum(item.get('total_articles', 0) for item in data)
                successful_articles = sum(item.get('processed_articles', 0) for item in data)
                
                return f"""- Stories ç¸½æ•¸: {total_stories}
- æ–‡ç« ç¸½æ•¸: {total_articles}
- æˆåŠŸè™•ç†æ–‡ç« : {successful_articles}"""
                
        except Exception as e:
            return f"è®€å–çµ±è¨ˆè³‡è¨Šå¤±æ•—ï¼š{e}"


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸš€ å®Œæ•´æ–°èè™•ç†æµæ°´ç·š")
    print("="*50)
    
    # æª¢æŸ¥ API Key
    api_key = NewsProcessorConfig.get_gemini_api_key()
    if not api_key:
        print("âŒ æœªè¨­å®š GEMINI_API_KEY")
        print("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š GEMINI_API_KEY=your_api_key")
        return
    
    print("âœ… API Key å·²è¨­å®š")
    
    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
    input_file = "cleaned_final_news.json"
    if not os.path.exists(input_file):
        print(f"âŒ è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨ï¼š{input_file}")
        return
    
    print(f"âœ… è¼¸å…¥æª”æ¡ˆå­˜åœ¨ï¼š{input_file}")
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs("processed", exist_ok=True)
    os.makedirs("reports", exist_ok=True)
    
    try:
        # å‰µå»ºæµæ°´ç·š
        pipeline = CompletePipeline(api_key=api_key)
        
        # åŸ·è¡Œå®Œæ•´æµæ°´ç·š
        final_output = pipeline.run_complete_pipeline(
            input_file=input_file,
            output_prefix="comprehensive_reports"
        )
        
        if final_output:
            print("\nğŸ‰ æµæ°´ç·šåŸ·è¡ŒæˆåŠŸï¼")
            print(f"ğŸ“„ æœ€çµ‚è¼¸å‡ºæª”æ¡ˆï¼š{final_output}")
        else:
            print("\nâŒ æµæ°´ç·šåŸ·è¡Œå¤±æ•—")
            
    except Exception as e:
        logger.error(f"âŒ ä¸»ç¨‹å¼åŸ·è¡Œå¤±æ•—ï¼š{e}")
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—ï¼š{e}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—ï¼š{e}")
