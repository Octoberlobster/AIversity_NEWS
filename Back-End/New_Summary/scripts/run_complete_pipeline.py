"""
å®Œæ•´æ–°èè™•ç†æµæ°´ç·š
å°‡è³‡æ–™è™•ç†å’Œå ±å°ç”Ÿæˆä¸²è¯åŸ·è¡Œï¼Œç›´æ¥ç”¢ç”Ÿæœ€çµ‚çµæœ
"""

import os
import sys
import json
from datetime import datetime
import logging

# ç¢ºä¿è¼‰å…¥ .env æª”æ¡ˆ
try:
    from dotenv import load_dotenv
    load_dotenv(os.path.join(os.path.dirname(__file__), "../.env"))
except ImportError:
    pass

# æ·»åŠ çˆ¶ç›®éŒ„åˆ° Python è·¯å¾‘ï¼Œä»¥ä¾¿å¼•ç”¨ core æ¨¡çµ„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.news_processor import NewsProcessor
from core.config import NewsProcessorConfig
from core.report_generator import ReportGenerator
from core.report_config import ReportGeneratorConfig
from core.db_client import SupabaseClient

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('outputs/logs/complete_pipeline.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CompletePipeline:
    """å®Œæ•´çš„æ–°èè™•ç†æµæ°´ç·š"""
    
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–æµæ°´ç·š"""
        self.api_key = api_key or NewsProcessorConfig.get_gemini_api_key()
        if not self.api_key:
            raise ValueError("æœªè¨­å®š GEMINI_API_KEY")
        
        logger.info("ğŸš€ åˆå§‹åŒ–å®Œæ•´æ–°èè™•ç†æµæ°´ç·š")
        
    def run_complete_pipeline(self):
        """
        åŸ·è¡Œå®Œæ•´æµæ°´ç·š
        """
        
        start_time = datetime.now()
        logger.info(f"â° æµæ°´ç·šé–‹å§‹æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ–°èè³‡æ–™è™•ç†
            logger.info("\n" + "="*60)
            logger.info("ğŸ”„ ç¬¬ä¸€æ­¥ï¼šåŸ·è¡Œæ–°èè³‡æ–™è™•ç†")
            logger.info("="*60)
            
            processed_result = self._run_news_processing()

            if not processed_result:
                logger.error("âŒ æ–°èè™•ç†å¤±æ•—ï¼Œæµæ°´ç·šçµ‚æ­¢")
                return None

            logger.info(f"âœ… æ–°èè™•ç†å®Œæˆï¼š{processed_result}")

            # ç¬¬äºŒæ­¥ï¼šå ±å°ç”Ÿæˆ
            logger.info("\n" + "="*60)
            logger.info("ğŸ“ ç¬¬äºŒæ­¥ï¼šåŸ·è¡Œå ±å°ç”Ÿæˆ")
            logger.info("="*60)

            report_result = self._run_report_generation(processed_result)

            if not report_result:
                logger.error("âŒ å ±å°ç”Ÿæˆå¤±æ•—ï¼Œæµæ°´ç·šçµ‚æ­¢")
                return None

            logger.info(f"âœ… å ±å°ç”Ÿæˆå®Œæˆï¼š{report_result}")

                      
            db_client = SupabaseClient()
            for idx in range(len(report_result)):
                single_report = report_result[idx]
                update_data = {
                    'story_id': single_report.get('story_info').get('story_id' ,''),
                    'category': single_report.get('story_info').get('category' ,''),
                    'total_articles': single_report.get('story_info').get('total_articles' ,''),
                    'news_title': single_report.get('comprehensive_report', '').get('title', ''),
                    'ultra_short': single_report.get('comprehensive_report', '').get('versions', {}).get('ultra_short', ''),
                    'short': single_report.get('comprehensive_report', '').get('versions', {}).get('short', ''),
                    'long': single_report.get('comprehensive_report', '').get('versions', {}).get('long', ''),
                    'generated_date': single_report.get('processed_at', '')
                }
                db_client.save_to_single_news(single_report.get('story_info').get('story_id' ,''), update_data)

        except Exception as e:
            logger.error(f"âŒ æµæ°´ç·šåŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return None
    
    def _run_news_processing(self):
        """åŸ·è¡Œæ–°èè³‡æ–™è™•ç†"""
        try:
            
            # åˆå§‹åŒ–æ–°èè™•ç†å™¨
            processor = NewsProcessor(
                api_key=self.api_key, 
                model_name=NewsProcessorConfig.GEMINI_MODEL
            )
            
            # åŸ·è¡Œè™•ç†
            processor_result = processor.process_all_stories()
            return processor_result

        except Exception as e:
            logger.error(f"âŒ æ–°èè™•ç†å¤±æ•—ï¼š{e}")
            return None
    
    def _run_report_generation(self, processed_result):
        """åŸ·è¡Œå ±å°ç”Ÿæˆ"""
        try:
            # åˆå§‹åŒ–å ±å°ç”Ÿæˆå™¨
            generator = ReportGenerator(
                api_key=self.api_key,
                model_name=ReportGeneratorConfig.GEMINI_MODEL
            )
            
            
            # åŸ·è¡Œå ±å°ç”Ÿæˆï¼ˆåªç”Ÿæˆç¶œåˆå ±å°ï¼‰
            generator_result = generator.generate_reports_for_all_stories(processed_result)
            return generator_result
            
        except Exception as e:
            logger.error(f"âŒ å ±å°ç”Ÿæˆå¤±æ•—ï¼š{e}")
            return None

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸš€ å®Œæ•´æ–°èè™•ç†æµæ°´ç·š")
    print("="*50)
    
    # æª¢æŸ¥ API Key
    api_key = NewsProcessorConfig.get_gemini_api_key()
    if not api_key:
        print("âŒ æœªè¨­å®š GEMINI_API_KEY")
        print("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š GEMINI_API_KEY=your_api_key")
        return
    
    print("âœ… API Key å·²è¨­å®š")

    
    try:
        # å‰µå»ºæµæ°´ç·š
        pipeline = CompletePipeline(api_key=api_key)
        
        # åŸ·è¡Œå®Œæ•´æµæ°´ç·š
        generator_result = pipeline.run_complete_pipeline()
        
        if generator_result:
            print("\nğŸ‰ æµæ°´ç·šåŸ·è¡ŒæˆåŠŸï¼")
            print(f"ğŸ“„ æœ€çµ‚è¼¸å‡ºï¼š{generator_result}")
        else:
            print("\nâŒ æµæ°´ç·šåŸ·è¡Œå¤±æ•—")
            
    except Exception as e:
        logger.error(f"âŒ ä¸»ç¨‹å¼åŸ·è¡Œå¤±æ•—ï¼š{e}")
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—ï¼š{e}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—ï¼š{e}")
