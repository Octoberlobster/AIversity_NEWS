
"""
å ±å°ç”Ÿæˆå™¨ - åŸºæ–¼è™•ç†å¾Œçš„æ–°èè³‡æ–™ç”Ÿæˆå„ç¨®é¡å‹çš„å ±å°
åŠŸèƒ½ï¼š
1. å–®ç¯‡æ–‡ç« çš„é•·/ä¸­/çŸ­æ‘˜è¦ç”Ÿæˆ
2. å¤šç¯‡æ–‡ç« çš„ç¶œåˆå ±å°ç”Ÿæˆ
3. é—œéµè©å’Œäººç‰©/æ©Ÿæ§‹çš„æ•´åˆ
"""

import json
import google.generativeai as genai
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
import logging
import os
from collections import Counter, defaultdict
from core.report_config import ReportGeneratorConfig

# è¨­ç½®æ—¥èªŒ
try:
    os.makedirs(ReportGeneratorConfig.LOG_DIR, exist_ok=True)
except Exception:
    pass
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(ReportGeneratorConfig.LOG_DIR, 'report_generation.log'), encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ReportGenerator:
    """å ±å°ç”Ÿæˆå™¨ - è² è²¬ç”Ÿæˆå„ç¨®é¡å‹çš„æ–°èå ±å°"""
    
    def __init__(self, api_key: str, model_name: Optional[str] = None):
        """
        åˆå§‹åŒ–å ±å°ç”Ÿæˆå™¨
        
        Args:
            api_key: Gemini API é‡‘é‘°
            model_name: ä½¿ç”¨çš„ Gemini æ¨¡å‹åç¨±
        """
        # é…ç½® Gemini
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name or ReportGeneratorConfig.GEMINI_MODEL)
        # å¾ Config å–å¾—åƒæ•¸
        self.generation_configs = ReportGeneratorConfig.GENERATION_CONFIGS
        self.api_delay = ReportGeneratorConfig.API_DELAY
        
    def create_single_article_prompt(self, article_data: Dict, summary_type: str) -> str:
        """ç‚ºå–®ç¯‡æ–‡ç« ç”Ÿæˆä¸åŒé¡å‹æ‘˜è¦çš„ prompt"""
        
        base_info = f"""
æ–‡ç« æ¨™é¡Œï¼š{article_data.get('original_title', '')}
ç™¼å¸ƒæ™‚é–“ï¼š{article_data.get('publish_date', '')}
åˆ†é¡ï¼š{article_data.get('category', '')}
æ ¸å¿ƒæ‘˜è¦ï¼š{article_data.get('core_summary', '')}
é—œéµè©ï¼š{', '.join(article_data.get('keywords', []))}
é‡è¦äººç‰©ï¼š{', '.join(article_data.get('key_persons', []))}
ç›¸é—œæ©Ÿæ§‹ï¼š{', '.join(article_data.get('key_organizations', []))}
åœ°é»ï¼š{', '.join(article_data.get('locations', []))}
æ™‚é–“è»¸ï¼š{', '.join(article_data.get('timeline', []))}
"""

        if summary_type == "short":
            prompt = f"""
åŸºæ–¼ä»¥ä¸‹æ–‡ç« è³‡è¨Šï¼Œç”Ÿæˆä¸€å€‹ç°¡æ½”çš„çŸ­æ‘˜è¦ï¼Œé©åˆç”¨æ–¼æ–°èåˆ—è¡¨é è¦½ï¼š

{base_info}

è¦æ±‚ï¼š
1. é•·åº¦ï¼š50-80å­—
2. çªå‡ºæœ€æ ¸å¿ƒçš„æ–°èè¦é»
3. é©åˆåœ¨é»æ“Šå‰è®“è®€è€…å¿«é€Ÿäº†è§£å…§å®¹
4. èªè¨€æµæš¢è‡ªç„¶
5. åŒ…å«æœ€é‡è¦çš„é—œéµè©

è«‹ç›´æ¥è¼¸å‡ºæ‘˜è¦å…§å®¹ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡‹ã€‚
"""
        
        elif summary_type == "medium":
            prompt = f"""
åŸºæ–¼ä»¥ä¸‹æ–‡ç« è³‡è¨Šï¼Œç”Ÿæˆä¸€å€‹ä¸­ç­‰é•·åº¦çš„æ‘˜è¦ï¼Œé©åˆå¿«é€Ÿé–±è®€ï¼š

{base_info}

è¦æ±‚ï¼š
1. é•·åº¦ï¼š150-250å­—
2. åŒ…å«ä¸»è¦äº‹å¯¦å’ŒèƒŒæ™¯è³‡è¨Š
3. æ¶µè“‹é‡è¦äººç‰©ã€æ©Ÿæ§‹ã€æ™‚é–“é»
4. æåŠé—œéµæ•¸æ“šæˆ–å½±éŸ¿
5. çµæ§‹æ¸…æ™°ï¼Œåˆ†æ®µæ˜ç¢º
6. èªè¨€å°ˆæ¥­ä½†æ˜“æ‡‚

è«‹ç›´æ¥è¼¸å‡ºæ‘˜è¦å…§å®¹ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡‹ã€‚
"""
        
        elif summary_type == "long":
            prompt = f"""
åŸºæ–¼ä»¥ä¸‹æ–‡ç« è³‡è¨Šï¼Œç”Ÿæˆä¸€å€‹è©³ç´°çš„é•·æ‘˜è¦ï¼Œé©åˆæ·±åº¦é–±è®€ï¼š

{base_info}

è¦æ±‚ï¼š
1. é•·åº¦ï¼š400-600å­—
2. å®Œæ•´æè¿°äº‹ä»¶çš„ä¾†é¾å»è„ˆ
3. è©³ç´°ä»‹ç´¹ç›¸é—œäººç‰©å’Œæ©Ÿæ§‹
4. æä¾›å……åˆ†çš„èƒŒæ™¯è³‡è¨Šå’Œåˆ†æ
5. åŒ…å«å…·é«”æ•¸æ“šã€æ™‚é–“ã€åœ°é»
6. åˆ†æå¯èƒ½çš„å½±éŸ¿å’Œæ„ç¾©
7. çµæ§‹å®Œæ•´ï¼Œé‚è¼¯æ¸…æ™°
8. å°ˆæ¥­çš„æ–°èå¯«ä½œé¢¨æ ¼

è«‹ç›´æ¥è¼¸å‡ºæ‘˜è¦å…§å®¹ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡‹ã€‚
"""
        
        return prompt
    
    def create_comprehensive_report_prompt(self, story_data: Dict, articles_data: List[Dict], version: str = "long") -> str:
        """ç‚ºå¤šç¯‡æ–‡ç« ç”Ÿæˆç¶œåˆå ±å°çš„ prompt

        version: "ultra_short" | "short" | "long"
        """
        
        # æ•´åˆæ‰€æœ‰é—œéµè³‡è¨Š
        all_keywords = []
        all_persons = []
        all_organizations = []
        all_locations = []
        all_timeline = []
        core_summaries = []
        
        for article in articles_data:
            all_keywords.extend(article.get('keywords', []))
            all_persons.extend(article.get('key_persons', []))
            all_organizations.extend(article.get('key_organizations', []))
            all_locations.extend(article.get('locations', []))
            all_timeline.extend(article.get('timeline', []))
            core_summaries.append(article.get('core_summary', ''))
        
        # çµ±è¨ˆé »æ¬¡ä¸¦å»é‡
        keyword_counts = Counter(all_keywords)
        top_keywords = [k for k, v in keyword_counts.most_common(10)]
        
        person_counts = Counter(all_persons)
        top_persons = [k for k, v in person_counts.most_common(5)]
        
        org_counts = Counter(all_organizations)
        top_organizations = [k for k, v in org_counts.most_common(5)]
        
        unique_locations = list(set(all_locations))
        unique_timeline = sorted(list(set(all_timeline)))
        
        length_cfg = ReportGeneratorConfig.COMPREHENSIVE_LENGTHS.get(version, ReportGeneratorConfig.COMPREHENSIVE_LENGTHS["long"])
        min_chars = length_cfg["min_chars"]
        max_chars = length_cfg["max_chars"]

        prompt = f"""
åŸºæ–¼ä»¥ä¸‹å¤šç¯‡ç›¸é—œæ–‡ç« çš„è³‡è¨Šï¼Œç”Ÿæˆä¸€ç¯‡ç¶œåˆå ±å°ï¼š

å°ˆé¡Œï¼š{story_data.get('story_title', '')}
åˆ†é¡ï¼š{story_data.get('category', '')}
æ–‡ç« æ•¸é‡ï¼š{len(articles_data)}ç¯‡

æ ¸å¿ƒå…§å®¹æ‘˜è¦ï¼š
{chr(10).join([f"â€¢ {summary}" for summary in core_summaries[:5]])}

ä¸»è¦é—œéµè©ï¼š{', '.join(top_keywords)}
é‡è¦äººç‰©ï¼š{', '.join(top_persons)}
ç›¸é—œæ©Ÿæ§‹ï¼š{', '.join(top_organizations)}
æ¶‰åŠåœ°é»ï¼š{', '.join(unique_locations)}
æ™‚é–“è»¸ï¼š{', '.join(unique_timeline)}

è¦æ±‚ï¼š
1. é•·åº¦ï¼š{min_chars}-{max_chars}å­—
2. æ•´åˆæ‰€æœ‰æ–‡ç« çš„æ ¸å¿ƒè³‡è¨Šï¼Œå»é™¤é‡è¤‡å…§å®¹
3. æŒ‰é‚è¼¯é †åºçµ„ç¹”å…§å®¹ï¼ˆèƒŒæ™¯â†’ç™¼å±•â†’ç¾ç‹€â†’å½±éŸ¿ï¼‰
4. çªå‡ºæœ€é‡è¦çš„äººç‰©ã€æ©Ÿæ§‹ã€æ•¸æ“š
5. æä¾›å®Œæ•´çš„æ™‚é–“è„ˆçµ¡
6. åˆ†æäº‹ä»¶çš„æ„ç¾©å’Œå¯èƒ½å½±éŸ¿
7. ä½¿ç”¨å°ˆæ¥­çš„æ–°èå ±å°å¯«ä½œé¢¨æ ¼
8. çµæ§‹æ¸…æ™°ï¼Œä½¿ç”¨é©ç•¶çš„æ®µè½åˆ†éš”
9. ç¢ºä¿è³‡è¨Šæº–ç¢ºï¼Œé¿å…æ¨æ¸¬

è«‹ç”Ÿæˆä¸€ç¯‡å®Œæ•´çš„ç¶œåˆå ±å°ï¼ŒåŒ…å«æ¨™é¡Œå’Œæ­£æ–‡ã€‚
"""
        
        return prompt
    
    def generate_single_article_summaries(self, article_data: Dict) -> Dict[str, str]:
        """ç‚ºå–®ç¯‡æ–‡ç« ç”Ÿæˆé•·ä¸­çŸ­ä¸‰ç¨®æ‘˜è¦"""
        
        summaries = {}
        
        for summary_type in ["short", "medium", "long"]:
            try:
                logger.info(f"ç”Ÿæˆ {summary_type} æ‘˜è¦ - æ–‡ç« ï¼š{article_data.get('original_title', 'Unknown')[:50]}...")
                
                prompt = self.create_single_article_prompt(article_data, summary_type)
                config = self.generation_configs[f"{summary_type}_summary"]
                
                response = self.model.generate_content(
                    prompt,
                    generation_config=config
                )
                
                if response.text:
                    summaries[f"{summary_type}_summary"] = response.text.strip()
                    logger.info(f"âœ… {summary_type} æ‘˜è¦ç”ŸæˆæˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ {summary_type} æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼šç©ºå›æ‡‰")
                    summaries[f"{summary_type}_summary"] = ""
                
                # API èª¿ç”¨å»¶é²
                time.sleep(self.api_delay)
                
            except Exception as e:
                logger.error(f"âŒ {summary_type} æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼š{e}")
                summaries[f"{summary_type}_summary"] = ""
        
        return summaries
    
    def generate_comprehensive_report(self, story_data: Dict, articles_data: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆç¶œåˆå ±å°ï¼ˆåŒæ™‚è¼¸å‡ºä¸‰ç¨®é•·åº¦ç‰ˆæœ¬ï¼‰"""
        
        try:
            logger.info(f"ç”Ÿæˆç¶œåˆå ±å° - å°ˆé¡Œï¼š{story_data.get('story_title', 'Unknown')}")
            
            outputs = {}
            for version, cfg_key in (
                ("ultra_short", "comprehensive_ultra_short"),
                ("short", "comprehensive_short"),
                ("long", "comprehensive_long"),
            ):
                prompt = self.create_comprehensive_report_prompt(story_data, articles_data, version=version)
                config = self.generation_configs[cfg_key]
                response = self.model.generate_content(
                    prompt,
                    generation_config=config
                )
                if response.text:
                    content = response.text.strip()
                    lines = content.split('\n')
                    title = lines[0].strip()
                    body = '\n'.join(lines[1:]).strip()
                    if len(title) > 100 or not title:
                        title = f"{story_data.get('category', '')}å°ˆé¡Œå ±å°ï¼š{story_data.get('story_title', '')}"
                        body = content
                    outputs[version] = {
                        "content": body,
                        "generated_at": datetime.now().isoformat(timespec='minutes')
                    }
                else:
                    outputs[version] = {
                        "title": "",
                        "content": ""
                    }
                time.sleep(self.api_delay)

            # é•·ç‰ˆä½œç‚ºä¸»æ¨™é¡Œï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            main = outputs.get("long", {})
            result = {
                "title": main.get("title", ""),
                "content": main.get("content", ""),
                "versions": outputs,  # æ–°å¢ï¼šä¸‰ç¨®ç‰ˆæœ¬éƒ½å­˜é€™è£¡
                "article_count": len(articles_data),
                "generated_at": datetime.now().isoformat(timespec='minutes')
            }
            logger.info("âœ… ç¶œåˆå ±å°ï¼ˆå¤šç‰ˆæœ¬ï¼‰ç”ŸæˆæˆåŠŸ")
            return result
                
        except Exception as e:
            logger.error(f"âŒ ç¶œåˆå ±å°ç”Ÿæˆå¤±æ•—ï¼š{e}")
            return {}
    
    def process_story_reports(self, story_data: Dict, generate_individual: bool = False) -> Dict[str, Any]:
        """è™•ç†å–®å€‹ story çš„æ‰€æœ‰å ±å°ç”Ÿæˆ"""
        
        articles_data = story_data.get('articles_analysis', [])
        if not articles_data:
            logger.warning(f"Story {story_data.get('story_index')} æ²’æœ‰å¯è™•ç†çš„æ–‡ç« è³‡æ–™")
            return {}
        
        logger.info(f"é–‹å§‹è™•ç† Story {story_data.get('story_index')} - {len(articles_data)} ç¯‡æ–‡ç« ")
        
        result = {
            "story_info": {
                "story_index": story_data.get('story_index'),
                "story_title": story_data.get('story_title'),
                "category": story_data.get('category'),
                "total_articles": len(articles_data)
            },
            "comprehensive_report": {},
            "processing_stats": {
                "processed_articles": len(articles_data),
                "successful_summaries": 0,
                "failed_summaries": 0
            }
        }
        
        # åªç”Ÿæˆå€‹åˆ¥æ‘˜è¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if generate_individual:
            result["individual_summaries"] = []
            for i, article_data in enumerate(articles_data):
                logger.info(f"è™•ç†æ–‡ç«  {i+1}/{len(articles_data)}")
                
                summaries = self.generate_single_article_summaries(article_data)
                
                article_result = {
                    "article_id": article_data.get('original_article_id'),
                    "original_title": article_data.get('original_title'),
                    "publish_date": article_data.get('publish_date'),
                    "source_url": article_data.get('source_url'),
                    "summaries": summaries,
                    "original_analysis": {
                        "keywords": article_data.get('keywords', []),
                        "key_persons": article_data.get('key_persons', []),
                        "key_organizations": article_data.get('key_organizations', []),
                        "confidence_score": article_data.get('confidence_score', 0)
                    }
                }
                
                result["individual_summaries"].append(article_result)
                
                # çµ±è¨ˆæˆåŠŸç”Ÿæˆçš„æ‘˜è¦æ•¸é‡
                successful_summaries = sum(1 for k, v in summaries.items() if v.strip())
                result["processing_stats"]["successful_summaries"] += successful_summaries
                result["processing_stats"]["failed_summaries"] += (3 - successful_summaries)
        
        # ç”Ÿæˆç¶œåˆå ±å°ï¼ˆä¸»è¦åŠŸèƒ½ï¼‰
        logger.info("ç”Ÿæˆç¶œåˆå ±å°...")
        comprehensive_report = self.generate_comprehensive_report(story_data, articles_data)
        result["comprehensive_report"] = comprehensive_report
        
        # å¦‚æœç¶œåˆå ±å°ç”ŸæˆæˆåŠŸï¼Œæ›´æ–°çµ±è¨ˆ
        if comprehensive_report.get('title'):
            result["processing_stats"]["comprehensive_report_success"] = True
        else:
            result["processing_stats"]["comprehensive_report_success"] = False
        
        result["processed_at"] = datetime.now().isoformat()
        
        logger.info(f"Story {story_data.get('story_index')} è™•ç†å®Œæˆ")
        return result
    
    def load_processed_data(self, file_path: str) -> List[Dict]:
        """è¼‰å…¥å·²è™•ç†çš„æ–°èè³‡æ–™"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"æˆåŠŸè¼‰å…¥è³‡æ–™ï¼š{len(data)} å€‹ stories")
            return data
        except Exception as e:
            logger.error(f"è¼‰å…¥è³‡æ–™å¤±æ•—ï¼š{e}")
            return []
    
    def save_reports(self, reports_data: List[Dict], output_path: str):
        """ä¿å­˜ç”Ÿæˆçš„å ±å°"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(reports_data, f, ensure_ascii=False, indent=2)
            logger.info(f"å ±å°å·²ä¿å­˜è‡³ï¼š{output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜å ±å°å¤±æ•—ï¼š{e}")
    
    def generate_reports_for_all_stories(self, input_file: Optional[str], output_file: str, 
                                       start_index: int = 0, max_stories: Optional[int] = None,
                                       generate_individual: bool = False):
        """ç‚ºæ‰€æœ‰ stories ç”Ÿæˆå ±å°"""
        
        # è¼‰å…¥è³‡æ–™
        # è‹¥æœªæŒ‡å®šï¼Œå¾è¨­å®šä¸­å°‹æ‰¾æœ€æ–°çš„è™•ç†æª”æ¡ˆ
        input_file = input_file or ReportGeneratorConfig.find_latest_processed_file()
        stories_data = self.load_processed_data(input_file)
        if not stories_data:
            logger.error("æ²’æœ‰å¯è™•ç†çš„è³‡æ–™")
            return
        
        # ç¢ºå®šè™•ç†ç¯„åœ
        end_index = len(stories_data)
        if max_stories:
            end_index = min(start_index + max_stories, len(stories_data))
        
        stories_to_process = stories_data[start_index:end_index]
        
        if generate_individual:
            logger.info(f"æº–å‚™è™•ç† {len(stories_to_process)} å€‹ stories (åŒ…å«å€‹åˆ¥æ‘˜è¦å’Œç¶œåˆå ±å°)")
        else:
            logger.info(f"æº–å‚™è™•ç† {len(stories_to_process)} å€‹ stories (åƒ…ç”Ÿæˆç¶œåˆå ±å°)")
        
        logger.info(f"è™•ç†ç¯„åœ: ç´¢å¼• {start_index}-{end_index-1}")
        
        results = []
        
        for i, story_data in enumerate(stories_to_process):
            actual_index = start_index + i
            logger.info(f"\n{'='*60}")
            logger.info(f"è™•ç† Story {actual_index + 1}/{len(stories_data)}")
            logger.info(f"{'='*60}")
            
            try:
                story_reports = self.process_story_reports(story_data, generate_individual=generate_individual)
                if story_reports:
                    results.append(story_reports)
                    logger.info(f"âœ… Story {actual_index + 1} è™•ç†æˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ Story {actual_index + 1} è™•ç†å¤±æ•—")
                
            except Exception as e:
                logger.error(f"âŒ Story {actual_index + 1} è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                continue
        
        # ä¿å­˜çµæœ
        if results:
            # è‹¥æœªæä¾›è¼¸å‡ºè·¯å¾‘ï¼Œä½¿ç”¨è¨­å®šæª”å‘½å
            if not output_file:
                output_file = ReportGeneratorConfig.get_output_filename(prefix="final_comprehensive_reports")
            self.save_reports(results, output_file)
            self.generate_reports_summary(results, output_file.replace('.json', '_summary.txt'))
        
        logger.info(f"\nğŸ‰ å ±å°ç”Ÿæˆå®Œæˆï¼æˆåŠŸè™•ç† {len(results)} å€‹ stories")
    
    def generate_reports_summary(self, reports_data: List[Dict], summary_path: str):
        """ç”Ÿæˆå ±å°è™•ç†æ‘˜è¦"""
        
        total_stories = len(reports_data)
        total_articles = sum(r.get('story_info', {}).get('total_articles', 0) for r in reports_data)
        total_individual_summaries = sum(r.get('processing_stats', {}).get('successful_summaries', 0) for r in reports_data)
        total_comprehensive = sum(1 for r in reports_data if r.get('comprehensive_report', {}).get('title'))
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å€‹åˆ¥æ‘˜è¦
        has_individual = any('individual_summaries' in r for r in reports_data)
        
        summary_content = f"""
å ±å°ç”Ÿæˆæ‘˜è¦
====================
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç”Ÿæˆæ¨¡å¼: {'ç¶œåˆå ±å° + å€‹åˆ¥æ‘˜è¦' if has_individual else 'åƒ…ç¶œåˆå ±å°'}

çµ±è¨ˆè³‡è¨Š:
- è™•ç† Stories ç¸½æ•¸: {total_stories}
- æ–‡ç« ç¸½æ•¸: {total_articles}
- ç”Ÿæˆç¶œåˆå ±å°æ•¸: {total_comprehensive}"""

        if has_individual:
            summary_content += f"""
- ç”Ÿæˆå€‹åˆ¥æ‘˜è¦ç¸½æ•¸: {total_individual_summaries}"""
        
        summary_content += """

å„ Story è©³æƒ…:
"""
        
        for report in reports_data:
            story_info = report.get('story_info', {})
            stats = report.get('processing_stats', {})
            comprehensive = report.get('comprehensive_report', {})
            
            summary_content += f"""
Story {story_info.get('story_index', 'Unknown')}: {story_info.get('story_title', 'Unknown')}
  - åˆ†é¡: {story_info.get('category', 'Unknown')}
  - æ–‡ç« æ•¸: {story_info.get('total_articles', 0)}
  - ç¶œåˆå ±å°: {'âœ…' if comprehensive.get('title') else 'âŒ'}"""
            
            if comprehensive.get('title'):
                content_length = len(comprehensive.get('content', ''))
                summary_content += f"""
  - å ±å°é•·åº¦: {content_length} å­—"""
            
            if has_individual:
                summary_content += f"""
  - å€‹åˆ¥æ‘˜è¦: {stats.get('successful_summaries', 0)} æˆåŠŸ / {stats.get('failed_summaries', 0)} å¤±æ•—"""
        
        try:
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(summary_content)
            logger.info(f"æ‘˜è¦å ±å‘Šå·²ä¿å­˜è‡³ï¼š{summary_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ‘˜è¦å ±å‘Šå¤±æ•—ï¼š{e}")
