import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
import logging
import os
from collections import Counter
from pydantic import BaseModel
from google import genai
from google.genai import types  # æ–°ç‰ˆ SDK å‹åˆ¥
from core.report_config import ReportGeneratorConfig

# è¨­ç½®æ—¥èªŒ
try:
    os.makedirs(ReportGeneratorConfig.LOG_DIR, exist_ok=True)
except Exception:
    pass

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(ReportGeneratorConfig.LOG_DIR, 'report_generation.log'), encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class HintPromptResponse(BaseModel):
    title: str
    content: str

class ReportGenerator:
    """å ±å°ç”Ÿæˆå™¨ - è² è²¬ç”Ÿæˆå„ç¨®é¡å‹çš„æ–°èå ±å°ï¼ˆæ–°ç‰ˆ google-genaiï¼‰"""

    def __init__(self, api_key: Optional[str] = None, model_name: Optional[str] = None):
        """
        åˆå§‹åŒ–å ±å°ç”Ÿæˆå™¨

        Args:
            api_key: Gemini API é‡‘é‘°ï¼ˆå¯çœç•¥ï¼Œå°‡è‡ª config æˆ–ç’°å¢ƒè®Šæ•¸è®€å–ï¼‰
            model_name: ä½¿ç”¨çš„ Gemini æ¨¡å‹åç¨±
        """
        # å–å¾— API Keyï¼ˆåƒæ•¸å„ªå…ˆï¼Œå…¶æ¬¡ configï¼Œæœ€å¾Œç’°å¢ƒè®Šæ•¸ï¼‰
        self.api_key = api_key or getattr(ReportGeneratorConfig, "GEMINI_API_KEY", None) or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("æ‰¾ä¸åˆ° Gemini API é‡‘é‘°ã€‚è«‹åœ¨åƒæ•¸ã€ReportGeneratorConfig æˆ–ç’°å¢ƒè®Šæ•¸ GEMINI_API_KEY è¨­å®šã€‚")

        # å»ºç«‹æ–°ç‰ˆ GenAI Client
        self.client = genai.Client(api_key=self.api_key)

        # æ¨¡å‹åç¨±
        self.model_name = model_name or ReportGeneratorConfig.GEMINI_MODEL

        # å¾ Config å–å¾—ç”Ÿæˆåƒæ•¸/å®‰å…¨è¨­å®š/å»¶é²
        self.generation_configs: Dict[str, Dict[str, Any]] = getattr(ReportGeneratorConfig, "GENERATION_CONFIGS", {})
        self.safety_settings = getattr(ReportGeneratorConfig, "SAFETY_SETTINGS", [])
        self.api_delay = getattr(ReportGeneratorConfig, "API_DELAY", 0.8)

    # ===== å·¥å…·ï¼šæŠŠ safety è¨­å®šè½‰æˆæ–°ç‰ˆå‹åˆ¥ï¼Œèˆ‡å»ºç«‹ GenerateContentConfig =====
    def _to_safety_settings(self) -> List[types.SafetySetting]:
        out: List[types.SafetySetting] = []
        for s in (self.safety_settings or []):
            if isinstance(s, types.SafetySetting):
                out.append(s)
            elif isinstance(s, dict):
                out.append(types.SafetySetting(
                    category=s.get("category"),
                    threshold=s.get("threshold"),
                ))
        return out

    def _build_generate_config_by_key(self, key: str) -> types.GenerateContentConfig:
        """
        å¾ GENERATION_CONFIGS[key] å»ºç«‹ GenerateContentConfigã€‚
        è‹¥ force_json=Trueï¼Œæœƒè¨­ç½® response_mime_type ç‚º application/json ä¸¦æŒ‡å®šåˆæ³•çš„ response_schemaã€‚
        """
        base = dict(self.generation_configs.get(key, {}))
        base["response_mime_type"] = "application/json"
        base['response_schema'] = HintPromptResponse
        base["safety_settings"] = self._to_safety_settings()
        return types.GenerateContentConfig(**base)
    def create_comprehensive_report_prompt(self, story_data: Dict, articles_data: List[Dict], version: str = "long") -> str:
        """ç‚ºå¤šç¯‡æ–‡ç« ç”Ÿæˆç¶œåˆå ±å°çš„ promptï¼ˆversion: "ultra_short" | "short" | "long"ï¼‰"""

        # æ•´åˆæ‰€æœ‰é—œéµè³‡è¨Š
        all_keywords: List[str] = []
        all_persons: List[str] = []
        all_organizations: List[str] = []
        all_locations: List[str] = []
        all_timeline: List[str] = []
        all_sourceurl: List[str] = []
        core_summaries: List[str] = []

        for article in articles_data:
            all_keywords.extend(article.get('keywords', []))
            all_persons.extend(article.get('key_persons', []))
            all_organizations.extend(article.get('key_organizations', []))
            all_locations.extend(article.get('locations', []))
            all_timeline.extend(article.get('timeline', []))
            # ä¿®æ­£ï¼šsource_url ç‚ºå­—ä¸²ï¼Œæ‡‰ä½¿ç”¨ append è€Œé extend
            src = article.get('source_url')
            if src:
                all_sourceurl.append(src)
            core_summaries.append(article.get('core_summary', ''))

        # çµ±è¨ˆé »æ¬¡ä¸¦å»é‡
        keyword_counts = Counter(all_keywords)
        top_keywords = [k for k, _ in keyword_counts.most_common(10)]

        person_counts = Counter(all_persons)
        top_persons = [k for k, _ in person_counts.most_common(5)]

        org_counts = Counter(all_organizations)
        top_organizations = [k for k, _ in org_counts.most_common(5)]

        unique_locations = list(dict.fromkeys(all_locations))  # å»é‡ä¿åº
        unique_timeline = sorted(set(all_timeline))            # æ™‚é–“é»å­—ä¸²æ’åº

        length_cfg = ReportGeneratorConfig.COMPREHENSIVE_LENGTHS.get(
            version, ReportGeneratorConfig.COMPREHENSIVE_LENGTHS["long"]
        )
        min_chars = length_cfg["min_chars"]
        max_chars = length_cfg["max_chars"]

        # é¡¯ç¤ºæœ€å¤š 10 å€‹ä¾†æºé€£çµï¼ˆè‹¥éœ€è¦ï¼‰
        src_preview = "\n".join([f"- {u}" for u in all_sourceurl[:10]]) if all_sourceurl else "ï¼ˆä¾†æºå½™æ•´ï¼‰"

        schema = {
                "title": "æ–°èæ¨™é¡Œï¼ˆ20å­—å…§ï¼‰",
                "content": f"æ–°èå…§æ–‡ï¼ˆ{max_chars}å­—å…§ï¼‰"
            }

        # ç¢ºä¿ max_chars æ˜¯ int ä¸¦ä¸” f-string å·²ç¶“è¢«æ­£ç¢ºè©•ä¼°
        schema["content"] = f"æ–°èå…§æ–‡ï¼ˆ{max_chars}å­—å…§ï¼‰"
        
        prompt = f"""
            åŸºæ–¼ä»¥ä¸‹å¤šç¯‡ç›¸é—œæ–‡ç« çš„è³‡è¨Šï¼Œç”Ÿæˆä¸€ç¯‡ç¶œåˆå ±å°ï¼š

            å°ˆé¡Œï¼š{story_data.get('story_title', '')}
            åˆ†é¡ï¼š{story_data.get('category', '')}
            æ–‡ç« æ•¸é‡ï¼š{len(articles_data)} ç¯‡

            æ ¸å¿ƒå…§å®¹æ‘˜è¦ï¼ˆç¯€éŒ„ï¼‰ï¼š
            {chr(30).join([f"â€¢ {s}" for s in core_summaries[:15] if s])}

            ä¸»è¦é—œéµè©ï¼š{', '.join(top_keywords) if top_keywords else 'ï¼ˆç„¡ï¼‰'}
            é‡è¦äººç‰©ï¼š{', '.join(top_persons) if top_persons else 'ï¼ˆç„¡ï¼‰'}
            ç›¸é—œæ©Ÿæ§‹ï¼š{', '.join(top_organizations) if top_organizations else 'ï¼ˆç„¡ï¼‰'}
            æ¶‰åŠåœ°é»ï¼š{', '.join(unique_locations) if unique_locations else 'ï¼ˆç„¡ï¼‰'}
            æ™‚é–“è»¸ï¼š{', '.join(unique_timeline) if unique_timeline else 'ï¼ˆç„¡ï¼‰'}

            åƒè€ƒä¾†æºï¼ˆç¯€éŒ„ï¼‰ï¼š
            {src_preview}

            è¦æ±‚ï¼š
            1. ç”Ÿæˆä¸€å€‹20å­—ä»¥å…§çš„æ–°èæ¨™é¡Œ
            2. æ ¹æ“šç‰ˆæœ¬è¦æ±‚ç”Ÿæˆå°æ‡‰é•·åº¦çš„å…§æ–‡ï¼š
               - ç•¶å‰ç‰ˆæœ¬ï¼š{version}
               - ç•¶å‰å­—æ•¸é™åˆ¶ï¼š{min_chars}-{max_chars} å­—
            3. æ•´åˆæ ¸å¿ƒè³‡è¨Šï¼Œçªå‡ºé‡è¦äººç‰©/æ©Ÿæ§‹/æ•¸æ“šï¼Œå»é™¤é‡è¤‡å…§å®¹
            4. ä½¿ç”¨å°ˆæ¥­æ–°èå¯«ä½œé¢¨æ ¼
            5. ç¢ºä¿è³‡è¨Šæº–ç¢ºï¼Œé¿å…æ¨æ¸¬
            6. æä¾›å®Œæ•´çš„æ™‚é–“è„ˆçµ¡
            7. åˆ†æäº‹ä»¶çš„å½±éŸ¿å’Œæ„ç¾©
            8. ç¢ºä¿è³‡è¨Šæº–ç¢ºï¼Œé¿å…æ¨æ¸¬
            9. æŒ‰é‚è¼¯é †åºçµ„ç¹”å…§å®¹ï¼Œç¢ºä¿çµæ§‹æ¸…æ™°

            è«‹è¼¸å‡ºä¸€å€‹JSONç‰©ä»¶ï¼ŒåŒ…å«æ¨™é¡Œå’Œå…§æ–‡ï¼š
            <JSONSchema>{json.dumps(schema)}</JSONSchema>
        """.strip()

        return prompt
    def generate_comprehensive_report(self, story_data: Dict, articles_data: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆç¶œåˆå ±å°ï¼ˆåŒæ™‚è¼¸å‡ºä¸‰ç¨®é•·åº¦ç‰ˆæœ¬ï¼‰"""
        try:
            logger.info(f"ç”Ÿæˆç¶œåˆå ±å° - å°ˆé¡Œï¼š{story_data.get('story_title', 'Unknown')}")

            outputs: Dict[str, Dict[str, Any]] = {}
            main_title = ""
            for version, cfg_key in (
                ("ultra_short", "comprehensive_ultra_short"),
                ("short", "comprehensive_short"),
                ("long", "comprehensive_long"),
            ):
                prompt = self.create_comprehensive_report_prompt(story_data, articles_data, version=version)
                gen_cfg = self._build_generate_config_by_key(cfg_key)

                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=gen_cfg
                )
                print("response = ",response)
                dump_genai_response(response, label=version)  # â† ä¸€éµå…¨å°

                if response.parsed is not None:
                    # å¯èƒ½æ˜¯å–®ä¸€ BaseModelï¼Œä¹Ÿå¯èƒ½æ˜¯ list[BaseModel]
                    parsed = response.parsed
                    items = parsed if isinstance(parsed, list) else [parsed]
                    for item in items:
                        # Pydantic ç‰©ä»¶ï¼šç›´æ¥ç”¨å±¬æ€§
                        title = item.title
                        body  = item.content
                        outputs[version] = body
                        if version == "long":
                            main_title = title
                else:
                    # å‚™æ¡ˆï¼šèµ°æ–‡å­—æˆ– dict
                    raw_text = (response.text or "").strip()   # å¯èƒ½ç‚º None
                    if raw_text:
                        try:
                            data = json.loads(raw_text)
                        except json.JSONDecodeError:
                            data = {}
                        title = (data.get("title") or "").strip()
                        body  = (data.get("content") or raw_text).strip()
                        outputs[version] = body
                    else:
                        outputs[version] = ""

                time.sleep(self.api_delay)
                

            # å–å¾—æ‰€æœ‰ä¾†æº URL
            all_sourceurl = list(set(article.get('source_url') for article in articles_data if article.get('source_url')))
            
            
            result = {
                "title": main_title,
                "versions": outputs,  # åªåŒ…å«å„ç‰ˆæœ¬çš„ content
                "source_urls": all_sourceurl
            }
            logger.info("âœ… ç¶œåˆå ±å°ï¼ˆå¤šç‰ˆæœ¬ï¼‰ç”ŸæˆæˆåŠŸ")
            return result

        except Exception as e:
            logger.error(f"âŒ ç¶œåˆå ±å°ç”Ÿæˆå¤±æ•—ï¼š{e}")
            return {}

    # ===== æª”æ¡ˆ I/O èˆ‡æ•´æ‰¹è™•ç† =====
    def load_processed_data(self, file_path: str) -> List[Dict]:
        """è¼‰å…¥å·²è™•ç†çš„æ–°èè³‡æ–™"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            logger.info(f"æˆåŠŸè¼‰å…¥è³‡æ–™ï¼š{len(data)} å€‹ stories")
            return data
        except Exception as e:
            logger.error(f"è¼‰å…¥è³‡æ–™å¤±æ•—ï¼š{e}")
            return []

    def save_reports(self, reports_data: List[Dict], output_path: str):
        """ä¿å­˜ç”Ÿæˆçš„å ±å°"""
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(reports_data, f, ensure_ascii=False, indent=2)
            logger.info(f"å ±å°å·²ä¿å­˜è‡³ï¼š{output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜å ±å°å¤±æ•—ï¼š{e}")

    def generate_reports_for_all_stories(
        self,
        input_file: Optional[str],
        output_file: Optional[str],
        start_index: int = 0,
        max_stories: Optional[int] = None,
        generate_individual: bool = False
    ):
        """ç‚ºæ‰€æœ‰ stories ç”Ÿæˆå ±å°"""

        # è‹¥æœªæŒ‡å®šï¼Œå¾è¨­å®šä¸­å°‹æ‰¾æœ€æ–°çš„è™•ç†æª”æ¡ˆ
        input_file = input_file or ReportGeneratorConfig.find_latest_processed_file()
        stories_data = self.load_processed_data(input_file)
        if not stories_data:
            logger.error("æ²’æœ‰å¯è™•ç†çš„è³‡æ–™")
            return

        # ç¢ºå®šè™•ç†ç¯„åœ
        end_index = len(stories_data)
        if max_stories:
            end_index = min(start_index + max_stories, len(stories_data))

        stories_to_process = stories_data[start_index:end_index]

        if generate_individual:
            logger.info(f"æº–å‚™è™•ç† {len(stories_to_process)} å€‹ storiesï¼ˆåŒ…å«å€‹åˆ¥æ‘˜è¦èˆ‡ç¶œåˆå ±å°ï¼‰")
        else:
            logger.info(f"æº–å‚™è™•ç† {len(stories_to_process)} å€‹ storiesï¼ˆåƒ…ç”Ÿæˆç¶œåˆå ±å°ï¼‰")

        logger.info(f"è™•ç†ç¯„åœ: ç´¢å¼• {start_index}-{end_index-1}")

        results: List[Dict[str, Any]] = []

        for i, story_data in enumerate(stories_to_process):
            actual_index = start_index + i
            logger.info("\n" + "=" * 60)
            logger.info(f"è™•ç† Story {actual_index + 1}/{len(stories_data)}")
            logger.info("=" * 60)

            try:
                story_reports = self.process_story_reports(story_data, generate_individual=generate_individual)
                if story_reports:
                    results.append(story_reports)
                    logger.info(f"âœ… Story {actual_index + 1} è™•ç†æˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ Story {actual_index + 1} è™•ç†å¤±æ•—")

            except Exception as e:
                logger.error(f"âŒ Story {actual_index + 1} è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                continue

        # ä¿å­˜çµæœ
        if results:
            if not output_file:
                output_file = ReportGeneratorConfig.get_output_filename(prefix="final_comprehensive_reports")
            self.save_reports(results, output_file)
            self.generate_reports_summary(results, output_file.replace('.json', '_summary.txt'))

        logger.info(f"\nğŸ‰ å ±å°ç”Ÿæˆå®Œæˆï¼æˆåŠŸè™•ç† {len(results)} å€‹ stories")

    def process_story_reports(self, story_data: Dict, generate_individual: bool = False) -> Dict[str, Any]:
        """è™•ç†ç¶œåˆå ±å°ç”Ÿæˆ"""
        articles_data = story_data.get('articles_analysis', [])
        if not articles_data:
            logger.warning(f"Story {story_data.get('story_index')} æ²’æœ‰å¯è™•ç†çš„æ–‡ç« è³‡æ–™")
            return {}

        logger.info(f"é–‹å§‹è™•ç† Story {story_data.get('story_index')} - {len(articles_data)} ç¯‡æ–‡ç« ")

        result: Dict[str, Any] = {
            "story_info": {
                "story_index": story_data.get('story_index'),
                "story_title": story_data.get('story_title'),
                "category": story_data.get('category'),
                "total_articles": len(articles_data)
            },
            "comprehensive_report": {},
            "processing_stats": {
                "processed_articles": len(articles_data)
            }
        }

        # åƒ…ç”Ÿæˆç¶œåˆå ±å°ï¼ˆä¸»è¦åŠŸèƒ½ï¼‰
        logger.info("ç”Ÿæˆç¶œåˆå ±å°")
        comprehensive_report = self.generate_comprehensive_report(story_data, articles_data)
        result["comprehensive_report"] = comprehensive_report

        title_ok = bool((comprehensive_report.get('title') or "").strip())
        content_ok = bool((comprehensive_report.get('content') or "").strip())
        result["processing_stats"]["comprehensive_report_success"] = (title_ok or content_ok)

        result["processed_at"] = datetime.now().isoformat(timespec='minutes')
        logger.info(f"Story {story_data.get('story_index')} è™•ç†å®Œæˆ")
        return result


    def generate_reports_summary(self, reports_data: List[Dict], summary_path: str):
        """ç”Ÿæˆå ±å°è™•ç†æ‘˜è¦"""
        total_stories = len(reports_data)
        total_articles = sum(r.get('story_info', {}).get('total_articles', 0) for r in reports_data)
        total_comprehensive = sum(1 for r in reports_data if r.get('comprehensive_report', {}).get('title'))


        summary_content = f"""
å ±å°ç”Ÿæˆæ‘˜è¦
====================
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç”Ÿæˆæ¨¡å¼: {'ç¶œåˆå ±å°'}

çµ±è¨ˆè³‡è¨Š:
- è™•ç† Stories ç¸½æ•¸: {total_stories}
- æ–‡ç« ç¸½æ•¸: {total_articles}
- ç”Ÿæˆç¶œåˆå ±å°æ•¸: {total_comprehensive}"""
        summary_content += """

å„ Story è©³æƒ…:
"""

        for report in reports_data:
            story_info = report.get('story_info', {})
            stats = report.get('processing_stats', {})
            comprehensive = report.get('comprehensive_report', {})

            summary_content += f"""
Story {story_info.get('story_index', 'Unknown')}: {story_info.get('story_title', 'Unknown')}
  - åˆ†é¡: {story_info.get('category', 'Unknown')}
  - æ–‡ç« æ•¸: {story_info.get('total_articles', 0)}
  - ç¶œåˆå ±å°: {'âœ…' if comprehensive.get('title') else 'âŒ'}"""

            if comprehensive.get('title'):
                content_length = len(comprehensive.get('content', ''))
                summary_content += f"""
  - å ±å°é•·åº¦: {content_length} å­—"""

        try:
            os.makedirs(os.path.dirname(summary_path), exist_ok=True)
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(summary_content)
            logger.info(f"æ‘˜è¦å ±å‘Šå·²ä¿å­˜è‡³ï¼š{summary_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ‘˜è¦å ±å‘Šå¤±æ•—ï¼š{e}")
            
def _json_default(o):
    # ç›¡é‡æŠŠä¸èªè­˜çš„ç‰©ä»¶è½‰æˆå¯å°çš„æ¨£å­
    try:
        from pydantic import BaseModel
        if isinstance(o, BaseModel):
            return o.model_dump()
    except Exception:
        pass
    try:
        return o.__dict__
    except Exception:
        return str(o)

def dump_genai_response(resp, label: str = ""):
    print("\n" + "="*20 + f" [GENAI RESPONSE DUMP: {label}] " + "="*20)
    # 1) æ–‡å­—è¼¸å‡ºï¼ˆè‹¥æœ‰ï¼‰
    try:
        print("[text]:", (resp.text or "").strip())
    except Exception as e:
        print("[text] <error>", e)

    # 2) çµæ§‹åŒ– parsedï¼ˆå¯èƒ½æ˜¯ BaseModel æˆ– list[BaseModel]ï¼‰
    try:
        parsed = resp.parsed
        print("[parsed type]:", type(parsed).__name__)
        if parsed is None:
            print("[parsed]: None")
        elif isinstance(parsed, list):
            print(f"[parsed list] len={len(parsed)}")
            for i, item in enumerate(parsed[:5]):  # æœ€å¤šåˆ— 5 å€‹
                try:
                    from pydantic import BaseModel
                    if isinstance(item, BaseModel):
                        print(f"  - [{i}] (BaseModel)", item.model_dump())
                    else:
                        print(f"  - [{i}]", item)
                except Exception:
                    print(f"  - [{i}]", item)
        else:
            try:
                from pydantic import BaseModel
                if isinstance(parsed, BaseModel):
                    print("[parsed] (BaseModel):", parsed.model_dump())
                else:
                    print("[parsed]:", parsed)
            except Exception:
                print("[parsed]:", parsed)
    except Exception as e:
        print("[parsed] <error>", e)

    # 3) å€™é¸/å®‰å…¨/çµ‚æ­¢åŸå› 
    try:
        cands = getattr(resp, "candidates", None) or []
        print(f"[candidates]: {len(cands)}")
        for idx, c in enumerate(cands):
            fr = getattr(c, "finish_reason", None)
            print(f"  - cand[{idx}].finish_reason =", fr)
            # å…§å®¹é›¶ä»¶
            content = getattr(c, "content", None)
            parts = getattr(content, "parts", None) if content else None
            if parts:
                for j, p in enumerate(parts[:5]):
                    t = getattr(p, "text", None)
                    if t:
                        print(f"    * part[{j}].text:", t[:500])
                    else:
                        print(f"    * part[{j}] (non-text):", _json_default(p))
            # å®‰å…¨è©•åˆ†
            sr = getattr(c, "safety_ratings", None)
            if sr:
                print("    * safety_ratings:", _json_default(sr))
    except Exception as e:
        print("[candidates] <error>", e)

    # 4) prompt feedback / usage
    try:
        pf = getattr(resp, "prompt_feedback", None)
        if pf:
            print("[prompt_feedback]:", _json_default(pf))
    except Exception as e:
        print("[prompt_feedback] <error>", e)

    try:
        usage = getattr(resp, "usage_metadata", None)
        if usage:
            print("[usage_metadata]:", _json_default(usage))
    except Exception as e:
        print("[usage_metadata] <error>", e)

    print("="*20 + " [END DUMP] " + "="*20 + "\n")
