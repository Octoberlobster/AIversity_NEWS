"""æ–°èäº‹ä»¶åˆ†çµ„å™¨ - å®Œæ•´ç‰ˆ
ä½¿ç”¨ google.genai é€²è¡Œæ™ºèƒ½åˆ†çµ„
"""

import os
import sys
import json
import time
import uuid
from datetime import datetime
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸ - è‡ªå‹•æœå°‹ .env æª”æ¡ˆ
def find_env_file():
    """æœå°‹ .env æª”æ¡ˆï¼Œå¾ç•¶å‰ç›®éŒ„é–‹å§‹å¾€ä¸Šå±¤æœå°‹"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æœå°‹ç•¶å‰ç›®éŒ„å’Œä¸Šå±¤ç›®éŒ„
    search_dirs = [
        current_dir,  # ç•¶å‰ç›®éŒ„
        os.path.dirname(current_dir),  # ä¸Šä¸€å±¤
        os.path.dirname(os.path.dirname(current_dir)),  # ä¸ŠäºŒå±¤
        os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'Picture_generate_system'),  # Picture_generate_system
    ]
    
    for directory in search_dirs:
        env_file_path = os.path.join(directory, '.env')
        if os.path.exists(env_file_path):
            print(f"âœ“ æ‰¾åˆ°ç’°å¢ƒè®Šæ•¸æª”æ¡ˆ: {env_file_path}")
            return env_file_path
    
    print("âœ— æœªæ‰¾åˆ° .env æª”æ¡ˆï¼Œè«‹ç¢ºä¿ä»¥ä¸‹ä»»ä¸€ä½ç½®æœ‰ .env æª”æ¡ˆ:")
    for directory in search_dirs:
        print(f"  - {os.path.join(directory, '.env')}")
    return None

env_path = find_env_file()
if env_path:
    load_dotenv(env_path)
else:
    print("ç„¡æ³•è¼‰å…¥ç’°å¢ƒè®Šæ•¸ï¼Œç¨‹å¼çµæŸ")
    sys.exit(1)

SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    print("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š SUPABASE_URL èˆ‡ SUPABASE_KEY")
    sys.exit(1)
if not GEMINI_API_KEY:
    print("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š GEMINI_API_KEY")
    sys.exit(1)

try:
    from supabase import create_client
    print("âœ“ Supabase å¥—ä»¶å·²è¼‰å…¥")
except ImportError:
    print("è«‹å…ˆå®‰è£ supabase-pyï¼špip install supabase-py postgrest-py")
    sys.exit(1)

try:
    import google.genai as genai
    print("âœ“ Google Genai å¥—ä»¶å·²è¼‰å…¥")
except ImportError:
    print("è«‹å…ˆå®‰è£ google genai SDKï¼špip install google-genai")
    sys.exit(1)

class NewsEventGrouper:
    """æ–°èäº‹ä»¶åˆ†çµ„å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®¢æˆ¶ç«¯"""
        self.supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        try:
            self.genai_client = genai.Client(api_key=GEMINI_API_KEY)
            print("âœ“ Gemini Client åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âœ— Gemini Client åˆå§‹åŒ–å¤±æ•—: {e}")
            # ä½¿ç”¨ fallback æ–¹æ³•
            print("åˆ‡æ›åˆ° fallback æ¨¡å¼...")
            self.genai_client = None
        
    def fetch_topic_news_map_from_supabase(self):
        """å¾ Supabase çš„ topic_news_map è¡¨ç²å–ä¸»é¡Œæ–°èæ˜ å°„"""
        try:
            print("é–‹å§‹å¾ topic_news_map è¡¨ç²å–è³‡æ–™...")
            response = self.supabase.table('topic_news_map').select(
                'topic_id, story_id'
            ).execute()
            
            if response.data:
                print(f"âœ“ æˆåŠŸç²å– {len(response.data)} ç­†ä¸»é¡Œæ–°èæ˜ å°„è³‡æ–™")
                return response.data
            else:
                print("âœ— topic_news_map è¡¨ç„¡è³‡æ–™")
                return []
                
        except Exception as e:
            print(f"âœ— ç²å– topic_news_map è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def group_by_topic_id(self, topic_news_map):
        """æ ¹æ“š topic_id å°‡æ–°èåˆ†çµ„"""
        topic_groups = {}
        
        for item in topic_news_map:
            topic_id = item.get('topic_id')
            story_id = item.get('story_id')
            
            if topic_id and story_id:
                if topic_id not in topic_groups:
                    topic_groups[topic_id] = []
                topic_groups[topic_id].append(story_id)
        
        print(f"âœ“ æ ¹æ“š topic_id åˆ†æˆ {len(topic_groups)} å€‹ä¸»é¡Œçµ„")
        for topic_id, story_ids in topic_groups.items():
            print(f"  ä¸»é¡Œ {topic_id}: {len(story_ids)} å‰‡æ–°è")
        
        return topic_groups
    
    def read_story_ids_from_json(self, json_file_path):
        """å¾ JSON æª”æ¡ˆè®€å– story_id åˆ—è¡¨"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            story_ids = []
            if isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        story_id = (item.get('story_id') or 
                                  item.get('id') or 
                                  item.get('storyId'))
                        if story_id:
                            story_ids.append(str(story_id))
            
            print(f"å¾ {json_file_path} è®€å–åˆ° {len(story_ids)} å€‹ story_id")
            return list(set(story_ids))  # å»é‡
            
        except Exception as e:
            print(f"è®€å– JSON æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def fetch_news_from_supabase(self, story_ids):
        """å¾ Supabase ç²å–æ–°èå…§å®¹"""
        news_items = []
        
        print(f"é–‹å§‹å¾ Supabase ç²å– {len(story_ids)} å‰‡æ–°è...")
        
        for i, story_id in enumerate(story_ids, 1):
            try:
                response = self.supabase.table('single_news').select(
                    'story_id, news_title, long'
                ).eq('story_id', story_id).execute()
                
                if response.data and len(response.data) > 0:
                    news_data = response.data[0]
                    news_items.append({
                        'story_id': news_data.get('story_id'),
                        'news_title': news_data.get('news_title', ''),
                        'content': news_data.get('long', '')
                    })
                    print(f"âœ“ {i}/{len(story_ids)}: æˆåŠŸç²å– story_id {story_id}")
                else:
                    print(f"âœ— {i}/{len(story_ids)}: story_id {story_id} æœªæ‰¾åˆ°å°æ‡‰æ–°è")
                    
            except Exception as e:
                print(f"âœ— {i}/{len(story_ids)}: ç²å– story_id {story_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
            # é¿å…è«‹æ±‚éæ–¼é »ç¹
            time.sleep(0.1)
        
        print(f"æˆåŠŸç²å– {len(news_items)} å‰‡æ–°èå…§å®¹")
        return news_items
    
    def group_news_by_events_ai(self, news_items):
        """ä½¿ç”¨ Gemini AI å°‡æ–°èåˆ†çµ„ç‚ºäº‹ä»¶åˆ†æ”¯"""
        if not self.genai_client or not news_items:
            return self.simple_group_news(news_items)
        
        print("é–‹å§‹ä½¿ç”¨ Gemini AI åˆ†ææ–°èäº‹ä»¶...")
        
        # æº–å‚™æ–°èæ‘˜è¦è³‡æ–™ä¾›æ¨¡å‹åˆ†æ
        news_summaries = []
        for i, news in enumerate(news_items):
            title = news['news_title'][:100]  # å¢åŠ æ¨™é¡Œé•·åº¦
            content = news['content'][:300]   # å¢åŠ å…§å®¹é•·åº¦ä»¥æä¾›æ›´å¤šç´°ç¯€
            summary = f"æ–°è{i+1}ï¼šæ¨™é¡Œï¼š{title}ï¼›å…§å®¹ï¼š{content}..."
            news_summaries.append(summary)
        
        # æ§‹å»ºæç¤ºèª
        prompt = f"""
è«‹åˆ†æä»¥ä¸‹ {len(news_items)} å‰‡æ–°èï¼Œå°‡å®ƒå€‘æŒ‰ç…§ä¸»è¦äº‹ä»¶ä¸»é¡Œé€²è¡Œé©ä¸­çš„åˆ†çµ„ã€‚è¦æ±‚åšåˆç†çš„åˆ†é¡ï¼Œé¿å…éæ–¼ç´°ç·»æˆ–éæ–¼ç²—ç³™ã€‚

é‡è¦åˆ†çµ„è¦æ±‚ï¼š

2. æŒ‰ç…§ä¸»è¦äº‹ä»¶ä¸»é¡Œä¾†åˆ†çµ„ï¼Œä½†å¯ä»¥åŒ…å«è©²ä¸»é¡Œçš„ä¸åŒç™¼å±•éšæ®µ
3. æ¯å‰‡æ–°èåªèƒ½åˆ†é…åˆ°ä¸€å€‹åˆ†çµ„ï¼Œä¸å¯é‡è¤‡åˆ†é…
4. å°‹æ‰¾æ–°èé–“çš„ä¸»è¦é—œè¯æ€§ï¼Œé©åº¦åˆ†çµ„

æ–°èè³‡æ–™ï¼š
{chr(1000).join(news_summaries)}

è«‹æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºåˆ†çµ„çµæœï¼š
{{
  "groups": [
    {{
      "event_title": "ä¸»è¦äº‹ä»¶çš„æ¨™é¡Œï¼ˆ18å­—ä»¥å…§ï¼‰",
      "event_summary": "è©²äº‹ä»¶ä¸»é¡Œçš„æ¦‚è¦èªªæ˜ï¼ˆ100å­—ä»¥å…§ï¼‰",
      "news_indices": [1, 2, 3]
    }},
    {{
      "event_title": "å¦ä¸€å€‹ä¸»è¦äº‹ä»¶çš„æ¨™é¡Œ",
      "event_summary": "å¦ä¸€å€‹äº‹ä»¶ä¸»é¡Œçš„æ¦‚è¦èªªæ˜",
      "news_indices": [4, 5, 6, 7]
    }}
  ]
}}

åˆ†çµ„åŸå‰‡ï¼š
1. ä»¥ä¸»è¦äº‹ä»¶æˆ–æ”¿ç­–ç‚ºæ ¸å¿ƒåˆ†çµ„
2. åŒä¸€äº‹ä»¶çš„ä¸åŒç™¼å±•éšæ®µå¯ä»¥æ”¾åœ¨åŒä¸€çµ„

4. äº‹ä»¶æ¨™é¡Œè¦èƒ½æ¶µè“‹çµ„å…§æ‰€æœ‰æ–°èçš„å…±åŒä¸»é¡Œ
5. news_indices å°æ‡‰æ–°èçš„ç·¨è™Ÿï¼ˆå¾1é–‹å§‹ï¼‰
6. ç¢ºä¿æ‰€æœ‰æ–°èéƒ½è¢«åˆ†é…åˆ°æŸå€‹åˆ†çµ„
7. åªå›å‚³ JSONï¼Œä¸è¦å…¶ä»–èªªæ˜æ–‡å­—

"""

        try:
            response = self.genai_client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt
            )
            result_text = response.text.strip()
            
            # è§£æ JSON çµæœ
            if result_text.startswith('```json'):
                result_text = result_text[7:-3].strip()
            elif result_text.startswith('```'):
                result_text = result_text[3:-3].strip()
            
            result = json.loads(result_text)
            groups = result.get('groups', [])
            
            print(f"AI åˆ†æå®Œæˆï¼Œå…±åˆ†ç‚º {len(groups)} å€‹äº‹ä»¶åˆ†æ”¯")
            
            # è½‰æ›ç‚ºæœ€çµ‚æ ¼å¼ï¼Œä¸¦ç¢ºä¿æ¯å‰‡æ–°èåªå‡ºç¾åœ¨ä¸€å€‹åˆ†æ”¯
            event_groups = []
            used_news_indices = set()  # è¿½è¹¤å·²ä½¿ç”¨çš„æ–°èç´¢å¼•
            
            for group in groups:
                event_title = group.get('event_title', 'æœªå‘½åäº‹ä»¶')
                event_summary = group.get('event_summary', '')
                news_indices = group.get('news_indices', [])
                
                # ç²å–å°æ‡‰çš„æ–°èé …ç›®ï¼Œæ’é™¤å·²ä½¿ç”¨çš„æ–°è
                group_news = []
                for idx in news_indices:
                    if 1 <= idx <= len(news_items) and idx not in used_news_indices:
                        group_news.append(news_items[idx - 1])  # è½‰æ›ç‚º0-basedç´¢å¼•
                        used_news_indices.add(idx)  # æ¨™è¨˜ç‚ºå·²ä½¿ç”¨
                
                if group_news:  # åªæ·»åŠ æœ‰æ–°èçš„åˆ†çµ„
                    event_groups.append({
                        'event_id': str(uuid.uuid4()),
                        'event_title': event_title,
                        'event_summary': event_summary,
                        'news_count': len(group_news),
                        'news_items': group_news
                    })
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æœªåˆ†é…çš„æ–°è
            all_indices = set(range(1, len(news_items) + 1))
            unused_indices = all_indices - used_news_indices
            
            if unused_indices:
                # å°‡æœªåˆ†é…çš„æ–°èå‰µå»ºç‚ºä¸€å€‹é¡å¤–çš„åˆ†çµ„
                unused_news = [news_items[idx - 1] for idx in unused_indices]
                event_groups.append({
                    'event_id': str(uuid.uuid4()),
                    'event_title': 'å…¶ä»–ç›¸é—œæ–°è',
                    'event_summary': f'åŒ…å« {len(unused_news)} å‰‡æœªè¢«å…¶ä»–åˆ†æ”¯åŒ…å«çš„ç›¸é—œæ–°è',
                    'news_count': len(unused_news),
                    'news_items': unused_news
                })
                print(f"æ³¨æ„ï¼šæœ‰ {len(unused_indices)} å‰‡æ–°èæœªè¢« AI åˆ†çµ„ï¼Œå·²è‡ªå‹•å‰µå»ºã€Œå…¶ä»–ç›¸é—œæ–°èã€åˆ†æ”¯")
            
            return event_groups
            
        except Exception as e:
            print(f"AI åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("åˆ‡æ›åˆ°ç°¡å–®åˆ†çµ„æ¨¡å¼...")
            return self.simple_group_news(news_items)
    
    def simple_group_news(self, news_items):
        """ç°¡å–®çš„æ–°èåˆ†çµ„ï¼ˆä¸ä½¿ç”¨ AIï¼‰"""
        return [{
            'event_id': str(uuid.uuid4()),
            'event_title': 'ç¶œåˆæ–°èäº‹ä»¶',
            'event_summary': f'åŒ…å« {len(news_items)} å‰‡ç›¸é—œæ–°èçš„ç¶œåˆäº‹ä»¶',
            'news_count': len(news_items),
            'news_items': news_items
        }]
    
    def save_to_database(self, event_groups, save_mode="both"):
        """å°‡äº‹ä»¶åˆ†æ”¯å’Œæ–°èæ˜ å°„å­˜å…¥è³‡æ–™åº«
        
        Args:
            event_groups: è™•ç†å¥½çš„äº‹ä»¶åˆ†çµ„è³‡æ–™
            save_mode: å„²å­˜æ¨¡å¼ - "preview"(åƒ…é è¦½), "database"(åƒ…è³‡æ–™åº«), "both"(é è¦½+è³‡æ–™åº«)
        """
        try:
            print(f"\né–‹å§‹è³‡æ–™åº«å„²å­˜æµç¨‹ (æ¨¡å¼: {save_mode})...")
            
            # æº–å‚™å…©å€‹è³‡æ–™è¡¨çš„è³‡æ–™
            topic_branch_news_map_data = []
            topic_branch_data = []
            
            for topic_group in event_groups:
                topic_id = topic_group.get('topic_id')
                sub_events = topic_group.get('sub_events', [])
                
                for sub_event in sub_events:
                    topic_branch_id = sub_event.get('event_id')
                    topic_branch_title = sub_event.get('event_title')
                    topic_branch_content = sub_event.get('event_summary')
                    news_items = sub_event.get('news_items', [])
                    
                    # 1. æº–å‚™ topic_branch è³‡æ–™
                    if topic_id and topic_branch_id and topic_branch_title:
                        topic_branch_data.append({
                            'topic_id': topic_id,
                            'topic_branch_id': topic_branch_id,
                            'topic_branch_title': topic_branch_title,
                            'topic_branch_content': topic_branch_content or ''
                        })
                    
                    # 2. æº–å‚™ topic_branch_news_map è³‡æ–™
                    for news_item in news_items:
                        story_id = news_item.get('story_id')
                        if topic_branch_id and story_id:
                            topic_branch_news_map_data.append({
                                'topic_branch_id': topic_branch_id,
                                'story_id': story_id
                            })
            
            print(f"æº–å‚™è³‡æ–™: {len(topic_branch_data)} å€‹åˆ†æ”¯, {len(topic_branch_news_map_data)} ç­†æ–°èå°æ‡‰")
            
            # æ ¹æ“šæ¨¡å¼åŸ·è¡Œç›¸æ‡‰æ“ä½œ
            if save_mode in ["preview", "both"]:
                self._save_database_preview(topic_branch_data, topic_branch_news_map_data)
            
            if save_mode in ["database", "both"]:
                self._save_to_actual_database(topic_branch_data, topic_branch_news_map_data)
                
        except Exception as e:
            print(f"âœ— è³‡æ–™åº«å„²å­˜æµç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _save_database_preview(self, topic_branch_data, topic_branch_news_map_data):
        """å„²å­˜è³‡æ–™åº«é è¦½æª”æ¡ˆ"""
        try:
            print("\n--- ç”Ÿæˆè³‡æ–™åº«é è¦½æª”æ¡ˆ ---")
            
            # å„²å­˜ topic_branch é è¦½
            topic_branch_file = "database_preview_topic_branch.json"
            with open(topic_branch_file, 'w', encoding='utf-8') as f:
                json.dump(topic_branch_data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ topic_branch é è¦½å·²å„²å­˜: {topic_branch_file}")
            print(f"  å…± {len(topic_branch_data)} å€‹ä¸»é¡Œåˆ†æ”¯")
            
            # å„²å­˜ topic_branch_news_map é è¦½
            topic_branch_news_map_file = "database_preview_topic_branch_news_map.json"
            with open(topic_branch_news_map_file, 'w', encoding='utf-8') as f:
                json.dump(topic_branch_news_map_data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ topic_branch_news_map é è¦½å·²å„²å­˜: {topic_branch_news_map_file}")
            print(f"  å…± {len(topic_branch_news_map_data)} ç­†å°æ‡‰é—œä¿‚")
            
            # é¡¯ç¤ºç¯„ä¾‹
            print("\nã€topic_branch ç¯„ä¾‹ã€‘")
            for i, item in enumerate(topic_branch_data[:3], 1):
                print(f"{i}. topic_id: {item['topic_id']}")
                print(f"   topic_branch_id: {item['topic_branch_id']}")
                print(f"   topic_branch_title: {item['topic_branch_title']}")
                print(f"   topic_branch_content: {item['topic_branch_content'][:50]}...")
                print()
            
            print("ã€topic_branch_news_map ç¯„ä¾‹ã€‘")
            for i, item in enumerate(topic_branch_news_map_data[:5], 1):
                print(f"{i}. {item['topic_branch_id']} <-> {item['story_id']}")
            
            if len(topic_branch_news_map_data) > 5:
                print(f"... é‚„æœ‰ {len(topic_branch_news_map_data) - 5} ç­†è³‡æ–™")
            
        except Exception as e:
            print(f"âœ— ç”Ÿæˆé è¦½æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _save_to_actual_database(self, topic_branch_data, topic_branch_news_map_data):
        """å„²å­˜åˆ°å¯¦éš›è³‡æ–™åº«"""
        try:
            print("\n--- é–‹å§‹å„²å­˜åˆ°å¯¦éš›è³‡æ–™åº« ---")
            
            # å„²å­˜ topic_branch è³‡æ–™
            print("1. å„²å­˜ topic_branch è³‡æ–™...")
            if topic_branch_data:
                # æ¸…é™¤ç¾æœ‰è³‡æ–™ï¼ˆå¯é¸ - æ ¹æ“šéœ€æ±‚æ±ºå®šï¼‰
                # self.supabase.table('topic_branch').delete().neq('topic_id', '').execute()
                
                batch_size = 50
                success_count = 0
                
                for i in range(0, len(topic_branch_data), batch_size):
                    batch = topic_branch_data[i:i + batch_size]
                    try:
                        response = self.supabase.table('topic_branch').upsert(batch).execute()
                        if response.data:
                            success_count += len(batch)
                            print(f"   âœ“ topic_branch ç¬¬ {i//batch_size + 1} æ‰¹ ({len(batch)} ç­†)")
                        else:
                            print(f"   âœ— topic_branch ç¬¬ {i//batch_size + 1} æ‰¹æ’å…¥å¤±æ•—")
                    except Exception as e:
                        print(f"   âœ— topic_branch ç¬¬ {i//batch_size + 1} æ‰¹ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                print(f"   â†’ topic_branch æˆåŠŸå„²å­˜: {success_count}/{len(topic_branch_data)} ç­†")
            
            # å„²å­˜ topic_branch_news_map è³‡æ–™
            print("2. å„²å­˜ topic_branch_news_map è³‡æ–™...")
            if topic_branch_news_map_data:
                # æ¸…é™¤ç¾æœ‰è³‡æ–™ï¼ˆå¯é¸ï¼‰
                # self.supabase.table('topic_branch_news_map').delete().neq('topic_branch_id', '').execute()
                
                batch_size = 100
                success_count = 0
                
                for i in range(0, len(topic_branch_news_map_data), batch_size):
                    batch = topic_branch_news_map_data[i:i + batch_size]
                    try:
                        response = self.supabase.table('topic_branch_news_map').upsert(batch).execute()
                        if response.data:
                            success_count += len(batch)
                            print(f"   âœ“ topic_branch_news_map ç¬¬ {i//batch_size + 1} æ‰¹ ({len(batch)} ç­†)")
                        else:
                            print(f"   âœ— topic_branch_news_map ç¬¬ {i//batch_size + 1} æ‰¹æ’å…¥å¤±æ•—")
                    except Exception as e:
                        print(f"   âœ— topic_branch_news_map ç¬¬ {i//batch_size + 1} æ‰¹ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                print(f"   â†’ topic_branch_news_map æˆåŠŸå„²å­˜: {success_count}/{len(topic_branch_news_map_data)} ç­†")
            
            print("\nâœ… è³‡æ–™åº«å„²å­˜å®Œæˆï¼")
            
        except Exception as e:
            print(f"âœ— å„²å­˜åˆ°å¯¦éš›è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def save_to_json(self, event_groups, output_path):
        """å„²å­˜çµæœåˆ° JSON æª”æ¡ˆ"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(event_groups, f, ensure_ascii=False, indent=2)
            print(f"çµæœå·²å„²å­˜è‡³: {output_path}")
        except Exception as e:
            print(f"å„²å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def process_from_topic_map(self, output_path, save_to_db=True):
        """ä½¿ç”¨ topic_news_map çš„è™•ç†æµç¨‹
        
        Args:
            output_path: JSON è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            save_to_db: æ˜¯å¦å„²å­˜åˆ°è³‡æ–™åº« (True=å„²å­˜, False=åƒ…é è¦½)
        """
        print("=" * 60)
        print("æ–°èäº‹ä»¶åˆ†çµ„å™¨ - å¾ topic_news_map é–‹å§‹è™•ç†")
        print("=" * 60)
        
        # 1. å¾ topic_news_map ç²å–è³‡æ–™
        topic_news_map = self.fetch_topic_news_map_from_supabase()
        if not topic_news_map:
            print("æœªç²å–åˆ° topic_news_map è³‡æ–™ï¼Œç¨‹å¼çµæŸ")
            return
        
        # 2. æ ¹æ“š topic_id åˆ†çµ„
        topic_groups = self.group_by_topic_id(topic_news_map)
        if not topic_groups:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ä¸»é¡Œåˆ†çµ„ï¼Œç¨‹å¼çµæŸ")
            return
        
        # 3. ç‚ºæ¯å€‹ä¸»é¡Œçµ„ç²å–æ–°èå…§å®¹ï¼Œç„¶å¾Œå†ç”¨ AI åšç´°åˆ†
        all_topic_events = []
        
        for topic_id, story_ids in topic_groups.items():
            print(f"\nè™•ç†ä¸»é¡Œ {topic_id} ({len(story_ids)} å‰‡æ–°è)...")
            
            # ç²å–è©²ä¸»é¡Œçš„æ–°èå…§å®¹
            news_items = self.fetch_news_from_supabase(story_ids)
            
            if not news_items:
                print(f"âœ— ä¸»é¡Œ {topic_id}: æœªç²å–åˆ°æœ‰æ•ˆæ–°èå…§å®¹")
                continue
            
            # ç‚ºè©²ä¸»é¡Œç”Ÿæˆç¸½é«”æ¨™é¡Œ
            topic_title = self.generate_topic_title(news_items)
            print(f"âœ“ ä¸»é¡Œ {topic_id}: {topic_title}")
            
            # å¦‚æœæ–°èæ•¸é‡è¼ƒå°‘ï¼ˆ<=3å‰‡ï¼‰ï¼Œç›´æ¥ä½œç‚ºä¸€å€‹åˆ†æ”¯
            if len(news_items) <= 3:
                topic_summary = self.generate_topic_summary(news_items)
                topic_event = {
                    'topic_id': topic_id,
                    'topic_title': topic_title,
                    'sub_events': [
                        {
                            'event_id': str(uuid.uuid4()),
                            'event_title': topic_title,
                            'event_summary': topic_summary,
                            'news_count': len(news_items),
                            'news_items': news_items
                        }
                    ]
                }
                all_topic_events.append(topic_event)
                print(f"  â†’ å–®ä¸€åˆ†æ”¯: {topic_title} ({len(news_items)} å‰‡æ–°è)")
            
            else:
                # æ–°èæ•¸é‡è¼ƒå¤šï¼Œä½¿ç”¨ AI é€²è¡Œç´°åˆ†
                print(f"  æ­£åœ¨å° {len(news_items)} å‰‡æ–°èé€²è¡Œ AI ç´°åˆ†...")
                sub_events = self.group_news_by_events_ai(news_items)
                
                # ç‚ºæ¯å€‹å­äº‹ä»¶æ·»åŠ  topic ç›¸é—œè³‡è¨Š
                for sub_event in sub_events:
                    sub_event['topic_id'] = topic_id
                
                topic_event = {
                    'topic_id': topic_id,
                    'topic_title': topic_title,
                    'sub_events': sub_events
                }
                all_topic_events.append(topic_event)
                
                print(f"  â†’ ç´°åˆ†ç‚º {len(sub_events)} å€‹åˆ†æ”¯:")
                for i, sub_event in enumerate(sub_events, 1):
                    print(f"    åˆ†æ”¯ {i}: {sub_event['event_title']} ({sub_event['news_count']} å‰‡æ–°è)")
        
        # 4. å„²å­˜çµæœåˆ° JSON
        self.save_to_json(all_topic_events, output_path)
        
        # 5. å„²å­˜åˆ°è³‡æ–™åº«æˆ–ç”Ÿæˆé è¦½
        if save_to_db:
            save_mode = "both"  # åŒæ™‚ç”Ÿæˆé è¦½å’Œå„²å­˜åˆ°è³‡æ–™åº«
            print("\nå°‡åŒæ™‚ç”Ÿæˆé è¦½æª”æ¡ˆä¸¦å„²å­˜åˆ°è³‡æ–™åº«...")
        else:
            save_mode = "preview"  # åƒ…ç”Ÿæˆé è¦½
            print("\nåƒ…ç”Ÿæˆè³‡æ–™åº«é è¦½æª”æ¡ˆ...")
        
        self.save_to_database(all_topic_events, save_mode)
        
        # 6. è¼¸å‡ºçµ±è¨ˆè³‡è¨Š
        print("\n" + "=" * 60)
        print("è™•ç†å®Œæˆ - çµ±è¨ˆè³‡è¨Š")
        print("=" * 60)
        print(f"ä¸»é¡Œæ•¸é‡: {len(topic_groups)}")
        print(f"æˆåŠŸè™•ç†çš„ä¸»é¡Œ: {len(all_topic_events)}")
        
        total_sub_events = sum(len(topic['sub_events']) for topic in all_topic_events)
        total_news = sum(
            sum(sub_event['news_count'] for sub_event in topic['sub_events'])
            for topic in all_topic_events
        )
        print(f"ç¸½åˆ†æ”¯æ•¸é‡: {total_sub_events}")
        print(f"ç¸½æ–°èæ•¸é‡: {total_news}")
        
        for i, topic in enumerate(all_topic_events, 1):
            print(f"\nä¸»é¡Œ {i}: {topic['topic_title']} (ID: {topic['topic_id']})")
            for j, sub_event in enumerate(topic['sub_events'], 1):
                print(f"  åˆ†æ”¯ {j}: {sub_event['event_title']} ({sub_event['news_count']} å‰‡æ–°è)")
        
        return all_topic_events
    
    def generate_topic_title(self, news_items):
        """ç‚ºæ•´å€‹ä¸»é¡Œç”Ÿæˆæ¨™é¡Œ"""
        if not self.genai_client or not news_items:
            return f"ä¸»é¡Œäº‹ä»¶ ({len(news_items)} å‰‡æ–°è)"
        
        # å–å‰3å‰‡æ–°èçš„æ¨™é¡Œå’Œå…§å®¹ç‰‡æ®µ
        sample_news = []
        for i, news in enumerate(news_items[:3], 1):
            title = news['news_title'][:50]
            content = news['content'][:80]
            sample_news.append(f"æ–°è{i}: {title} - {content}...")
        
        prompt = f"""
åŸºæ–¼ä»¥ä¸‹ {len(news_items)} å‰‡æ–°èï¼Œç”Ÿæˆä¸€å€‹ç°¡æ½”çš„ä¸»é¡Œæ¨™é¡Œã€‚

ç¯„ä¾‹æ–°èï¼š
{chr(10).join(sample_news)}

è«‹ç”Ÿæˆä¸€å€‹15å­—ä»¥å…§çš„ä¸»é¡Œæ¨™é¡Œï¼Œåªå›å‚³æ¨™é¡Œæ–‡å­—ï¼Œä¸è¦å…¶ä»–å…§å®¹ã€‚
"""
        
        try:
            response = self.genai_client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt
            )
            title = response.text.strip().replace('"', '').replace("'", '')
            return title if len(title) <= 20 else title[:17] + "..."
            
        except Exception as e:
            print(f"  âœ— AI ç”Ÿæˆä¸»é¡Œæ¨™é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"ä¸»é¡Œäº‹ä»¶ ({len(news_items)} å‰‡æ–°è)"
    
    def generate_topic_summary(self, news_items):
        """ç‚ºä¸»é¡Œç”Ÿæˆæ¦‚è¦"""
        if not self.genai_client or not news_items:
            return f"åŒ…å« {len(news_items)} å‰‡ç›¸é—œæ–°èçš„ä¸»é¡Œäº‹ä»¶"
        
        # å–å‰3å‰‡æ–°èçš„å…§å®¹
        sample_news = []
        for i, news in enumerate(news_items[:3], 1):
            content = news['content'][:100]
            sample_news.append(f"æ–°è{i}: {content}...")
        
        prompt = f"""
åŸºæ–¼ä»¥ä¸‹æ–°èå…§å®¹ï¼Œç”Ÿæˆä¸€å€‹80å­—ä»¥å…§çš„äº‹ä»¶æ¦‚è¦ã€‚

æ–°èå…§å®¹ï¼š
{chr(1000).join(sample_news)}

è«‹ç”Ÿæˆç°¡æ½”çš„æ¦‚è¦èªªæ˜ï¼Œåªå›å‚³æ¦‚è¦æ–‡å­—ï¼Œä¸è¦å…¶ä»–å…§å®¹ã€‚
"""
        
        try:
            response = self.genai_client.models.generate_content(
                model='gemini-1.5-flash',
                contents=prompt
            )
            summary = response.text.strip()
            return summary if len(summary) <= 100 else summary[:97] + "..."
            
        except Exception as e:
            print(f"  âœ— AI ç”Ÿæˆæ¦‚è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"åŒ…å« {len(news_items)} å‰‡ç›¸é—œæ–°èçš„ä¸»é¡Œäº‹ä»¶"
    
    def generate_topic_title_and_summary(self, news_items):
        """ä½¿ç”¨ AI ç‚ºä¸»é¡Œç”Ÿæˆæ¨™é¡Œå’Œæ¦‚è¦"""
        if not self.genai_client or not news_items:
            return f"ä¸»é¡Œäº‹ä»¶ ({len(news_items)} å‰‡æ–°è)", f"åŒ…å« {len(news_items)} å‰‡ç›¸é—œæ–°èçš„ä¸»é¡Œäº‹ä»¶"
        
        # æº–å‚™æ–°èæ‘˜è¦
        news_summaries = []
        for i, news in enumerate(news_items[:5], 1):  # æœ€å¤šå–å‰5å‰‡æ–°èåˆ†æ
            title = news['news_title'][:60]
            content = news['content'][:100]
            news_summaries.append(f"æ–°è{i}: {title} - {content}...")
        
        prompt = f"""
åŸºæ–¼ä»¥ä¸‹ {len(news_items)} å‰‡æ–°èï¼Œç”Ÿæˆä¸€å€‹ç°¡æ½”çš„äº‹ä»¶æ¨™é¡Œå’Œæ¦‚è¦ã€‚

æ–°èå…§å®¹ï¼š
{chr(10).join(news_summaries)}

è«‹ä»¥ JSON æ ¼å¼å›å‚³ï¼š
{{
  "title": "ç°¡æ½”çš„äº‹ä»¶æ¨™é¡Œï¼ˆ15å­—ä»¥å…§ï¼‰",
  "summary": "äº‹ä»¶æ¦‚è¦èªªæ˜ï¼ˆ80å­—ä»¥å…§ï¼‰"
}}

åªå›å‚³ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = self.genai_client.models.generate_content(
                model='gemini-1.5-flash',
                contents=prompt
            )
            result_text = response.text.strip()
            
            # æ¸…ç† JSON æ ¼å¼
            if result_text.startswith('```json'):
                result_text = result_text[7:-3].strip()
            elif result_text.startswith('```'):
                result_text = result_text[3:-3].strip()
            
            result = json.loads(result_text)
            return result.get('title', 'ä¸»é¡Œäº‹ä»¶'), result.get('summary', f'åŒ…å« {len(news_items)} å‰‡ç›¸é—œæ–°è')
            
        except Exception as e:
            print(f"âœ— AI ç”Ÿæˆæ¨™é¡Œå’Œæ¦‚è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"ä¸»é¡Œäº‹ä»¶ ({len(news_items)} å‰‡æ–°è)", f"åŒ…å« {len(news_items)} å‰‡ç›¸é—œæ–°èçš„ä¸»é¡Œäº‹ä»¶"
    
    def process(self, json_file_path, output_path):
        """å®Œæ•´è™•ç†æµç¨‹ï¼ˆåŸç‰ˆ - å¾JSONæª”æ¡ˆé–‹å§‹ï¼‰"""
        print("=" * 60)
        print("æ–°èäº‹ä»¶åˆ†çµ„å™¨ - é–‹å§‹è™•ç†")
        print("=" * 60)
        
        # 1. è®€å– story_id
        story_ids = self.read_story_ids_from_json(json_file_path)
        if not story_ids:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ story_idï¼Œç¨‹å¼çµæŸ")
            return
        
        # 2. å¾ Supabase ç²å–æ–°è
        news_items = self.fetch_news_from_supabase(story_ids)
        if not news_items:
            print("æœªç²å–åˆ°ä»»ä½•æ–°èå…§å®¹ï¼Œç¨‹å¼çµæŸ")
            return
        
        # 3. ä½¿ç”¨ AI åˆ†çµ„
        event_groups = self.group_news_by_events_ai(news_items)
        
        # 4. å„²å­˜çµæœåˆ° JSON æª”æ¡ˆ
        self.save_to_json(event_groups, output_path)
        
        # 5. å„²å­˜çµæœåˆ°è³‡æ–™åº«
        self.save_to_database(event_groups)
        
        # 5. è¼¸å‡ºçµ±è¨ˆè³‡è¨Š
        print("\n" + "=" * 60)
        print("è™•ç†å®Œæˆ - çµ±è¨ˆè³‡è¨Š")
        print("=" * 60)
        print(f"åŸå§‹ story_id æ•¸é‡: {len(story_ids)}")
        print(f"æˆåŠŸç²å–æ–°èæ•¸é‡: {len(news_items)}")
        print(f"åˆ†çµ„å¾Œäº‹ä»¶åˆ†æ”¯æ•¸é‡: {len(event_groups)}")
        
        for i, group in enumerate(event_groups, 1):
            print(f"  åˆ†æ”¯ {i}: {group['event_title']} ({group['news_count']} å‰‡æ–°è)")


def main():
    """ä¸»ç¨‹å¼å…¥å£ - ç›´æ¥åŸ·è¡Œå³å¯"""
    print("ğŸš€ æ–°èäº‹ä»¶åˆ†çµ„å™¨ - å•Ÿå‹•ä¸­...")
    print("ğŸ’¾ æ¨¡å¼ï¼šå¾ topic_news_map è®€å–è³‡æ–™ï¼ŒAIåˆ†çµ„å¾Œå„²å­˜åˆ°è³‡æ–™åº«")
    print("=" * 60)
    
    # å‰µå»ºè™•ç†å™¨
    try:
        grouper = NewsEventGrouper()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        print("è«‹æª¢æŸ¥ç’°å¢ƒè¨­å®šå’Œç¶²è·¯é€£ç·š")
        return
    
    # è¨­å®šè¼¸å‡ºæª”æ¡ˆåç¨±ï¼ˆåŒ…å«æ™‚é–“æˆ³è¨˜ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"topic_grouped_news_{timestamp}.json"
    
    # é è¨­æ¨¡å¼ï¼šå¾ topic_news_map è™•ç†ä¸¦å„²å­˜åˆ°è³‡æ–™åº«
    save_to_db = True
    
    print(f"ğŸ“„ çµæœå°‡å„²å­˜åˆ°: {output_path}")
    print("ğŸ’¾ å°‡åŒæ™‚ç”Ÿæˆé è¦½æª”æ¡ˆä¸¦å„²å­˜åˆ°è³‡æ–™åº«")
    print()
    
    try:
        # åŸ·è¡Œä¸»è¦è™•ç†æµç¨‹
        result = grouper.process_from_topic_map(output_path, save_to_db)
        
        if result:
            print("\nğŸ‰ è™•ç†å®Œæˆï¼")
            print(f"âœ… JSON æª”æ¡ˆ: {output_path}")
            print("âœ… è³‡æ–™åº«é è¦½æª”æ¡ˆ: database_preview_*.json")
            print("âœ… è³‡æ–™å·²å„²å­˜åˆ° Supabase è³‡æ–™åº«")
        else:
            print("\nâš ï¸ è™•ç†éç¨‹ä¸­é‡åˆ°å•é¡Œï¼Œè«‹æª¢æŸ¥è¼¸å‡ºè¨Šæ¯")
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ä½¿ç”¨è€…ä¸­æ–·ç¨‹å¼åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("è«‹æª¢æŸ¥ç¶²è·¯é€£ç·šå’Œè³‡æ–™åº«è¨­å®š")
    
    print("\nç¨‹å¼çµæŸ")


if __name__ == "__main__":
    main()
