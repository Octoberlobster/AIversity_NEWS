"""
æ–°èäº‹ä»¶åˆ†çµ„å™¨ - å®Œæ•´ç‰ˆ
- ä½¿ç”¨ google.genai é€²è¡Œæ™ºèƒ½åˆ†çµ„ (èªæ„å‘é‡ + DBSCAN åˆ†ç¾¤)
- æ•´åˆ Supabase è³‡æ–™åº«è®€å¯«
"""

import os
import sys
import json
import time
import uuid
from datetime import datetime
from dotenv import load_dotenv

# è¼‰å…¥ .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸
load_dotenv()

# --- ç’°å¢ƒè®Šæ•¸è¨­å®š ---
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
print(f"Gemini API Key Loaded: {'Yes' if GEMINI_API_KEY else 'No'}")

# --- ç’°å¢ƒæª¢æŸ¥ ---
if not SUPABASE_URL or not SUPABASE_KEY:
    print("éŒ¯èª¤ï¼šè«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š SUPABASE_URL èˆ‡ SUPABASE_KEY")
    sys.exit(1)
if not GEMINI_API_KEY:
    print("éŒ¯èª¤ï¼šè«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š GEMINI_API_KEY")
    sys.exit(1)

# --- å¥—ä»¶è¼‰å…¥èˆ‡æª¢æŸ¥ ---
try:
    from supabase import create_client
    print("âœ“ Supabase å¥—ä»¶å·²è¼‰å…¥")
except ImportError:
    print("éŒ¯èª¤ï¼šè«‹å…ˆå®‰è£ supabase-pyï¼špip install supabase")
    sys.exit(1)

try:
    import google.genai as genai
    print("âœ“ Google Genai å¥—ä»¶å·²è¼‰å…¥")
except ImportError:
    print("éŒ¯èª¤ï¼šè«‹å…ˆå®‰è£ google-genai SDKï¼špip install google-generativeai")
    sys.exit(1)

try:
    import numpy as np
    from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering
    from sklearn.metrics import silhouette_score, calinski_harabasz_score
    from sklearn.metrics.pairwise import cosine_similarity
    print("âœ“ Scikit-learn å’Œ NumPy å¥—ä»¶å·²è¼‰å…¥")
except ImportError:
    print("éŒ¯èª¤ï¼šè«‹å…ˆå®‰è£ scikit-learn å’Œ numpyï¼špip install scikit-learn numpy")
    sys.exit(1)


class NewsEventGrouper:
    """æ–°èäº‹ä»¶åˆ†çµ„å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å®¢æˆ¶ç«¯"""
        self.supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        try:
            self.genai_client = genai.Client(api_key=GEMINI_API_KEY)
            self.use_ai = True  # è¨­å®š AI åŠŸèƒ½å¯ç”¨æ——æ¨™
            print("âœ“ Gemini Client åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âœ— Gemini Client åˆå§‹åŒ–å¤±æ•—: {e}")
            print("åˆ‡æ›åˆ° fallback æ¨¡å¼...")
            self.genai_client = None
            self.use_ai = False # è¨­å®š AI åŠŸèƒ½ä¸å¯ç”¨æ——æ¨™

    def fetch_topic_news_map_from_supabase(self):
        """å¾ Supabase çš„ topic_news_map è¡¨ç²å–ä¸»é¡Œæ–°èæ˜ å°„"""
        try:
            response = self.supabase.table('topic_news_map').select(
                'topic_id, story_id'
            ).execute()
            
            if response.data:
                print(f"âœ“ æˆåŠŸç²å– {len(response.data)} ç­†ä¸»é¡Œæ–°èæ˜ å°„è³‡æ–™")
                return response.data
            else:
                print("âœ— topic_news_map è¡¨ç„¡è³‡æ–™")
                return []
        except Exception as e:
            print(f"âœ— ç²å– topic_news_map è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def group_by_topic_id(self, topic_news_map):
        """æ ¹æ“š topic_id å°‡æ–°èåˆ†çµ„"""
        topic_groups = {}
        for item in topic_news_map:
            topic_id = item.get('topic_id')
            story_id = item.get('story_id')
            if topic_id and story_id:
                if topic_id not in topic_groups:
                    topic_groups[topic_id] = []
                topic_groups[topic_id].append(story_id)
        
        print(f"âœ“ æ ¹æ“š topic_id åˆ†æˆ {len(topic_groups)} å€‹ä¸»é¡Œçµ„")
        for topic_id, story_ids in topic_groups.items():
            print(f"  ä¸»é¡Œ {topic_id}: {len(story_ids)} å‰‡æ–°è")
        return topic_groups
    
    def fetch_news_from_supabase(self, story_ids):
        """å¾ Supabase æ‰¹æ¬¡ç²å–æ–°èå…§å®¹"""
        news_items = []
        if not story_ids:
            return news_items
            
        print(f"é–‹å§‹å¾ Supabase æ‰¹æ¬¡ç²å– {len(story_ids)} å‰‡æ–°è...")
        try:
            response = self.supabase.table('single_news').select(
                'story_id, news_title, long'
            ).in_('story_id', story_ids).execute()

            if response.data:
                id_to_news_map = {item['story_id']: item for item in response.data}
                # æŒ‰ç…§å‚³å…¥çš„ story_ids é †åºé‡çµ„ï¼Œç¢ºä¿èˆ‡å‘é‡é †åºä¸€è‡´
                for story_id in story_ids:
                    if story_id in id_to_news_map:
                        news_data = id_to_news_map[story_id]
                        news_items.append({
                            'story_id': news_data.get('story_id'),
                            'news_title': news_data.get('news_title', ''),
                            'content': news_data.get('long', '')
                        })
                    else:
                         print(f"âœ— story_id {story_id} åœ¨ single_news è¡¨ä¸­æœªæ‰¾åˆ°")
            
            print(f"âœ“ æˆåŠŸç²å– {len(news_items)}/{len(story_ids)} å‰‡æ–°èå…§å®¹")
        except Exception as e:
            print(f"âœ— æ‰¹æ¬¡ç²å–æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return news_items

    def group_news_by_vectors(self, news_items):
        """ä½¿ç”¨èªæ„å‘é‡å’ŒDBSCANåˆ†ç¾¤æ¼”ç®—æ³•å°‡æ–°èåˆ†çµ„ç‚ºäº‹ä»¶åˆ†æ”¯"""
        if not self.use_ai or len(news_items) < 2:
            print("AIå®¢æˆ¶ç«¯æœªåˆå§‹åŒ–æˆ–æ–°èæ•¸é‡ä¸è¶³ï¼Œåˆ‡æ›åˆ°ç°¡å–®åˆ†çµ„æ¨¡å¼...")
            return self.simple_group_news(news_items)
            
        print("æ­¥é©Ÿ 1/3: é–‹å§‹ä½¿ç”¨ Gemini AI ç”¢ç”Ÿæ–°èèªæ„å‘é‡...")
        texts_to_embed = [
            f"{news['news_title']}\n{news['content'][:500]}" for news in news_items
        ]
        
        try:
            # ä½¿ç”¨æ­£ç¢ºçš„ API æ–¹æ³•è™•ç†å¤šå€‹æ–‡æœ¬
            embeddings = []
            for text in texts_to_embed:
                result = self.genai_client.models.embed_content(
                    model="models/text-embedding-004",
                    contents=text  # ç§»é™¤ä¸æ”¯æ´çš„ task_type åƒæ•¸
                )
                # æ­£ç¢ºæå– embedding æ•¸æ“šï¼šEmbedContentResponse.embeddings[0].values
                if hasattr(result, 'embeddings') and len(result.embeddings) > 0:
                    embedding = result.embeddings[0]  # å–ç¬¬ä¸€å€‹ ContentEmbedding
                    if hasattr(embedding, 'values'):
                        embeddings.append(embedding.values)
                    else:
                        embeddings.append(embedding)
                elif hasattr(result, 'embedding') and hasattr(result.embedding, 'values'):
                    embeddings.append(result.embedding.values)
                elif hasattr(result, 'embedding'):
                    embeddings.append(result.embedding)
                else:
                    # å¦‚æœæ˜¯ EmbedContentResponse ç‰©ä»¶ï¼Œæ‰“å°å…¶å±¬æ€§ä»¥é™¤éŒ¯
                    print(f"åµéŒ¯ï¼šresult é¡å‹ = {type(result)}, å±¬æ€§ = {dir(result)}")
                    embeddings.append(result)
            print(f"âœ“ æˆåŠŸç”Ÿæˆ {len(embeddings)} å€‹æ–°èå‘é‡")
        except Exception as e:
            print(f"âœ— AI ç”Ÿæˆå‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("åˆ‡æ›åˆ°ç°¡å–®åˆ†çµ„æ¨¡å¼...")
            return self.simple_group_news(news_items)

        print("æ­¥é©Ÿ 2/3: ä½¿ç”¨å¤šç¨®æ¼”ç®—æ³•é€²è¡Œæ–°èåˆ†ç¾¤ä¸¦é¸æ“‡æœ€ä½³çµæœ...")
        try:
            # ç¢ºä¿ embeddings æ˜¯æ•¸å€¼é™£åˆ—
            processed_embeddings = []
            for i, emb in enumerate(embeddings):
                if isinstance(emb, (list, tuple)) and all(isinstance(x, (int, float)) for x in emb):
                    processed_embeddings.append(emb)
                else:
                    # å˜—è©¦è½‰æ›ç‚ºæ•¸å€¼
                    if hasattr(emb, 'values'):
                        processed_embeddings.append(emb.values)
                    elif hasattr(emb, 'tolist'):
                        processed_embeddings.append(emb.tolist())
                    else:
                        print(f"ç„¡æ³•è™•ç†ç¬¬ {i} å€‹ embeddingï¼Œè·³é")
                        continue
            
            X = np.array(processed_embeddings)
            print(f"âœ“ æˆåŠŸæº–å‚™ {len(processed_embeddings)} å€‹å‘é‡ï¼Œç¶­åº¦ï¼š{X.shape}")
        except Exception as e:
            print(f"âœ— æº–å‚™å‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("åˆ‡æ›åˆ°ç°¡å–®åˆ†çµ„æ¨¡å¼...")
            return self.simple_group_news(news_items)
        
        # ä½¿ç”¨å¤šç¨®æ¼”ç®—æ³•ä¸¦é¸æ“‡æœ€ä½³çµæœ
        best_labels, best_algorithm, best_score = self._find_best_clustering(X, len(news_items))
        
        labels = best_labels
        num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        num_noise = list(labels).count(-1)
        print(f"âœ“ æœ€ä½³åˆ†ç¾¤æ¼”ç®—æ³•ï¼š{best_algorithm}ï¼Œæ‰¾åˆ° {num_clusters} å€‹äº‹ä»¶åˆ†æ”¯å’Œ {num_noise} å‰‡ç¨ç«‹æ–°è")
        print(f"  åˆ†ç¾¤å“è³ªè©•åˆ†ï¼š{best_score:.3f}")
        
        clusters = {}
        for i, label in enumerate(labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(news_items[i])
            
        print("æ­¥é©Ÿ 3/3: ç‚ºæ¯å€‹äº‹ä»¶åˆ†æ”¯ç”Ÿæˆæ¨™é¡Œå’Œæ‘˜è¦...")
        event_groups = []
        
        # è™•ç†æ­£å¸¸åˆ†ç¾¤çš„æ–°è
        normal_clusters = {k: v for k, v in clusters.items() if k != -1}
        noise_news = clusters.get(-1, [])
        
        # å¦‚æœæœ‰å™ªéŸ³é»ï¼ˆæœªåˆ†ç¾¤çš„æ–°èï¼‰ï¼Œå˜—è©¦å°‡å®ƒå€‘åˆ†é…åˆ°æœ€ç›¸ä¼¼çš„åˆ†ç¾¤ä¸­
        if noise_news:
            print(f"  æ­£åœ¨é‡æ–°åˆ†é… {len(noise_news)} å‰‡æœªåˆ†ç¾¤æ–°èåˆ°æœ€ç›¸ä¼¼çš„åˆ†æ”¯...")
            noise_indices = [i for i, label in enumerate(labels) if label == -1]
            
            for noise_idx in noise_indices:
                noise_vector = X[noise_idx:noise_idx+1]  # ä¿æŒ2Dæ ¼å¼
                best_cluster = -1
                best_similarity = -1
                
                # è¨ˆç®—èˆ‡æ¯å€‹åˆ†ç¾¤çš„å¹³å‡ç›¸ä¼¼åº¦
                for cluster_label, cluster_news in normal_clusters.items():
                    cluster_indices = [i for i, label in enumerate(labels) if label == cluster_label]
                    if cluster_indices:
                        cluster_vectors = X[cluster_indices]
                        # è¨ˆç®—èˆ‡è©²åˆ†ç¾¤çš„å¹³å‡ç›¸ä¼¼åº¦
                        similarities = cosine_similarity(noise_vector, cluster_vectors)[0]
                        avg_similarity = np.mean(similarities)
                        
                        if avg_similarity > best_similarity:
                            best_similarity = avg_similarity
                            best_cluster = cluster_label
                
                # å°‡å™ªéŸ³æ–°èåˆ†é…åˆ°æœ€ç›¸ä¼¼çš„åˆ†ç¾¤
                if best_cluster != -1 and best_similarity > 0.3:  # ç›¸ä¼¼åº¦é–¾å€¼
                    normal_clusters[best_cluster].append(news_items[noise_idx])
                    print(f"    æ–°è {noise_idx+1} å·²åˆ†é…åˆ°åˆ†æ”¯ {best_cluster+1} (ç›¸ä¼¼åº¦: {best_similarity:.3f})")
                else:
                    # å¦‚æœç›¸ä¼¼åº¦å¤ªä½ï¼Œå‰µå»ºæ–°çš„å–®ç¨åˆ†æ”¯
                    new_label = max(normal_clusters.keys()) + 1 if normal_clusters else 0
                    normal_clusters[new_label] = [news_items[noise_idx]]
                    print(f"    æ–°è {noise_idx+1} å‰µå»ºæ–°åˆ†æ”¯ {new_label+1}")
        
        # è™•ç†æ‰€æœ‰åˆ†ç¾¤ï¼ˆç¾åœ¨ä¸æœƒæœ‰å™ªéŸ³é»äº†ï¼‰
        for label, grouped_news in normal_clusters.items():
            print(f"  æ­£åœ¨è™•ç†åˆ†æ”¯ {label+1} ({len(grouped_news)} å‰‡æ–°è)...")
            title, summary = self._generate_event_title_and_summary_for_group(grouped_news)
            event_groups.append({
                'event_id': str(uuid.uuid4()),
                'event_title': title,
                'event_summary': summary,
                'news_count': len(grouped_news),
                'news_items': grouped_news
            })
            
        return event_groups

    def _find_best_clustering(self, X, n_samples):
        """å˜—è©¦å¤šç¨®åˆ†ç¾¤æ¼”ç®—æ³•ä¸¦é¸æ“‡æœ€ä½³çµæœ - åå¥½æ›´ç´°ç·»çš„åˆ†ç¾¤"""
        print("æ­£åœ¨æ¸¬è©¦å¤šç¨®åˆ†ç¾¤æ¼”ç®—æ³•...")
        
        algorithms = []
        # èª¿æ•´åƒæ•¸ä»¥ç”¢ç”Ÿæ›´å¤šç´°ç·»åˆ†ç¾¤
        max_clusters = min(8, max(3, n_samples // 1.5))  # å¢åŠ æœ€å¤§åˆ†ç¾¤æ•¸ï¼Œé™ä½æœ€å°æ¨£æœ¬è¦æ±‚
        
        # 1. éšå±¤å¼åˆ†ç¾¤ (Agglomerative) - æ¸¬è©¦æ›´å¤šåˆ†ç¾¤æ•¸
        print("  æ¸¬è©¦éšå±¤å¼åˆ†ç¾¤...")
        for n_clusters in range(2, int(max_clusters) + 1):
            try:
                agg = AgglomerativeClustering(
                    n_clusters=n_clusters, 
                    metric='cosine', 
                    linkage='average'
                )
                labels = agg.fit_predict(X)
                score = self._evaluate_clustering(X, labels)
                algorithms.append(('Agglomerative', labels, score, n_clusters))
                print(f"    n_clusters={n_clusters}: åˆ†ç¾¤è©•åˆ†={score:.3f}")
            except Exception as e:
                print(f"    n_clusters={n_clusters}: å¤±æ•— ({e})")
        
        # 2. æ”¹è‰¯å‹ DBSCAN - ä½¿ç”¨æ›´åš´æ ¼çš„åƒæ•¸ç”¢ç”Ÿæ›´å¤šåˆ†ç¾¤
        print("  æ¸¬è©¦æ”¹è‰¯å‹ DBSCAN...")
        eps_values = [0.03, 0.05, 0.07, 0.08, 0.1, 0.12, 0.15]  # åŠ å…¥æ›´å°çš„ eps å€¼
        for eps in eps_values:
            try:
                dbscan = DBSCAN(eps=eps, min_samples=2, metric='cosine')
                labels = dbscan.fit_predict(X)
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                if n_clusters >= 2 and n_clusters <= max_clusters:
                    score = self._evaluate_clustering(X, labels)
                    algorithms.append(('DBSCAN', labels, score, eps))
                    print(f"    eps={eps}: {n_clusters}å€‹åˆ†æ”¯, åˆ†ç¾¤è©•åˆ†={score:.3f}")
            except Exception as e:
                print(f"    eps={eps}: å¤±æ•— ({e})")
        
        # 3. K-means++ - æ¸¬è©¦æ›´å¤šåˆ†ç¾¤æ•¸
        print("  æ¸¬è©¦ K-means++...")
        for n_clusters in range(2, int(max_clusters) + 1):
            try:
                kmeans = KMeans(
                    n_clusters=n_clusters, 
                    init='k-means++', 
                    n_init=20,
                    random_state=42
                )
                labels = kmeans.fit_predict(X)
                score = self._evaluate_clustering(X, labels)
                algorithms.append(('K-means++', labels, score, n_clusters))
                print(f"    n_clusters={n_clusters}: åˆ†ç¾¤è©•åˆ†={score:.3f}")
            except Exception as e:
                print(f"    n_clusters={n_clusters}: å¤±æ•— ({e})")
        
        # 4. èªç¾©ç›¸ä¼¼åº¦é–¾å€¼åˆ†ç¾¤ - ä½¿ç”¨æ›´åš´æ ¼çš„é–¾å€¼
        print("  æ¸¬è©¦èªç¾©ç›¸ä¼¼åº¦é–¾å€¼åˆ†ç¾¤...")
        try:
            labels = self._semantic_threshold_clustering(X)
            if labels is not None:
                n_clusters = len(set(labels))
                if n_clusters >= 2:
                    score = self._evaluate_clustering(X, labels)
                    algorithms.append(('SemanticThreshold', labels, score, 'adaptive'))
                    print(f"    è‡ªé©æ‡‰é–¾å€¼: {n_clusters}å€‹åˆ†æ”¯, åˆ†ç¾¤è©•åˆ†={score:.3f}")
        except Exception as e:
            print(f"    èªç¾©é–¾å€¼åˆ†ç¾¤å¤±æ•—: {e}")
        
        # 5. æ–°å¢ï¼šå¤šå±¤æ¬¡éšå±¤åˆ†ç¾¤ - ç”¢ç”Ÿæ›´ç´°ç·»çš„åˆ†ç¾¤
        print("  æ¸¬è©¦å¤šå±¤æ¬¡éšå±¤åˆ†ç¾¤...")
        try:
            labels = self._hierarchical_refined_clustering(X)
            if labels is not None:
                n_clusters = len(set(labels))
                if n_clusters >= 2:
                    score = self._evaluate_clustering(X, labels)
                    algorithms.append(('HierarchicalRefined', labels, score, 'multi-level'))
                    print(f"    å¤šå±¤æ¬¡åˆ†ç¾¤: {n_clusters}å€‹åˆ†æ”¯, åˆ†ç¾¤è©•åˆ†={score:.3f}")
        except Exception as e:
            print(f"    å¤šå±¤æ¬¡éšå±¤åˆ†ç¾¤å¤±æ•—: {e}")
        
        # é¸æ“‡æœ€ä½³æ¼”ç®—æ³•
        if not algorithms:
            print("  æ‰€æœ‰æ¼”ç®—æ³•éƒ½å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®åˆ†ç¾¤")
            return list(range(n_samples)), "Simple", 0.0
        
        # æ ¹æ“šè©•åˆ†é¸æ“‡æœ€ä½³çµæœï¼Œä½†åå¥½æ›´å¤šåˆ†ç¾¤çš„çµæœ
        best_algo, best_labels, best_score, best_param = max(algorithms, key=lambda x: x[2])
        print(f"âœ“ æœ€ä½³æ¼”ç®—æ³•ï¼š{best_algo} (åƒæ•¸: {best_param}), è©•åˆ†: {best_score:.3f}")
        
        return best_labels, f"{best_algo}(param={best_param})", best_score
    
    def _evaluate_clustering(self, X, labels):
        """è©•ä¼°åˆ†ç¾¤å“è³ª - åå¥½ç²¾ç´°ä½†æœ‰æ„ç¾©çš„åˆ†ç¾¤"""
        try:
            unique_labels = set(labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            
            if n_clusters < 2:
                return 0.0
            
            # è¨ˆç®—å™ªéŸ³é»æ¯”ä¾‹ï¼Œä¸¦çµ¦äºˆæ‡²ç½°
            noise_ratio = list(labels).count(-1) / len(labels) if len(labels) > 0 else 0
            noise_penalty = 1.0 - noise_ratio * 0.4  # é™ä½å™ªéŸ³æ‡²ç½°ï¼Œå…è¨±ä¸€äº›å™ªéŸ³é»
            
            # ç§»é™¤å™ªéŸ³é»é€²è¡Œè©•ä¼°
            if -1 in labels:
                mask = labels != -1
                if np.sum(mask) < 2:
                    return 0.0
                X_clean = X[mask]
                labels_clean = labels[mask]
            else:
                X_clean = X
                labels_clean = labels
            
            # è¼ªå»“ä¿‚æ•¸ (Silhouette Score) - è¡¡é‡åˆ†ç¾¤å…§ç·Šå¯†åº¦èˆ‡åˆ†ç¾¤é–“åˆ†é›¢åº¦
            sil_score = silhouette_score(X_clean, labels_clean, metric='cosine')
            
            # Calinski-Harabasz æŒ‡æ•¸ - è¡¡é‡åˆ†ç¾¤é–“å·®ç•°èˆ‡åˆ†ç¾¤å…§å·®ç•°çš„æ¯”å€¼
            ch_score = calinski_harabasz_score(X_clean, labels_clean)
            ch_score_normalized = min(ch_score / 1000, 1.0)  # æ¨™æº–åŒ–åˆ° 0-1
            
            # èª¿æ•´åˆ†ç¾¤æ•¸é‡åå¥½ - åå¥½æ›´å¤šåˆ†ç¾¤ä½†ä¸éåº¦
            total_samples = len(X)
            if total_samples <= 8:
                ideal_min, ideal_max = 3, 5
            elif total_samples <= 15:
                ideal_min, ideal_max = 4, 7
            else:
                ideal_min, ideal_max = 5, 8
            
            if ideal_min <= n_clusters <= ideal_max:
                cluster_bonus = 1.1  # åœ¨ç†æƒ³ç¯„åœå…§çµ¦äºˆåŠ åˆ†
            elif n_clusters > ideal_max:
                cluster_bonus = 1.0 - (n_clusters - ideal_max) * 0.05  # éå¤šåˆ†ç¾¤è¼•å¾®æ‡²ç½°
            else:
                cluster_bonus = 0.8  # åˆ†ç¾¤å¤ªå°‘é‡é‡æ‡²ç½°
            
            # åˆ†ç¾¤å¤§å°å¤šæ¨£æ€§ - å…è¨±å¤§å°ä¸ä¸€çš„åˆ†ç¾¤ï¼Œä½†é¿å…æ¥µç«¯ä¸å¹³è¡¡
            cluster_sizes = [np.sum(labels_clean == label) for label in set(labels_clean)]
            min_size, max_size = min(cluster_sizes), max(cluster_sizes)
            if min_size == 0:
                size_diversity = 0.5
            else:
                size_ratio = max_size / min_size
                if size_ratio <= 3:  # å…è¨±3å€çš„å¤§å°å·®ç•°
                    size_diversity = 1.0
                else:
                    size_diversity = max(0.3, 1.0 - (size_ratio - 3) * 0.1)
            
            # å…§ç¾¤ç·Šå¯†åº¦ - è¨ˆç®—æ¯å€‹åˆ†ç¾¤å…§éƒ¨çš„å¹³å‡ç›¸ä¼¼åº¦
            intra_cluster_scores = []
            for label in set(labels_clean):
                cluster_mask = labels_clean == label
                cluster_vectors = X_clean[cluster_mask]
                if len(cluster_vectors) > 1:
                    cluster_sim_matrix = cosine_similarity(cluster_vectors)
                    # å–ä¸Šä¸‰è§’ï¼ˆæ’é™¤å°è§’ç·šï¼‰çš„å¹³å‡ç›¸ä¼¼åº¦
                    upper_triangle = cluster_sim_matrix[np.triu_indices_from(cluster_sim_matrix, k=1)]
                    if len(upper_triangle) > 0:
                        intra_cluster_scores.append(np.mean(upper_triangle))
                else:
                    intra_cluster_scores.append(1.0)  # å–®å€‹æ¨£æœ¬çš„åˆ†ç¾¤çµ¦æ»¿åˆ†
            
            intra_cohesion = np.mean(intra_cluster_scores) if intra_cluster_scores else 0.5
            
            # ç¶œåˆè©•åˆ† - é‡æ–°èª¿æ•´æ¬Šé‡ä»¥åå¥½ç²¾ç´°åˆ†ç¾¤
            final_score = (
                sil_score * 0.3 +           # é™ä½è¼ªå»“ä¿‚æ•¸æ¬Šé‡
                ch_score_normalized * 0.15 + # é™ä½ CH æŒ‡æ•¸æ¬Šé‡
                cluster_bonus * 0.25 +       # å¢åŠ åˆ†ç¾¤æ•¸é‡åå¥½æ¬Šé‡
                noise_penalty * 0.15 +       # ä¿æŒå™ªéŸ³æ‡²ç½°
                size_diversity * 0.1 +       # æ–°å¢å¤§å°å¤šæ¨£æ€§
                intra_cohesion * 0.15        # æ–°å¢å…§ç¾¤ç·Šå¯†åº¦
            )
            return max(0.0, min(1.5, final_score))  # å…è¨±è¶…é1.0çš„åˆ†æ•¸
            
        except Exception:
            return 0.0
    
    def _semantic_threshold_clustering(self, X):
        """åŸºæ–¼èªç¾©ç›¸ä¼¼åº¦é–¾å€¼çš„è‡ªå®šç¾©åˆ†ç¾¤æ¼”ç®—æ³• - ç¢ºä¿æ²’æœ‰å­¤ç«‹é»"""
        try:
            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦çŸ©é™£
            similarity_matrix = cosine_similarity(X)
            
            # å˜—è©¦å¤šå€‹é–¾å€¼ï¼Œæ‰¾åˆ°ç”¢ç”Ÿæœ€ä½³åˆ†ç¾¤çš„é–¾å€¼
            upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
            percentiles = [60, 65, 70, 75, 80]  # å˜—è©¦å¤šå€‹ç™¾åˆ†ä½æ•¸
            
            best_labels = None
            best_score = -1
            
            for p in percentiles:
                threshold = np.percentile(upper_triangle, p)
                labels = self._graph_clustering_with_merge(similarity_matrix, threshold)
                
                if labels is not None:
                    # è©•ä¼°é€™å€‹åˆ†ç¾¤çµæœ
                    n_clusters = len(set(labels))
                    if 2 <= n_clusters <= 6:  # åˆç†çš„åˆ†ç¾¤æ•¸é‡ç¯„åœ
                        score = self._evaluate_clustering(X, labels)
                        if score > best_score:
                            best_score = score
                            best_labels = labels
            
            return best_labels
            
        except Exception:
            return None
    
    def _graph_clustering_with_merge(self, similarity_matrix, threshold):
        """åœ–è«–åˆ†ç¾¤ä¸¦åˆä½µå°åˆ†ç¾¤ä»¥é¿å…å­¤ç«‹é»"""
        try:
            # å»ºç«‹ç›¸ä¼¼åº¦åœ–
            adjacency = similarity_matrix > threshold
            n = len(similarity_matrix)
            
            # ä½¿ç”¨æ·±åº¦å„ªå…ˆæœç´¢æ‰¾é€£é€šåˆ†é‡
            visited = [False] * n
            components = []
            
            def dfs(node, component):
                visited[node] = True
                component.append(node)
                for neighbor in range(n):
                    if adjacency[node][neighbor] and not visited[neighbor]:
                        dfs(neighbor, component)
            
            # æ‰¾æ‰€æœ‰é€£é€šåˆ†é‡
            for i in range(n):
                if not visited[i]:
                    component = []
                    dfs(i, component)
                    components.append(component)
            
            # åˆä½µéå°çš„åˆ†ç¾¤åˆ°æœ€ç›¸ä¼¼çš„å¤§åˆ†ç¾¤ä¸­
            large_components = [comp for comp in components if len(comp) >= 2]
            small_components = [comp for comp in components if len(comp) == 1]
            
            # å°‡å°åˆ†ç¾¤åˆä½µåˆ°æœ€ç›¸ä¼¼çš„å¤§åˆ†ç¾¤
            for small_comp in small_components:
                node = small_comp[0]
                best_target = 0
                best_similarity = -1
                
                for i, large_comp in enumerate(large_components):
                    # è¨ˆç®—èˆ‡å¤§åˆ†ç¾¤çš„å¹³å‡ç›¸ä¼¼åº¦
                    similarities = [similarity_matrix[node][target] for target in large_comp]
                    avg_sim = np.mean(similarities)
                    
                    if avg_sim > best_similarity:
                        best_similarity = avg_sim
                        best_target = i
                
                # å°‡å°åˆ†ç¾¤åˆä½µåˆ°æœ€ç›¸ä¼¼çš„å¤§åˆ†ç¾¤
                if large_components and best_similarity > 0.2:  # æœ€ä½ç›¸ä¼¼åº¦é–¾å€¼
                    large_components[best_target].extend(small_comp)
                else:
                    # å¦‚æœç›¸ä¼¼åº¦å¤ªä½ï¼Œå°±åˆä½µåˆ°æœ€è¿‘çš„å¤§åˆ†ç¾¤
                    if large_components:
                        large_components[0].extend(small_comp)
            
            # ç”Ÿæˆæœ€çµ‚æ¨™ç±¤
            labels = [-1] * n
            for i, component in enumerate(large_components):
                for node in component:
                    labels[node] = i
            
            return np.array(labels)
            
        except Exception:
            return None
    
    def _hierarchical_refined_clustering(self, X, n_clusters=None):
        """
        å±¤æ¬¡ç²¾ç´°åŒ–åˆ†ç¾¤ - å…ˆç”¨è¼ƒé¬†çš„æ¢ä»¶åˆ†ç¾¤ï¼Œå†ç´°åˆ†æ¯å€‹å¤§ç¾¤
        """
        try:
            n_samples = len(X)
            if n_samples < 4:
                return np.zeros(n_samples, dtype=int)
            
            # ç¬¬ä¸€å±¤ï¼šè¼ƒå¯¬é¬†çš„åˆ†ç¾¤
            initial_clusters = min(4, n_samples // 2)
            
            # ä½¿ç”¨ K-means++ åšåˆæ­¥åˆ†ç¾¤
            kmeans_initial = KMeans(
                n_clusters=initial_clusters, 
                init='k-means++', 
                random_state=42, 
                n_init=10
            )
            initial_labels = kmeans_initial.fit_predict(X)
            
            final_labels = np.copy(initial_labels)
            current_max_label = np.max(initial_labels)
            
            # ç¬¬äºŒå±¤ï¼šå°æ¯å€‹åˆæ­¥åˆ†ç¾¤é€²è¡Œç´°åˆ†
            for cluster_id in range(initial_clusters):
                cluster_mask = initial_labels == cluster_id
                cluster_points = X[cluster_mask]
                
                if len(cluster_points) >= 4:  # åªå°æœ‰è¶³å¤ é»æ•¸çš„ç¾¤çµ„é€²è¡Œç´°åˆ†
                    # è¨ˆç®—ç¾¤çµ„å…§çš„ç›¸ä¼¼åº¦åˆ†ä½ˆ
                    similarity_matrix = cosine_similarity(cluster_points)
                    avg_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])
                    
                    # å¦‚æœç¾¤çµ„å…§ç›¸ä¼¼åº¦è¼ƒä½ï¼Œå˜—è©¦ç´°åˆ†
                    if avg_similarity < 0.7:  # ç›¸ä¼¼åº¦é–¾å€¼
                        # ä½¿ç”¨ AgglomerativeClustering é€²è¡Œç´°åˆ†
                        n_sub_clusters = min(3, len(cluster_points) // 2)
                        if n_sub_clusters > 1:
                            agg_clustering = AgglomerativeClustering(
                                n_clusters=n_sub_clusters,
                                metric='cosine',
                                linkage='average'
                            )
                            sub_labels = agg_clustering.fit_predict(cluster_points)
                            
                            # å°‡ç´°åˆ†çµæœæ˜ å°„å›åŸå§‹æ¨™ç±¤
                            for i, (original_idx, sub_label) in enumerate(zip(np.where(cluster_mask)[0], sub_labels)):
                                if sub_label > 0:  # ä¿ç•™ç¬¬ä¸€å€‹å­ç¾¤çš„åŸå§‹æ¨™ç±¤
                                    final_labels[original_idx] = current_max_label + sub_label
                            
                            current_max_label += np.max(sub_labels)
            
            return final_labels
            
        except Exception as e:
            print(f"å±¤æ¬¡ç²¾ç´°åŒ–åˆ†ç¾¤éŒ¯èª¤: {e}")
            # é™ç´šåˆ°ç°¡å–® K-means
            n_clusters = min(n_clusters or 5, len(X) // 2) if n_clusters else min(5, len(X) // 2)
            if n_clusters < 2:
                return np.zeros(len(X), dtype=int)
            
            kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42, n_init=10)
            return kmeans.fit_predict(X)

    def _generate_event_title_and_summary_for_group(self, news_group):
        """ç‚ºä¸€å€‹å·²åˆ†ç¾¤çš„æ–°èçµ„ç”Ÿæˆæ¨™é¡Œå’Œæ‘˜è¦"""
        if not self.use_ai:
            return f"ç¶œåˆäº‹ä»¶", f"åŒ…å« {len(news_group)} å‰‡æ–°èçš„äº‹ä»¶"

        news_summaries = []
        for news in news_group[:5]: # æœ€å¤šå–å‰5å‰‡æ–°èåšåƒè€ƒ
            summary = f"æ¨™é¡Œ: {news['news_title']}\nå…§å®¹: {news['content'][:200]}..."
            news_summaries.append(summary)

        prompt = f"""
è«‹åˆ†æä»¥ä¸‹å±¬æ–¼åŒä¸€å€‹äº‹ä»¶çš„æ–°èæ‘˜è¦ï¼Œç‚ºé€™å€‹æ ¸å¿ƒäº‹ä»¶ç”Ÿæˆä¸€å€‹ç²¾ç…‰çš„æ¨™é¡Œå’Œç¸½çµã€‚

æ–°èæ‘˜è¦:
{"-"*20}\n{chr(10).join(news_summaries)}\n{"-"*20}

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š
{{
  "event_title": "ç²¾ç…‰ä¸”å…·é«”çš„äº‹ä»¶æ¨™é¡Œ (10å­—ä»¥å…§)",
  "event_summary": "å°æ•´å€‹äº‹ä»¶çš„ç°¡æ½”æ‘˜è¦ (80å­—ä»¥å…§)"
}}

**é‡è¦**: åªå›å‚³ JSON ç‰©ä»¶ï¼Œä¸è¦åŒ…å«ä»»ä½•é¡å¤–çš„è§£é‡‹æˆ– markdown æ ¼å¼ã€‚
"""
        try:
            response = self.genai_client.models.generate_content(
                model="models/gemini-2.5-flash",
                contents=prompt
            )
            result_text = response.text.strip()
            
            if result_text.startswith('```json'):
                result_text = result_text[7:-3].strip()
            
            result = json.loads(result_text)
            title = result.get('event_title', f"ç¶œåˆäº‹ä»¶ ({len(news_group)}å‰‡)")
            summary = result.get('event_summary', "æ­¤äº‹ä»¶çš„ç¶œåˆå ±å°ã€‚")
            return title, summary
        except Exception as e:
            print(f"  âœ— AI ç”Ÿæˆæ¨™é¡Œæ‘˜è¦æ™‚å‡ºéŒ¯: {e}")
            fallback_title = news_group[0]['news_title'] if news_group else "ç¶œåˆäº‹ä»¶"
            return fallback_title[:15], f"åŒ…å« {len(news_group)} å‰‡ç›¸é—œæ–°èçš„äº‹ä»¶"
            
    def simple_group_news(self, news_items):
        """ç°¡å–®çš„æ–°èåˆ†çµ„ï¼ˆä¸ä½¿ç”¨ AI æ™‚çš„å‚™ç”¨æ–¹æ¡ˆï¼‰"""
        return [{
            'event_id': str(uuid.uuid4()),
            'event_title': 'ç¶œåˆæ–°èäº‹ä»¶',
            'event_summary': f'åŒ…å« {len(news_items)} å‰‡ç›¸é—œæ–°èçš„ç¶œåˆäº‹ä»¶',
            'news_count': len(news_items),
            'news_items': news_items
        }]
    
    def save_to_database(self, event_groups, save_mode="both"):
        """å°‡äº‹ä»¶åˆ†æ”¯å’Œæ–°èæ˜ å°„å­˜å…¥è³‡æ–™åº«"""
        try:
            print(f"\né–‹å§‹è³‡æ–™åº«å„²å­˜æµç¨‹ (æ¨¡å¼: {save_mode})...")
            topic_branch_news_map_data, topic_branch_data = [], []
            for topic_group in event_groups:
                topic_id = topic_group.get('topic_id')
                for sub_event in topic_group.get('sub_events', []):
                    topic_branch_id = sub_event.get('event_id')
                    if topic_id and topic_branch_id and sub_event.get('event_title'):
                        topic_branch_data.append({
                            'topic_id': topic_id,
                            'topic_branch_id': topic_branch_id,
                            'topic_branch_title': sub_event.get('event_title'),
                            'topic_branch_content': sub_event.get('event_summary', '')
                        })
                        for news_item in sub_event.get('news_items', []):
                            if news_item.get('story_id'):
                                topic_branch_news_map_data.append({
                                    'topic_branch_id': topic_branch_id,
                                    'story_id': news_item.get('story_id')
                                })
            
            print(f"æº–å‚™è³‡æ–™: {len(topic_branch_data)} å€‹åˆ†æ”¯, {len(topic_branch_news_map_data)} ç­†æ–°èå°æ‡‰")
            if save_mode in ["preview", "both"]:
                self._save_database_preview(topic_branch_data, topic_branch_news_map_data)
            if save_mode in ["database", "both"]:
                self._save_to_actual_database(topic_branch_data, topic_branch_news_map_data)
        except Exception as e:
            print(f"âœ— è³‡æ–™åº«å„²å­˜æµç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _save_database_preview(self, topic_branch_data, topic_branch_news_map_data):
        """å„²å­˜è³‡æ–™åº«é è¦½æª”æ¡ˆ"""
        try:
            print("\n--- ç”Ÿæˆè³‡æ–™åº«é è¦½æª”æ¡ˆ ---")
            for name, data in [("topic_branch", topic_branch_data), ("topic_branch_news_map", topic_branch_news_map_data)]:
                filename = f"database_preview_{name}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print(f"âœ“ {name} é è¦½å·²å„²å­˜: {filename} ({len(data)} ç­†)")
        except Exception as e:
            print(f"âœ— ç”Ÿæˆé è¦½æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _save_to_actual_database(self, topic_branch_data, topic_branch_news_map_data):
        """åˆ†æ‰¹å„²å­˜åˆ°å¯¦éš›è³‡æ–™åº«"""
        print("\n--- é–‹å§‹å„²å­˜åˆ°å¯¦éš›è³‡æ–™åº« ---")
        for table_name, data, batch_size in [("topic_branch", topic_branch_data, 50), ("topic_branch_news_map", topic_branch_news_map_data, 100)]:
            print(f"å„²å­˜ {table_name} è³‡æ–™...")
            if not data:
                print(f"  {table_name} ç„¡è³‡æ–™éœ€å„²å­˜ã€‚")
                continue

            success_count = 0
            for i in range(0, len(data), batch_size):
                batch = data[i:i + batch_size]
                try:
                    self.supabase.table(table_name).upsert(batch).execute()
                    success_count += len(batch)
                    print(f"  âœ“ {table_name} ç¬¬ {i//batch_size + 1} æ‰¹ ({len(batch)} ç­†) æˆåŠŸ")
                except Exception as e:
                    print(f"  âœ— {table_name} ç¬¬ {i//batch_size + 1} æ‰¹ç™¼ç”ŸéŒ¯èª¤: {e}")
            print(f"  â†’ {table_name} ç¸½è¨ˆæˆåŠŸå„²å­˜: {success_count}/{len(data)} ç­†")
        print("\nâœ… è³‡æ–™åº«å„²å­˜å®Œæˆï¼")

    def save_to_json(self, data, output_path):
        """å°‡çµæœå„²å­˜åˆ° JSON æª”æ¡ˆ"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ è™•ç†çµæœå·²å„²å­˜è‡³: {output_path}")
        except Exception as e:
            print(f"âœ— å„²å­˜ JSON æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def process_from_topic_map(self, output_path, save_to_db=True):
        """ä¸»è¦è™•ç†æµç¨‹ï¼šå¾ topic_news_map é–‹å§‹"""
        print("=" * 60)
        print("æ–°èäº‹ä»¶åˆ†çµ„å™¨ - è™•ç†æµç¨‹å•Ÿå‹•")
        print("=" * 60)
        
        topic_news_map = self.fetch_topic_news_map_from_supabase()
        if not topic_news_map: return
        
        topic_groups = self.group_by_topic_id(topic_news_map)
        if not topic_groups: return
        
        all_topic_events = []
        for topic_id, story_ids in topic_groups.items():
            print(f"\n{'='*20} è™•ç†ä¸»é¡Œ {topic_id} ({len(story_ids)} å‰‡æ–°è) {'='*20}")
            news_items = self.fetch_news_from_supabase(story_ids)
            if not news_items:
                print(f"âœ— ä¸»é¡Œ {topic_id}: æœªç²å–åˆ°æœ‰æ•ˆæ–°èå…§å®¹ï¼Œè·³éæ­¤ä¸»é¡Œ")
                continue
            
            topic_title, _ = self._generate_event_title_and_summary_for_group(news_items)
            print(f"âœ“ ä¸»é¡Œ {topic_id} AIå‘½åç‚º: {topic_title}")
            
            if len(news_items) <= 3: # æ–°èæ•¸é‡å°‘ï¼Œä¸éœ€ç´°åˆ†
                _, topic_summary = self._generate_event_title_and_summary_for_group(news_items)
                sub_events = [{
                    'event_id': str(uuid.uuid4()), 'event_title': topic_title,
                    'event_summary': topic_summary, 'news_count': len(news_items),
                    'news_items': news_items
                }]
                print(f"  â†’ æ–°èé‡å°‘ï¼Œè¦–ç‚ºå–®ä¸€åˆ†æ”¯")
            else: # æ–°èæ•¸é‡å¤šï¼Œé€²è¡ŒAIç´°åˆ†
                print(f"  â†’ æ–°èé‡å¤šï¼Œæ­£åœ¨é€²è¡Œ AI ç´°åˆ† (å‘é‡åŒ–+åˆ†ç¾¤)...")
                sub_events = self.group_news_by_vectors(news_items)
            
            all_topic_events.append({
                'topic_id': topic_id, 'topic_title': topic_title, 'sub_events': sub_events
            })
            print(f"  â†’ ä¸»é¡Œ {topic_id} å·²ç´°åˆ†ç‚º {len(sub_events)} å€‹åˆ†æ”¯")
        
        self.save_to_json(all_topic_events, output_path)
        self.save_to_database(all_topic_events, "both" if save_to_db else "preview")
        
        # è¼¸å‡ºæœ€çµ‚çµ±è¨ˆè³‡è¨Š
        self._print_summary_stats(topic_groups, all_topic_events)
        return all_topic_events

    def _print_summary_stats(self, topic_groups, all_topic_events):
        print("\n" + "=" * 60)
        print("è™•ç†å®Œæˆ - æœ€çµ‚çµ±è¨ˆè³‡è¨Š")
        print("=" * 60)
        total_sub_events = sum(len(t['sub_events']) for t in all_topic_events)
        total_news = sum(sum(se['news_count'] for se in t['sub_events']) for t in all_topic_events)
        print(f"åŸå§‹ä¸»é¡Œæ•¸é‡: {len(topic_groups)}")
        print(f"æˆåŠŸè™•ç†çš„ä¸»é¡Œ: {len(all_topic_events)}")
        print(f"ç”¢ç”Ÿçš„ç¸½åˆ†æ”¯æ•¸é‡: {total_sub_events}")
        print(f"ç´å…¥åˆ†æ”¯çš„ç¸½æ–°èæ•¸: {total_news}")
        for i, topic in enumerate(all_topic_events, 1):
            print(f"\nä¸»é¡Œ {i}: {topic['topic_title']} (ID: {topic['topic_id']})")
            for j, sub_event in enumerate(topic['sub_events'], 1):
                print(f"  åˆ†æ”¯ {j}: {sub_event['event_title']} ({sub_event['news_count']} å‰‡æ–°è)")

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    print("ğŸš€ æ–°èäº‹ä»¶åˆ†çµ„å™¨ - å•Ÿå‹•ä¸­...")
    
    # æª¢æŸ¥å‘½ä»¤åˆ—åƒæ•¸
    test_mode = False
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        if command in ['test', '--test', '-t']:
            test_mode = True
            print("ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šåªç”Ÿæˆé è¦½æª”æ¡ˆï¼Œä¸å¯«å…¥è³‡æ–™åº«")
        elif command in ['help', '--help', '-h']:
            print("\nä½¿ç”¨æ–¹å¼:")
            print("  python new_complete_news_grouper.py           # æ­£å¸¸æ¨¡å¼ï¼ˆå¯«å…¥è³‡æ–™åº«ï¼‰")
            print("  python new_complete_news_grouper.py test      # æ¸¬è©¦æ¨¡å¼ï¼ˆåªç”Ÿæˆé è¦½ï¼‰")
            print("  python new_complete_news_grouper.py --help    # é¡¯ç¤ºæ­¤å¹«åŠ©")
            return
    
    if not test_mode:
        print("ğŸ’¾ æ¨¡å¼ï¼šå¾ topic_news_map è®€å–è³‡æ–™ï¼ŒAIåˆ†çµ„å¾Œå„²å­˜åˆ°è³‡æ–™åº«")
    
    print("=" * 60)
    
    try:
        grouper = NewsEventGrouper()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        return
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"topic_grouped_news_{timestamp}.json"
    print(f"ğŸ“„ çµæœå°‡å„²å­˜åˆ°: {output_path}")
    
    if test_mode:
        print("ğŸ“‹ å°‡åªç”Ÿæˆé è¦½æª”æ¡ˆï¼Œä¸æœƒå¯«å…¥è³‡æ–™åº«")
    else:
        print("ğŸ’¾ å°‡åŒæ™‚ç”Ÿæˆé è¦½æª”æ¡ˆä¸¦å„²å­˜åˆ°è³‡æ–™åº«")
    print()
    
    try:
        result = grouper.process_from_topic_map(output_path, save_to_db=(not test_mode))
        if result:
            if test_mode:
                print("\nğŸ‰ æ¸¬è©¦å®Œæˆï¼è«‹æª¢æŸ¥é è¦½æª”æ¡ˆ:")
                print("  - database_preview_topic_branch.json")
                print("  - database_preview_topic_branch_news_map.json")
                print(f"  - {output_path}")
            else:
                print("\nğŸ‰ å…¨éƒ¨è™•ç†å®Œæˆï¼")
        else:
            print("\nâš ï¸ è™•ç†æœªå®Œæˆæˆ–ç„¡è³‡æ–™ï¼Œè«‹æª¢æŸ¥ä¸Šæ–¹è¼¸å‡ºè¨Šæ¯ã€‚")
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ä½¿ç”¨è€…ä¸­æ–·ç¨‹å¼åŸ·è¡Œã€‚")
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
    
    print("\nç¨‹å¼çµæŸã€‚")

if __name__ == "__main__":
    main()