"""
å›°é›£é—œéµå­—æå–å™¨ - ä»¿ç…§ keyword_processor æ¶æ§‹
å¾ Supabase single_news è¡¨è®€å–è³‡æ–™ï¼Œæå–å›°é›£é—œéµå­—ä¸¦ç”Ÿæˆè§£é‡‹

ç”¨æ³•:
  python difficult_keyword_extractor.py [limit]

è«‹åœ¨ word_analysis_system/.env è¨­å®š GEMINI_API_KEYã€SUPABASE_URL èˆ‡ SUPABASE_KEY
"""

import os
import json
import time
import sys
from typing import List, Dict, Any, Set, Optional
from dotenv import load_dotenv
from tqdm import tqdm
import google.generativeai as genai


class DiffKeywordConfig:
    """å›°é›£é—œéµå­—è™•ç†å™¨è¨­å®š"""
    
    # API è¨­å®š
    API_CONFIG = {
        'model_name': 'gemini-2.5-flash-lite',
        'call_delay_seconds': 1,  # API å‘¼å«é–“éš”
        'max_retries': 3,
    }
    
    # è™•ç†è¨­å®š
    PROCESSING_CONFIG = {
        'explanation_word_limit': 50,  # è§£é‡‹å­—æ•¸é™åˆ¶
        'default_limit': None,  # é è¨­è®€å–ç­†æ•¸é™åˆ¶
    }
    
    # è³‡æ–™åº«è¨­å®š
    DB_CONFIG = {
        'table_name': 'single_news',
        'select_fields': ['story_id', 'news_title', 'ultra_short', 'short', 'long'],
        'primary_content_field': 'long',  # ä¸»è¦ç”¨æ–¼æå–é—œéµå­—çš„æ¬„ä½
        'title_field': 'news_title',
        'term_map_table': 'term_map',
        'term_map_fields': ['story_id', 'term'],
        'term_table': 'term',
        'term_fields': ['term', 'definition', 'example'],
    }
    
    # è¼¸å‡ºè¨­å®š
    OUTPUT_CONFIG = {
        'save_to_file': False,
        'output_filename': 'difficult_keywords_output.json',
        'terminal_width': 80,
    }


class DiffKeywordProcessor:
    """å›°é›£é—œéµå­—æå–èˆ‡è§£é‡‹çš„æ ¸å¿ƒé¡åˆ¥"""

    def __init__(self):
        """åˆå§‹åŒ–å›°é›£é—œéµå­—è™•ç†å™¨"""
        self.model = None
        self.supabase_client = None
        self.api_config = DiffKeywordConfig.API_CONFIG
        self.proc_config = DiffKeywordConfig.PROCESSING_CONFIG
        self.db_config = DiffKeywordConfig.DB_CONFIG
        self._setup_model()
        self._setup_supabase()

    def _setup_model(self):
        """è¼‰å…¥ç’°å¢ƒè®Šæ•¸ä¸¦åˆå§‹åŒ– Gemini æ¨¡å‹"""
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise EnvironmentError("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GEMINI_API_KEYï¼Œè«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š")
        
        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(self.api_config['model_name'])
            print(f"âœ“ Gemini API ({self.api_config['model_name']}) åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âœ— åˆå§‹åŒ– Gemini æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

    def _setup_supabase(self):
        """è¼‰å…¥ç’°å¢ƒè®Šæ•¸ä¸¦åˆå§‹åŒ– Supabase é€£ç·š"""
        load_dotenv()
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise EnvironmentError("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° SUPABASE_URL æˆ– SUPABASE_KEYï¼Œè«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š")
        
        try:
            from supabase import create_client
            self.supabase_client = create_client(supabase_url, supabase_key)
            print(f"âœ“ Supabase é€£ç·š ({supabase_url}) åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âœ— åˆå§‹åŒ– Supabase æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("è«‹ç¢ºèªå·²å®‰è£ supabase-pyï¼špip install supabase-py postgrest-py")
            raise

    def is_ready(self) -> bool:
        """æª¢æŸ¥æ¨¡å‹å’Œè³‡æ–™åº«é€£ç·šæ˜¯å¦å·²æˆåŠŸåˆå§‹åŒ–"""
        return self.model is not None and self.supabase_client is not None

    def _clean_response_text(self, text: str) -> str:
        """æ¸…ç† Gemini å›è¦†ä¸­çš„ markdown JSON æ¨™ç±¤"""
        cleaned_text = text.strip()
        # æª¢æŸ¥ä¸¦ç§»é™¤é–‹é ­çš„ markdown æ¨™ç±¤
        if cleaned_text.startswith("```json"):
            cleaned_text = cleaned_text[7:]
        elif cleaned_text.startswith("```"):
            cleaned_text = cleaned_text[3:]
        
        # æª¢æŸ¥ä¸¦ç§»é™¤çµå°¾çš„ markdown æ¨™ç±¤
        if cleaned_text.endswith("```json"):
            cleaned_text = cleaned_text[:-7]
        elif cleaned_text.endswith("```"):
            cleaned_text = cleaned_text[:-3]
            
        return cleaned_text.strip()

    def _call_gemini(self, prompt: str) -> Dict[str, Any]:
        """å‘¼å« Gemini API ä¸¦è™•ç†å›è¦†"""
        for attempt in range(self.api_config['max_retries']):
            try:
                response = self.model.generate_content(prompt)
                # ä½¿ç”¨ä¿®æ­£å¾Œçš„æ¸…ç†å‡½å¼
                cleaned_text = self._clean_response_text(response.text)
                return json.loads(cleaned_text)
            except json.JSONDecodeError as e:
                print(f"âœ— JSON è§£æéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{self.api_config['max_retries']}): {e}")
                if attempt == self.api_config['max_retries'] - 1:
                    print(f"åŸå§‹å›è¦†: {response.text}")
                    return {}
            except Exception as e:
                print(f"âœ— API å‘¼å«æ™‚ç™¼ç”ŸéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{self.api_config['max_retries']}): {e}")
                if attempt == self.api_config['max_retries'] - 1:
                    return {}
                time.sleep(2)  # é‡è©¦å‰ç­‰å¾…
        return {}

    def fetch_single_news(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """å¾ Supabase single_news è¡¨è®€å–è³‡æ–™"""
        print("è®€å– single_news è³‡æ–™...")
        
        try:
            table_name = self.db_config['table_name']
            fields = ','.join(self.db_config['select_fields'])
            
            query = self.supabase_client.table(table_name).select(fields)
            
            if limit:
                query = query.limit(limit)
                print(f"é™åˆ¶è®€å–å‰ {limit} ç­†")
            else:
                print("è®€å–æ‰€æœ‰è³‡æ–™")
            
            resp = query.execute()
            
            if getattr(resp, 'error', None):
                print(f"è®€å– {table_name} å¤±æ•—: {resp.error}")
                return []
            
            rows = resp.data or []
            print(f"æˆåŠŸè®€å– {len(rows)} ç­†è³‡æ–™")
            return rows
            
        except Exception as e:
            print(f"è®€å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

    def fetch_term_map(self) -> Dict[str, List[str]]:
        """å¾ Supabase term_map è¡¨è®€å–è³‡æ–™ï¼Œä¸¦çµ„ç¹”æˆ story_id -> terms çš„å­—å…¸"""
        print("è®€å– term_map è³‡æ–™...")
        
        try:
            table_name = self.db_config['term_map_table']
            fields = ','.join(self.db_config['term_map_fields'])
            
            query = self.supabase_client.table(table_name).select(fields)
            resp = query.execute()
            
            if getattr(resp, 'error', None):
                print(f"è®€å– {table_name} å¤±æ•—: {resp.error}")
                return {}
            
            rows = resp.data or []
            print(f"æˆåŠŸè®€å– {len(rows)} ç­† term_map è³‡æ–™")
            
            # çµ„ç¹”æˆ story_id -> terms çš„å­—å…¸
            term_map = {}
            for row in rows:
                story_id = row.get('story_id')
                term = row.get('term')
                
                if story_id and term:
                    if story_id not in term_map:
                        term_map[story_id] = []
                    term_map[story_id].append(term)
            
            print(f"çµ„ç¹” term_map: {len(term_map)} å€‹ä¸åŒçš„ story_id")
            return term_map
            
        except Exception as e:
            print(f"è®€å– term_map è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {}

    def fetch_combined_data(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """è®€å–ä¸¦åˆä½µ single_news å’Œ term_map è³‡æ–™"""
        print("\n=== è®€å–åˆä½µè³‡æ–™ ===")
        
        # è®€å– single_news è³‡æ–™
        news_data = self.fetch_single_news(limit)
        if not news_data:
            return []
        
        # è®€å– term_map è³‡æ–™
        term_map = self.fetch_term_map()
        
        # åˆä½µè³‡æ–™
        combined_data = []
        for news in news_data:
            story_id = news.get('story_id')
            existing_terms = term_map.get(story_id, [])
            
            # å°‡ existing_terms æ·»åŠ åˆ°æ–°èè³‡æ–™ä¸­
            news_with_terms = news.copy()
            news_with_terms['existing_terms'] = existing_terms
            combined_data.append(news_with_terms)
        
        print(f"åˆä½µå®Œæˆ: {len(combined_data)} ç­†æ–°èè³‡æ–™ï¼Œå…¶ä¸­ {len([n for n in combined_data if n['existing_terms']])} ç­†æœ‰ç¾æœ‰é—œéµå­—")
        return combined_data

    def extract_keywords_from_text(self, text: str, title: str) -> List[str]:
        """å¾å–®ç¯‡æ–‡æœ¬ä¸­æå–å›°é›£é—œéµå­—"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„çŸ¥è­˜ç·¨è¼¯ï¼Œæ“…é•·ç‚ºå¤§çœ¾è®€è€…è§£é‡‹è¤‡é›œæ¦‚å¿µã€‚
        è«‹å¾ä»¥ä¸‹æ–°èå…§å®¹ä¸­ï¼Œ**åš´æ ¼ç¯©é¸**å‡ºå°ä¸€èˆ¬å¤§çœ¾è€Œè¨€ï¼Œæœ€å…·å°ˆæ¥­æ€§ã€æŠ€è¡“æ€§æˆ–è¼ƒç‚ºè‰±æ·±é›£æ‡‚çš„é—œéµå­—ã€‚
        
        **åš´æ ¼æ¨™æº–ï¼šåªæå–çœŸæ­£å›°é›£çš„è©å½™**
        å¿…é ˆç¬¦åˆä»¥ä¸‹è‡³å°‘ä¸€å€‹åš´æ ¼æ¢ä»¶ï¼š
        - é«˜åº¦å°ˆæ¥­è¡“èªï¼ˆéœ€è¦å°ˆæ¥­èƒŒæ™¯æ‰èƒ½ç†è§£ï¼Œå¦‚é†«å­¸ã€æ³•å¾‹ã€å·¥ç¨‹ã€é‡‘èå°ˆæ¥­è¡“èªï¼‰
        - å¤–ä¾†èªæˆ–ç¸®å¯«ï¼ˆä¸€èˆ¬äººä¸ç†Ÿæ‚‰çš„è‹±æ–‡ç¸®å¯«ã€çµ„ç¹”åç¨±ï¼‰
        - ç‰¹å®šé ˜åŸŸæ¦‚å¿µï¼ˆéœ€è¦ç‰¹æ®ŠçŸ¥è­˜èƒŒæ™¯æ‰èƒ½ç†è§£çš„æ¦‚å¿µï¼‰
        - æ–°èˆˆæŠ€è¡“è¡“èªï¼ˆå¦‚äººå·¥æ™ºæ…§ã€å€å¡Šéˆç­‰æ–°ç§‘æŠ€åè©ï¼‰
        
        **ä¸è¦æå–çš„è©å½™ï¼š**
        - å¸¸è¦‹çš„åœ°åã€äººåã€å…¬å¸åï¼ˆé™¤ééå¸¸å°ˆæ¥­æˆ–ç½•è¦‹ï¼‰
        - ä¸€èˆ¬æ€§å½¢å®¹è©ã€å‹•è©ã€å‰¯è©
        - æ—¥å¸¸ç”Ÿæ´»å¸¸è¦‹è©å½™
        - ç°¡å–®çš„æ•¸å­—ã€æ™‚é–“ã€æ¯”ä¾‹
        - æ”¿æ²»äººç‰©å§“åï¼ˆé™¤éæ˜¯å°ˆé–€è¡“èªï¼‰
        
        **æå–åŸå‰‡ï¼šå¯§ç¼ºå‹¿æ¿«ï¼Œåªé¸æ“‡çœŸæ­£éœ€è¦è§£é‡‹çš„å›°é›£è©å½™**

        æ¨™é¡Œï¼š{title}
        å…§å®¹ï¼š{text}

        è«‹åš´æ ¼ä»¥ JSON æ ¼å¼å›å‚³ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {{"keywords": ["é—œéµå­—1", "é—œéµå­—2", "..."]}}
        """
        result = self._call_gemini(prompt)
        time.sleep(self.api_config['call_delay_seconds'])
        return result.get('keywords', [])

    def get_word_explanation(self, word: str) -> Dict[str, Any]:
        """ç‚ºå–®ä¸€è©å½™ç”¢ç”Ÿè§£é‡‹å’Œå¯¦éš›æ‡‰ç”¨å¯¦ä¾‹"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä½çŸ¥è­˜æ·µåšçš„è©å…¸ç·¨çº‚å°ˆå®¶ï¼Œæ“…é•·ç”¨å…·é«”å¯¦ä¾‹èªªæ˜æ¦‚å¿µã€‚
        é‡å°ä»¥ä¸‹è©å½™ï¼Œè«‹æä¾›ç´„ {self.proc_config['explanation_word_limit']} å­—çš„ã€Œåè©è§£é‡‹ã€å’Œã€Œæ‡‰ç”¨å¯¦ä¾‹ã€ã€‚

        è¦è§£é‡‹çš„è©å½™æ˜¯ï¼šã€Œ{word}ã€

        ã€Œæ‡‰ç”¨å¯¦ä¾‹ã€éƒ¨åˆ†ï¼Œè«‹ä¸è¦ç”¨å®Œæ•´çš„å¥å­é€ å¥ã€‚è«‹ç›´æ¥åˆ—å‡ºè©²è©å½™æœƒè¢«ä½¿ç”¨åˆ°çš„å…·é«”å ´æ™¯ã€æŠ€è¡“æˆ–ç”¢å“ã€‚
        æ ¼å¼è«‹åƒé€™æ¨£ï¼Œåˆ—èˆ‰å¹¾å€‹å¯¦éš›ä¾‹å­ï¼š
        - **ç¯„ä¾‹è¼¸å…¥ï¼š** äººå·¥æ™ºæ…§
        - **æœŸæœ›çš„æ‡‰ç”¨å¯¦ä¾‹è¼¸å‡ºï¼š** èªéŸ³åŠ©æ‰‹ï¼ˆå¦‚ Siriã€Alexaï¼‰ã€æ¨è–¦ç³»çµ±ã€è‡ªå‹•é§•é§›æ±½è»Šã€é†«ç™‚å½±åƒåˆ†æã€‚

        è«‹åš´æ ¼ä¾ç…§ä»¥ä¸‹ JSON æ ¼å¼å›å‚³ï¼Œä¸è¦æœ‰ä»»ä½• markdown æ¨™ç±¤æˆ–èªªæ˜æ–‡å­—ï¼š
        {{
            "term": "{word}",
            "definition": "ï¼ˆåœ¨æ­¤å¡«å¯«ç°¡æ½”çš„åè©è§£é‡‹ï¼‰",
            "examples": [
                {{
                    "title": "æ‡‰ç”¨å¯¦ä¾‹",
                    "text": "ï¼ˆåœ¨æ­¤æ¢åˆ—å¼å¡«å¯«å…·é«”çš„æ‡‰ç”¨å ´æ™¯æˆ–ç”¢å“ï¼Œè€Œéé€ å¥ï¼‰"
                }}
            ]
        }}
        """
        result = self._call_gemini(prompt)
        time.sleep(self.api_config['call_delay_seconds'])
        return result

    def print_progress_summary(self, story_keywords: Dict, all_keywords: Set[str]):
        """å°å‡ºé€²åº¦æ‘˜è¦"""
        width = DiffKeywordConfig.OUTPUT_CONFIG['terminal_width']
        print("\n" + "=" * width)
        print("  å›°é›£é—œéµå­—æå–é€²åº¦æ‘˜è¦")
        print("=" * width)
        print(f"è™•ç†æ–°èæ•¸é‡: {len(story_keywords)}")
        print(f"ä¸é‡è¤‡é—œéµå­—ç¸½æ•¸: {len(all_keywords)}")
        print(f"å¹³å‡æ¯ç¯‡é—œéµå­—æ•¸: {sum(len(data['keywords']) for data in story_keywords.values()) / len(story_keywords):.1f}")
        print("=" * width)

    def check_existing_term_combinations(self, story_keywords: Dict) -> List[Dict[str, str]]:
        """æª¢æŸ¥ä¸¦æº–å‚™éœ€è¦æ’å…¥åˆ° term_map çš„æ–°çµ„åˆ"""
        print("\n=== æª¢æŸ¥ term_map é‡è¤‡æ€§ ===")
        
        # å…ˆå–å¾—ç¾æœ‰çš„æ‰€æœ‰ term_map çµ„åˆ
        try:
            table_name = self.db_config['term_map_table']
            query = self.supabase_client.table(table_name).select('story_id,term')
            resp = query.execute()
            
            if getattr(resp, 'error', None):
                print(f"è®€å– {table_name} å¤±æ•—: {resp.error}")
                return []
            
            existing_combinations = set()
            for row in resp.data or []:
                story_id = row.get('story_id')
                term = row.get('term')
                if story_id and term:
                    existing_combinations.add((story_id, term))
            
            print(f"ç¾æœ‰ term_map çµ„åˆæ•¸é‡: {len(existing_combinations)}")
            
        except Exception as e:
            print(f"è®€å–ç¾æœ‰ term_map è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
        
        # æª¢æŸ¥å“ªäº›çµ„åˆæ˜¯æ–°çš„
        new_combinations = []
        total_new_keywords = 0
        
        for story_id, story_data in story_keywords.items():
            new_keywords = story_data.get("new_keywords", [])
            
            for keyword in new_keywords:
                combination = (story_id, keyword)
                if combination not in existing_combinations:
                    new_combinations.append({
                        'story_id': story_id,
                        'term': keyword
                    })
                    total_new_keywords += 1
        
        print(f"æº–å‚™æ’å…¥çš„æ–°çµ„åˆæ•¸é‡: {len(new_combinations)}")
        return new_combinations

    def check_existing_terms(self, word_explanations: Dict) -> List[Dict[str, str]]:
        """æª¢æŸ¥ä¸¦æº–å‚™éœ€è¦æ’å…¥åˆ° term è¡¨çš„æ–°é—œéµå­—å®šç¾©"""
        print("\n=== æª¢æŸ¥ term è¡¨é‡è¤‡æ€§ ===")
        
        # å…ˆå–å¾—ç¾æœ‰çš„æ‰€æœ‰ term
        try:
            table_name = self.db_config['term_table']
            query = self.supabase_client.table(table_name).select('term')
            resp = query.execute()
            
            if getattr(resp, 'error', None):
                print(f"è®€å– {table_name} å¤±æ•—: {resp.error}")
                return []
            
            existing_terms = set()
            for row in resp.data or []:
                term = row.get('term')
                if term:
                    existing_terms.add(term)
            
            print(f"ç¾æœ‰ term è¡¨ä¸­çš„é—œéµå­—æ•¸é‡: {len(existing_terms)}")
            
        except Exception as e:
            print(f"è®€å–ç¾æœ‰ term è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
        
        # æª¢æŸ¥å“ªäº›é—œéµå­—æ˜¯æ–°çš„
        new_terms = []
        
        for word, explanation in word_explanations.items():
            if word not in existing_terms:
                # å¾è§£é‡‹ä¸­æå–å®šç¾©å’Œæ‡‰ç”¨
                definition = explanation.get('definition', '')
                examples = explanation.get('examples', [])
                example_text = examples[0].get('text', '') if examples else ''
                
                new_terms.append({
                    'term': word,
                    'definition': definition,
                    'example': example_text
                })
        
        print(f"æº–å‚™æ’å…¥çš„æ–°é—œéµå­—æ•¸é‡: {len(new_terms)}")
        return new_terms

    def display_term_insertion_preview(self, new_terms: List[Dict[str, str]]):
        """é¡¯ç¤ºæº–å‚™æ’å…¥ term è¡¨çš„è³‡æ–™é è¦½"""
        width = DiffKeywordConfig.OUTPUT_CONFIG['terminal_width']
        
        print("\n" + "=" * width)
        print("  æº–å‚™æ’å…¥ term è¡¨çš„è³‡æ–™é è¦½ (è³‡æ–™åº«æ ¼å¼)")
        print("=" * width)
        
        if not new_terms:
            print("æ²’æœ‰æ–°çš„é—œéµå­—éœ€è¦æ’å…¥åˆ° term è¡¨")
            return
        
        print(f"ç¸½è¨ˆæº–å‚™æ’å…¥: {len(new_terms)} ç­†é—œéµå­—å®šç¾©")
        print(f"ç›®æ¨™è³‡æ–™è¡¨: {self.db_config['term_table']}")
        
        print("\nè³‡æ–™åº«æ’å…¥æ ¼å¼é è¦½ (å‰5ç­†):")
        print("-" * 80)
        print("INSERT INTO term (term, definition, example) VALUES")
        
        # é¡¯ç¤ºå‰5ç­†çš„å®Œæ•´SQLæ ¼å¼
        display_limit = min(5, len(new_terms))
        for i, term_data in enumerate(new_terms[:display_limit]):
            term = term_data['term']
            definition = term_data['definition']
            example = term_data['example']
            
            # è™•ç†å¯èƒ½çš„å–®å¼•è™Ÿè½‰ç¾©
            escaped_term = term.replace("'", "''")
            escaped_definition = definition.replace("'", "''")
            escaped_example = example.replace("'", "''")
            
            if i == len(new_terms) - 1 or i == display_limit - 1:
                print(f"  ('{escaped_term}', '{escaped_definition}', '{escaped_example}');")
            else:
                print(f"  ('{escaped_term}', '{escaped_definition}', '{escaped_example}'),")
        
        if len(new_terms) > display_limit:
            print(f"  ... (é‚„æœ‰ {len(new_terms) - display_limit} ç­†è³‡æ–™)")
        
        print("-" * 80)
        
        print("\nJSON æ ¼å¼é è¦½ (å‰3ç­†):")
        print("-" * 80)
        for i, term_data in enumerate(new_terms[:3]):
            print(f"ç¬¬ {i+1} ç­†:")
            print("  {")
            print(f"    \"term\": \"{term_data['term']}\",")
            print(f"    \"definition\": \"{term_data['definition']}\",")
            print(f"    \"example\": \"{term_data['example']}\"")
            print("  }")
            if i < 2 and i < len(new_terms) - 1:
                print("  ,")
        
        if len(new_terms) > 3:
            print(f"  ... (é‚„æœ‰ {len(new_terms) - 3} ç­†)")
        
        print("-" * 80)
        
        print("\nè©³ç´°å…§å®¹é è¦½ (å‰3ç­†):")
        print("-" * 80)
        for i, term_data in enumerate(new_terms[:3]):
            print(f"\nã€é—œéµå­— {i+1}ã€‘: {term_data['term']}")
            print(f"å®šç¾©: {term_data['definition']}")
            print(f"æ‡‰ç”¨: {term_data['example']}")
            print("-" * 40)
        
        if len(new_terms) > 3:
            print(f"... é‚„æœ‰ {len(new_terms) - 3} å€‹é—œéµå­—")
        
        print("\nçµ±è¨ˆæ‘˜è¦:")
        print("-" * 40)
        print(f"ç¸½è¨ˆæ–°é—œéµå­—: {len(new_terms)} å€‹")
        
        # çµ±è¨ˆå®šç¾©å’Œæ‡‰ç”¨çš„é•·åº¦
        avg_def_length = sum(len(t['definition']) for t in new_terms) / len(new_terms) if new_terms else 0
        avg_example_length = sum(len(t['example']) for t in new_terms) / len(new_terms) if new_terms else 0
        
        print(f"å¹³å‡å®šç¾©é•·åº¦: {avg_def_length:.1f} å­—")
        print(f"å¹³å‡æ‡‰ç”¨é•·åº¦: {avg_example_length:.1f} å­—")
        
        # æª¢æŸ¥ç©ºç™½å…§å®¹
        empty_definitions = sum(1 for t in new_terms if not t['definition'].strip())
        empty_examples = sum(1 for t in new_terms if not t['example'].strip())
        
        if empty_definitions > 0:
            print(f"âš ï¸  ç©ºç™½å®šç¾©: {empty_definitions} å€‹")
        if empty_examples > 0:
            print(f"âš ï¸  ç©ºç™½æ‡‰ç”¨: {empty_examples} å€‹")
        
        print("-" * 40)
        
        print("\n" + "=" * width)
        print("âš ï¸  æ³¨æ„: ä»¥ä¸Šè³‡æ–™å°šæœªæ’å…¥è³‡æ–™åº«ï¼Œåƒ…ä¾›é è¦½ç¢ºèª")
        print("=" * width)

    def display_insertion_preview(self, new_combinations: List[Dict[str, str]]):
        """é¡¯ç¤ºæº–å‚™æ’å…¥ term_map çš„è³‡æ–™é è¦½"""
        width = DiffKeywordConfig.OUTPUT_CONFIG['terminal_width']
        
        print("\n" + "=" * width)
        print("  æº–å‚™æ’å…¥ term_map çš„è³‡æ–™é è¦½ (è³‡æ–™åº«æ ¼å¼)")
        print("=" * width)
        
        if not new_combinations:
            print("æ²’æœ‰æ–°çš„çµ„åˆéœ€è¦æ’å…¥åˆ° term_map")
            return
        
        print(f"ç¸½è¨ˆæº–å‚™æ’å…¥: {len(new_combinations)} ç­†è³‡æ–™")
        print(f"ç›®æ¨™è³‡æ–™è¡¨: {self.db_config['term_map_table']}")
        
        print("\nè³‡æ–™åº«æ’å…¥æ ¼å¼é è¦½:")
        print("-" * 80)
        print("INSERT INTO term_map (story_id, term) VALUES")
        
        # é¡¯ç¤ºå‰10ç­†çš„å®Œæ•´SQLæ ¼å¼
        display_limit = min(10, len(new_combinations))
        for i, combo in enumerate(new_combinations[:display_limit]):
            story_id = combo['story_id']
            term = combo['term']
            
            # è™•ç†å¯èƒ½çš„å–®å¼•è™Ÿè½‰ç¾©
            escaped_term = term.replace("'", "''")
            
            if i == len(new_combinations) - 1 or i == display_limit - 1:
                print(f"  ('{story_id}', '{escaped_term}');")
            else:
                print(f"  ('{story_id}', '{escaped_term}'),")
        
        if len(new_combinations) > display_limit:
            print(f"  ... (é‚„æœ‰ {len(new_combinations) - display_limit} ç­†è³‡æ–™)")
        
        print("-" * 80)
        
        print("\nJSON æ ¼å¼é è¦½ (å‰5ç­†):")
        print("-" * 50)
        for i, combo in enumerate(new_combinations[:5]):
            print(f"ç¬¬ {i+1} ç­†:")
            print("  {")
            print(f"    \"story_id\": \"{combo['story_id']}\",")
            print(f"    \"term\": \"{combo['term']}\"")
            print("  }")
            if i < 4 and i < len(new_combinations) - 1:
                print("  ,")
        
        if len(new_combinations) > 5:
            print(f"  ... (é‚„æœ‰ {len(new_combinations) - 5} ç­†)")
        
        print("-" * 50)
        
        # çµ±è¨ˆæ¯å€‹ story_id çš„æ–°é—œéµå­—æ•¸é‡
        grouped_by_story = {}
        for combo in new_combinations:
            story_id = combo['story_id']
            if story_id not in grouped_by_story:
                grouped_by_story[story_id] = []
            grouped_by_story[story_id].append(combo['term'])
        
        print("\næŒ‰ story_id çµ±è¨ˆ:")
        print("-" * 40)
        for story_id, terms in grouped_by_story.items():
            print(f"Story ID {story_id}: {len(terms)} å€‹æ–°é—œéµå­—")
            # é¡¯ç¤ºå‰3å€‹é—œéµå­—ä½œç‚ºç¤ºä¾‹
            sample_terms = terms[:3]
            print(f"  ç¤ºä¾‹: {', '.join(sample_terms)}")
            if len(terms) > 3:
                print(f"  ... é‚„æœ‰ {len(terms) - 3} å€‹")
            print()
        
        print("-" * 40)
        print(f"ç¸½è¨ˆä¸åŒ story_id: {len(grouped_by_story)} å€‹")
        print(f"ç¸½è¨ˆæ–° term çµ„åˆ: {len(new_combinations)} ç­†")
        
        print("\n" + "=" * width)
        print("âš ï¸  æ³¨æ„: ä»¥ä¸Šè³‡æ–™å°šæœªæ’å…¥è³‡æ–™åº«ï¼Œåƒ…ä¾›é è¦½ç¢ºèª")
        print("=" * width)

    def print_final_results(self, final_results: Dict):
        """å°‡æœ€çµ‚çµæœå°å‡ºåˆ°çµ‚ç«¯æ©Ÿ"""
        width = DiffKeywordConfig.OUTPUT_CONFIG['terminal_width']
        
        print("\n" + "=" * width)
        print("  å›°é›£é—œéµå­—æå–çµæœ")
        print("=" * width)
        
        # å°å‡ºçµ±è¨ˆæ‘˜è¦
        summary = final_results['summary']
        print(f"è™•ç†æ™‚é–“: {summary['processing_date']}")
        print(f"è™•ç†æ–°èæ•¸é‡: {summary['total_news']}")
        print(f"ä¸é‡è¤‡é—œéµå­—ç¸½æ•¸: {summary['total_unique_keywords']}")
        print(f"æˆåŠŸè§£é‡‹è©å½™æ•¸: {summary['successfully_explained_keywords']}")
        
        print("\n" + "-" * width)
        print("  å„æ–°èå›°é›£é—œéµå­—è©³ç´°çµæœ")
        print("-" * width)
        
        # å°å‡ºæ¯ç¯‡æ–°èçš„çµæœ
        for story_id, story_data in final_results['stories'].items():
            print(f"\nğŸ“° Story ID: {story_id}")
            print(f"æ¨™é¡Œ: {story_data['title']}")
            print(f"ç¸½é—œéµå­—æ•¸é‡: {story_data['keyword_count']}")
            
            # é¡¯ç¤ºæ–°æå–å’Œç¾æœ‰é—œéµå­—çš„åˆ†é¡
            if 'new_keyword_count' in story_data and 'existing_term_count' in story_data:
                print(f"  - æ–°æå–é—œéµå­—: {story_data['new_keyword_count']} å€‹")
                print(f"  - ç¾æœ‰ term_map: {story_data['existing_term_count']} å€‹")
            
            if story_data['keywords']:
                print("å›°é›£é—œéµå­—èˆ‡è§£é‡‹:")
                for idx, keyword_data in enumerate(story_data['keywords'], 1):
                    term = keyword_data.get('term', 'N/A')
                    definition = keyword_data.get('definition', 'ç„¡è§£é‡‹')
                    examples = keyword_data.get('examples', [])
                    source = keyword_data.get('source', 'æœªçŸ¥')  # æ¨™è¨˜ä¾†æº
                    
                    source_icon = "ğŸ†•" if source == "new" else "ğŸ“‹" if source == "existing" else "â“"
                    print(f"  {idx}. {source_icon} ã€{term}ã€‘")
                    print(f"     å®šç¾©: {definition}")
                    
                    if examples:
                        example_text = examples[0].get('text', 'ç„¡ç¯„ä¾‹') if examples else 'ç„¡ç¯„ä¾‹'
                        print(f"     æ‡‰ç”¨: {example_text}")
                    print()
            else:
                print("  (ç„¡æå–åˆ°å›°é›£é—œéµå­—)")
            
            print("-" * 40)
        
        print("\n" + "=" * width)
        print("çµæœè¼¸å‡ºå®Œç•¢")
        print("=" * width)
        """å°‡æœ€çµ‚çµæœå°å‡ºåˆ°çµ‚ç«¯æ©Ÿ"""
        width = DiffKeywordConfig.OUTPUT_CONFIG['terminal_width']
        
        print("\n" + "=" * width)
        print("  å›°é›£é—œéµå­—æå–çµæœ")
        print("=" * width)
        
        # å°å‡ºçµ±è¨ˆæ‘˜è¦
        summary = final_results['summary']
        print(f"è™•ç†æ™‚é–“: {summary['processing_date']}")
        print(f"è™•ç†æ–°èæ•¸é‡: {summary['total_news']}")
        print(f"ä¸é‡è¤‡é—œéµå­—ç¸½æ•¸: {summary['total_unique_keywords']}")
        print(f"æˆåŠŸè§£é‡‹è©å½™æ•¸: {summary['successfully_explained_keywords']}")
        
        print("\n" + "-" * width)
        print("  å„æ–°èå›°é›£é—œéµå­—è©³ç´°çµæœ")
        print("-" * width)
        
        # å°å‡ºæ¯ç¯‡æ–°èçš„çµæœ
        for story_id, story_data in final_results['stories'].items():
            print(f"\nğŸ“° Story ID: {story_id}")
            print(f"æ¨™é¡Œ: {story_data['title']}")
            print(f"ç¸½é—œéµå­—æ•¸é‡: {story_data['keyword_count']}")
            
            # é¡¯ç¤ºæ–°æå–å’Œç¾æœ‰é—œéµå­—çš„åˆ†é¡
            if 'new_keyword_count' in story_data and 'existing_term_count' in story_data:
                print(f"  - æ–°æå–é—œéµå­—: {story_data['new_keyword_count']} å€‹")
                print(f"  - ç¾æœ‰ term_map: {story_data['existing_term_count']} å€‹")
            
            if story_data['keywords']:
                print("å›°é›£é—œéµå­—èˆ‡è§£é‡‹:")
                for idx, keyword_data in enumerate(story_data['keywords'], 1):
                    term = keyword_data.get('term', 'N/A')
                    definition = keyword_data.get('definition', 'ç„¡è§£é‡‹')
                    examples = keyword_data.get('examples', [])
                    source = keyword_data.get('source', 'æœªçŸ¥')  # æ¨™è¨˜ä¾†æº
                    
                    source_icon = "ğŸ†•" if source == "new" else "ğŸ“‹" if source == "existing" else "â“"
                    print(f"  {idx}. {source_icon} ã€{term}ã€‘")
                    print(f"     å®šç¾©: {definition}")
                    
                    if examples:
                        example_text = examples[0].get('text', 'ç„¡ç¯„ä¾‹') if examples else 'ç„¡ç¯„ä¾‹'
                        print(f"     æ‡‰ç”¨: {example_text}")
                    print()
            else:
                print("  (ç„¡æå–åˆ°å›°é›£é—œéµå­—)")
            
            print("-" * 40)
        
        print("\n" + "=" * width)
        print("çµæœè¼¸å‡ºå®Œç•¢")
        print("=" * width)

    def run(self, limit: Optional[int] = None):
        """åŸ·è¡Œå®Œæ•´çš„å›°é›£é—œéµå­—æå–æµç¨‹"""
        if not self.is_ready():
            print("âœ— ç³»çµ±æœªå°±ç·’ï¼Œç„¡æ³•åŸ·è¡Œ")
            return

        print("\n" + "=" * 80)
        print("  å›°é›£é—œéµå­—æå–ç³»çµ±")
        print("=" * 80)

        # 1. è®€å–ä¸¦åˆä½µ Supabase single_news å’Œ term_map è³‡æ–™
        news_data = self.fetch_combined_data(limit)
        if not news_data:
            print("æœªå–å¾—ä»»ä½•è³‡æ–™")
            return

        # 2. æå–æ‰€æœ‰é—œéµå­—ï¼Œä¸¦æ ¹æ“š story_id çµ„ç¹”
        print("\n=== éšæ®µä¸€ï¼šå¾æ–°èä¸­æå–å›°é›£é—œéµå­— ===")
        story_keywords = {}
        all_keywords: Set[str] = set()
        
        content_field = self.db_config['primary_content_field']
        title_field = self.db_config['title_field']
        
        for news in tqdm(news_data, desc="è™•ç†æ–°è"):
            story_id = news.get('story_id')
            if story_id is None:
                continue

            title = news.get(title_field, 'æœªçŸ¥æ¨™é¡Œ')
            content = news.get(content_field, '')
            existing_terms = news.get('existing_terms', [])
            
            if not content:
                print(f"âš  story_id {story_id} çš„ {content_field} æ¬„ä½ç‚ºç©ºï¼Œè·³é")
                continue
            
            # æå–é—œéµå­—
            keywords = self.extract_keywords_from_text(content, title)
            
            # åˆä½µæ–°æå–çš„é—œéµå­—å’Œç¾æœ‰çš„ terms
            all_story_keywords = list(set(keywords + existing_terms))
            
            # æ›´æ–°ç¸½é—œéµå­—é›†åˆ
            all_keywords.update(all_story_keywords)
            
            # å°‡é—œéµå­—åŠ å…¥å°æ‡‰çš„ story_id
            story_keywords[story_id] = {
                "title": title,
                "keywords": all_story_keywords,
                "new_keywords": keywords,
                "existing_terms": existing_terms
            }

        unique_keywords = sorted(list(all_keywords))
        print(f"âœ“ éšæ®µä¸€å®Œæˆï¼šå…±æå– {len(unique_keywords)} å€‹ä¸é‡è¤‡é—œéµå­—ã€‚")
        
        # å°å‡ºé€²åº¦æ‘˜è¦
        self.print_progress_summary(story_keywords, all_keywords)

        # 3. ç‚ºé—œéµå­—ç”Ÿæˆè§£é‡‹
        print("\n=== éšæ®µäºŒï¼šç‚ºé—œéµå­—ç”Ÿæˆè§£é‡‹èˆ‡ç¯„ä¾‹ ===")
        word_explanations = {}
        for word in tqdm(unique_keywords, desc="ç”Ÿæˆè©å½™è§£é‡‹"):
            explanation = self.get_word_explanation(word)
            if explanation and "term" in explanation:
                word_explanations[word] = explanation
            else:
                print(f"âš  æœªèƒ½æˆåŠŸè§£é‡‹è©å½™ï¼š'{word}'")
        
        print(f"âœ“ éšæ®µäºŒå®Œæˆï¼šå…±æˆåŠŸè§£é‡‹ {len(word_explanations)} å€‹è©å½™ã€‚")

        # 4. æ•´ç†ä¸¦å„²å­˜æœ€çµ‚çµæœ
        print("\n=== éšæ®µä¸‰ï¼šæ•´ç†æœ€çµ‚çµæœ ===")
        final_results = {
            "summary": {
                "total_news": len(story_keywords),
                "total_unique_keywords": len(unique_keywords),
                "successfully_explained_keywords": len(word_explanations),
                "processing_date": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            "stories": {},
            "word_explanations": word_explanations
        }
        
        # å°‡è§£é‡‹åŠ å…¥åˆ°æ¯å€‹ story çš„é—œéµå­—ä¸­
        for story_id, story_data in story_keywords.items():
            keywords_with_explanations = []
            new_keywords = story_data.get("new_keywords", [])
            existing_terms = story_data.get("existing_terms", [])
            
            # è™•ç†æ–°æå–çš„é—œéµå­—
            for word in new_keywords:
                if word in word_explanations:
                    explanation = word_explanations[word].copy()
                    explanation['source'] = 'new'
                    keywords_with_explanations.append(explanation)
            
            # è™•ç†ç¾æœ‰çš„ terms
            for word in existing_terms:
                if word in word_explanations:
                    explanation = word_explanations[word].copy()
                    explanation['source'] = 'existing'
                    keywords_with_explanations.append(explanation)
            
            final_results["stories"][story_id] = {
                "title": story_data["title"],
                "keywords": keywords_with_explanations,
                "keyword_count": len(keywords_with_explanations),
                "new_keyword_count": len([k for k in keywords_with_explanations if k.get('source') == 'new']),
                "existing_term_count": len([k for k in keywords_with_explanations if k.get('source') == 'existing'])
            }

        # 5. æª¢æŸ¥ä¸¦é è¦½æº–å‚™æ’å…¥ term_map çš„è³‡æ–™
        new_combinations = self.check_existing_term_combinations(story_keywords)
        self.display_insertion_preview(new_combinations)

        # 6. æª¢æŸ¥ä¸¦é è¦½æº–å‚™æ’å…¥ term è¡¨çš„è³‡æ–™
        new_terms = self.check_existing_terms(word_explanations)
        self.display_term_insertion_preview(new_terms)

        # 7. å°å‡ºçµæœåˆ°çµ‚ç«¯æ©Ÿ
        self.print_final_results(final_results)

        print("\nâœ“ è™•ç†å®Œæˆï¼")
        print(f"- è™•ç†æ–°èæ•¸é‡: {final_results['summary']['total_news']}")
        print(f"- ä¸é‡è¤‡é—œéµå­—: {final_results['summary']['total_unique_keywords']}")
        print(f"- æˆåŠŸè§£é‡‹è©å½™: {final_results['summary']['successfully_explained_keywords']}")
        
        if new_combinations:
            print(f"- æº–å‚™æ’å…¥ term_map: {len(new_combinations)} ç­†æ–°çµ„åˆ")
        if new_terms:
            print(f"- æº–å‚™æ’å…¥ term: {len(new_terms)} å€‹æ–°é—œéµå­—")


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    print("=" * 80)
    print("  å›°é›£é—œéµå­—æå–ç³»çµ± - åŸºæ–¼ Supabase single_news")
    print("=" * 80)
    
    # è§£ææŒ‡ä»¤åˆ—åƒæ•¸
    limit = None
    if len(sys.argv) > 1:
        try:
            limit = int(sys.argv[1])
            print(f"âœ“ è¨­å®šè®€å–é™åˆ¶: {limit} ç­†")
        except ValueError:
            print("âš  ç„¡æ•ˆçš„ limit åƒæ•¸ï¼Œå°‡è®€å–æ‰€æœ‰è³‡æ–™")
            limit = None
    
    try:
        # åˆå§‹åŒ–ä¸¦åŸ·è¡Œè™•ç†å™¨
        processor = DiffKeywordProcessor()
        if processor.is_ready():
            processor.run(limit)
        
    except EnvironmentError as e:
        print(f"âœ— ç’°å¢ƒéŒ¯èª¤ï¼š{e}")
        print("è«‹æª¢æŸ¥æ‚¨çš„ .env è¨­å®šæª”ã€‚")
        sys.exit(1)
    except Exception as e:
        print(f"âœ— ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š{e}")
        sys.exit(1)
        
    print("\n" + "=" * 80)
    print("ç³»çµ±åŸ·è¡Œå®Œç•¢ã€‚")
    print("=" * 80)


# åœ¨ DiffKeywordProcessor é¡ä¸­æ·»åŠ æ’å…¥æ–¹æ³•ï¼ˆéœ€è¦æ‰‹å‹•ç§»å‹•åˆ°é¡å…§ï¼‰
def insert_term_map_data(self, new_combinations: List[Dict[str, str]]) -> bool:
    """å°‡æ–°çš„ term_map çµ„åˆæ’å…¥è³‡æ–™åº«"""
    if not new_combinations:
        print("æ²’æœ‰ term_map è³‡æ–™éœ€è¦æ’å…¥")
        return True
    
    print(f"\n=== é–‹å§‹æ’å…¥ term_map è³‡æ–™ ===")
    print(f"æº–å‚™æ’å…¥ {len(new_combinations)} ç­†è³‡æ–™åˆ° {self.db_config['term_map_table']} è¡¨")
    
    success_count = 0
    error_count = 0
    
    try:
        table_name = self.db_config['term_map_table']
        
        # æ‰¹æ¬¡æ’å…¥
        batch_size = 100  # æ¯æ‰¹æ’å…¥100ç­†
        for i in range(0, len(new_combinations), batch_size):
            batch = new_combinations[i:i + batch_size]
            
            try:
                resp = self.supabase_client.table(table_name).insert(batch).execute()
                
                if getattr(resp, 'error', None):
                    print(f"æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥å¤±æ•—: {resp.error}")
                    error_count += len(batch)
                else:
                    batch_success = len(batch)
                    success_count += batch_success
                    print(f"âœ“ æ‰¹æ¬¡ {i//batch_size + 1}: æˆåŠŸæ’å…¥ {batch_success} ç­†")
            
            except Exception as e:
                print(f"âœ— æ‰¹æ¬¡ {i//batch_size + 1} ç™¼ç”ŸéŒ¯èª¤: {e}")
                error_count += len(batch)
    
    except Exception as e:
        print(f"âœ— æ’å…¥ term_map æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False
    
    print(f"\nterm_map æ’å…¥çµæœ:")
    print(f"  æˆåŠŸ: {success_count} ç­†")
    print(f"  å¤±æ•—: {error_count} ç­†")
    
    return error_count == 0

def insert_term_data(self, new_terms: List[Dict[str, str]]) -> bool:
    """å°‡æ–°çš„é—œéµå­—å®šç¾©æ’å…¥ term è¡¨"""
    if not new_terms:
        print("æ²’æœ‰ term è³‡æ–™éœ€è¦æ’å…¥")
        return True
    
    print(f"\n=== é–‹å§‹æ’å…¥ term è³‡æ–™ ===")
    print(f"æº–å‚™æ’å…¥ {len(new_terms)} ç­†è³‡æ–™åˆ° {self.db_config['term_table']} è¡¨")
    
    success_count = 0
    error_count = 0
    
    try:
        table_name = self.db_config['term_table']
        
        # æ‰¹æ¬¡æ’å…¥
        batch_size = 50  # term è¡¨è³‡æ–™è¼ƒå¤§ï¼Œæ¯æ‰¹æ’å…¥50ç­†
        for i in range(0, len(new_terms), batch_size):
            batch = new_terms[i:i + batch_size]
            
            try:
                resp = self.supabase_client.table(table_name).insert(batch).execute()
                
                if getattr(resp, 'error', None):
                    print(f"æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥å¤±æ•—: {resp.error}")
                    error_count += len(batch)
                else:
                    batch_success = len(batch)
                    success_count += batch_success
                    print(f"âœ“ æ‰¹æ¬¡ {i//batch_size + 1}: æˆåŠŸæ’å…¥ {batch_success} ç­†")
            
            except Exception as e:
                print(f"âœ— æ‰¹æ¬¡ {i//batch_size + 1} ç™¼ç”ŸéŒ¯èª¤: {e}")
                error_count += len(batch)
    
    except Exception as e:
        print(f"âœ— æ’å…¥ term æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False
    
    print(f"\nterm æ’å…¥çµæœ:")
    print(f"  æˆåŠŸ: {success_count} ç­†")
    print(f"  å¤±æ•—: {error_count} ç­†")
    
    return error_count == 0


if __name__ == "__main__":
    main()
