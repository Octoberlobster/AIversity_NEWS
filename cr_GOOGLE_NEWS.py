from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
import time
import datetime as dt
from datetime import datetime, timedelta
import requests
from supabase import create_client, Client
import uuid
import os
import json
import random
import re
from urllib.parse import urljoin, urlparse
from collections import defaultdict

# === Supabase è¨­å®š ===
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
if SUPABASE_URL and SUPABASE_KEY:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
else:
    supabase = None
    print("âš ï¸ Supabase æœªè¨­å®šï¼Œå°‡è·³éé‡è¤‡æª¢æŸ¥åŠŸèƒ½")

# Chrome options è¨­å®š
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-web-security")
chrome_options.add_argument("--disable-features=VizDisplayCompositor")
chrome_options.add_argument("--page-load-strategy=eager")

# ç”¨æˆ¶ä»£ç†
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36")

# é˜²æ­¢è¢«è­˜åˆ¥ç‚ºè‡ªå‹•åŒ–
chrome_options.add_argument('--disable-blink-features=AutomationControlled')
chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
chrome_options.add_experimental_option('useAutomationExtension', False)

# å»£å‘Šå’Œè¿½è¹¤é˜»æ“‹
chrome_options.add_argument("--disable-background-timer-throttling")
chrome_options.add_argument("--disable-backgrounding-occluded-windows")
chrome_options.add_argument("--disable-renderer-backgrounding")
chrome_options.add_argument("--disable-features=TranslateUI")
chrome_options.add_argument("--disable-ipc-flooding-protection")

# åœ–ç‰‡å’Œåª’é«”å„ªåŒ–
chrome_options.add_argument("--disable-background-media")
chrome_options.add_argument("--disable-background-downloads")
chrome_options.add_argument("--aggressive-cache-discard")
chrome_options.add_argument("--disable-sync")

# ç¶²è·¯å„ªåŒ–
chrome_options.add_argument("--disable-default-apps")
chrome_options.add_argument("--disable-extensions")
chrome_options.add_argument("--disable-plugins")
chrome_options.add_argument("--disable-notifications")
chrome_options.add_argument("--disable-popup-blocking")

# è¨˜æ†¶é«”å’Œæ•ˆèƒ½å„ªåŒ–
chrome_options.add_argument("--memory-pressure-off")
chrome_options.add_argument("--max_old_space_size=4096")
chrome_options.add_argument("--single-process")
chrome_options.add_argument("--no-zygote")

# é˜»æ“‹ç‰¹å®šå…§å®¹é¡å‹
prefs = {
    "profile.default_content_setting_values": {
        "notifications": 2,  # é˜»æ“‹é€šçŸ¥
        "plugins": 2,        # é˜»æ“‹æ’ä»¶
        "popups": 2,         # é˜»æ“‹å½ˆå‡ºçª—å£
        "geolocation": 2,    # é˜»æ“‹ä½ç½®è«‹æ±‚
        "media_stream": 2,   # é˜»æ“‹æ”åƒé ­/éº¥å…‹é¢¨
    },
    "profile.managed_default_content_settings": {
        "images": 2,         # 1=å…è¨±, 2=é˜»æ“‹åœ–ç‰‡
    },
    "profile.default_content_settings": {
        "popups": 2
    }
}
chrome_options.add_experimental_option("prefs", prefs)

def check_story_exists_in_supabase(story_url, category):
    """
    æª¢æŸ¥story_urlæ˜¯å¦å·²ç¶“åœ¨Supabaseä¸­å­˜åœ¨
    å¦‚æœå­˜åœ¨ä¸”crawl_dateåœ¨ä¸‰å¤©å…§ï¼Œå‰‡è¿”å›Trueï¼ˆè·³éï¼‰
    å¦‚æœå­˜åœ¨ä½†crawl_dateè¶…éä¸‰å¤©ï¼Œå‰‡è¿”å›Falseï¼ˆéœ€è¦é‡æ–°çˆ¬å–ï¼‰
    å¦‚æœä¸å­˜åœ¨ï¼Œå‰‡è¿”å›Falseï¼ˆéœ€è¦çˆ¬å–ï¼‰
    
    Returns:
        tuple: (should_skip, message, is_old_story)
            - should_skip (bool): Trueè¡¨ç¤ºè·³éï¼ŒFalseè¡¨ç¤ºéœ€è¦çˆ¬å–
            - message (str): èªªæ˜ä¿¡æ¯
            - is_old_story (bool): Trueè¡¨ç¤ºæ˜¯èˆŠæ•…äº‹ï¼ˆè¶…é3å¤©ï¼‰ï¼ŒFalseè¡¨ç¤ºæ–°æ•…äº‹æˆ–ä¸å­˜åœ¨
    """
    if not supabase:
        return False, "Supabaseæœªè¨­å®š", False
    
    try:
        # å‡è¨­æ‚¨çš„è¡¨åç‚º 'stories'ï¼Œè«‹æ ¹æ“šå¯¦éš›æƒ…æ³ä¿®æ”¹
        response = supabase.table('stories').select('*').eq('story_url', story_url).eq('category', category).execute()
        
        if not response.data:
            return False, "æ•…äº‹ä¸å­˜åœ¨æ–¼è³‡æ–™åº«ä¸­ï¼Œéœ€è¦çˆ¬å–", False
        
        # å–å¾—æœ€æ–°çš„è¨˜éŒ„
        latest_record = max(response.data, key=lambda x: x['crawl_date'])
        crawl_date_str = latest_record['crawl_date']
        
        # è§£æcrawl_dateï¼ˆå‡è¨­æ ¼å¼ç‚º "2024/01/15 10:30"ï¼‰
        try:
            crawl_date = datetime.strptime(crawl_date_str, "%Y/%m/%d %H:%M")
        except ValueError:
            # å¦‚æœæ—¥æœŸæ ¼å¼ä¸åŒ¹é…ï¼Œå˜—è©¦å…¶ä»–æ ¼å¼
            try:
                crawl_date = datetime.fromisoformat(crawl_date_str.replace('Z', '+00:00'))
            except:
                return False, "ç„¡æ³•è§£æcrawl_dateï¼Œéœ€è¦é‡æ–°çˆ¬å–", False
        
        # è¨ˆç®—æ™‚é–“å·®
        current_time = datetime.now()
        time_difference = current_time - crawl_date
        
        if time_difference.days >= 3:
            return False, f"ä¸Šæ¬¡çˆ¬å–æ™‚é–“è¶…é3å¤©ï¼ˆ{time_difference.days}å¤©å‰ï¼‰ï¼Œéœ€è¦é‡æ–°çˆ¬å–3å¤©å…§æ–°è", True
        else:
            return True, f"ä¸Šæ¬¡çˆ¬å–æ™‚é–“åœ¨3å¤©å…§ï¼ˆ{time_difference.days}å¤©å‰ï¼‰ï¼Œè·³é", False
            
    except Exception as e:
        print(f"âŒ æª¢æŸ¥Supabaseæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False, "æª¢æŸ¥è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œé»˜èªé€²è¡Œçˆ¬å–", False

def parse_article_datetime(datetime_str):
    """
    è§£ææ–‡ç« çš„datetimeå­—ç¬¦ä¸²ç‚ºdatetimeå°è±¡
    
    Args:
        datetime_str (str): æ–‡ç« çš„datetimeå­—ç¬¦ä¸² (ä¾‹å¦‚: "2025-08-15T12:55:21Z")
        
    Returns:
        datetime or None: è§£ææˆåŠŸè¿”å›datetimeå°è±¡ï¼Œå¤±æ•—è¿”å›None
    """
    if not datetime_str or datetime_str == "æœªçŸ¥æ™‚é–“":
        return None
    
    try:
        # å„ªå…ˆè™•ç† ISO æ ¼å¼ (ä¾‹å¦‚: "2025-08-15T12:55:21Z")
        if 'T' in datetime_str:
            # è™•ç†å¸¶ Z çµå°¾çš„ UTC æ™‚é–“
            if datetime_str.endswith('Z'):
                # ç§»é™¤ Z ä¸¦è½‰æ›ç‚º UTC æ ¼å¼
                datetime_str = datetime_str.replace('Z', '+00:00')
            
            # ä½¿ç”¨ fromisoformat è§£æ
            dt = datetime.fromisoformat(datetime_str)
            
            # å¦‚æœæœ‰æ™‚å€ä¿¡æ¯ï¼Œè½‰æ›ç‚ºæœ¬åœ°æ™‚é–“ï¼ˆç§»é™¤æ™‚å€ä¿¡æ¯ï¼‰
            if dt.tzinfo:
                dt = dt.replace(tzinfo=None)
            
            return dt
        else:
            # å˜—è©¦å…¶ä»–å¸¸è¦‹æ ¼å¼
            formats = [
                "%Y-%m-%d %H:%M:%S",
                "%Y/%m/%d %H:%M",
                "%Y-%m-%d",
                "%Y/%m/%d"
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(datetime_str, fmt)
                except ValueError:
                    continue
    except Exception as e:
        print(f"âš ï¸ è§£ææ™‚é–“æ ¼å¼å¤±æ•—: {datetime_str}, éŒ¯èª¤: {e}")
    
    return None

def is_article_within_days(article_datetime_str, days=3):
    """
    æª¢æŸ¥æ–‡ç« æ˜¯å¦åœ¨æŒ‡å®šå¤©æ•¸å…§
    
    Args:
        article_datetime_str (str): æ–‡ç« çš„datetimeå­—ç¬¦ä¸²
        days (int): å¤©æ•¸é™åˆ¶ï¼Œé»˜èª3å¤©
        
    Returns:
        bool: Trueè¡¨ç¤ºåœ¨æŒ‡å®šå¤©æ•¸å…§ï¼ŒFalseè¡¨ç¤ºè¶…éæˆ–ç„¡æ³•åˆ¤æ–·
    """
    if not article_datetime_str or article_datetime_str == "æœªçŸ¥æ™‚é–“":
        # å¦‚æœæ²’æœ‰æ™‚é–“ä¿¡æ¯ï¼Œä¿éšªèµ·è¦‹è¿”å›Trueï¼ˆå…è¨±æŠ“å–ï¼‰
        return True
    
    article_dt = parse_article_datetime(article_datetime_str)
    if not article_dt:
        # ç„¡æ³•è§£ææ™‚é–“ï¼Œä¿éšªèµ·è¦‹è¿”å›True
        return True
    
    # ç§»é™¤æ™‚å€ä¿¡æ¯é€²è¡Œæ¯”è¼ƒï¼ˆç°¡åŒ–è™•ç†ï¼‰
    if article_dt.tzinfo:
        article_dt = article_dt.replace(tzinfo=None)
    
    current_time = datetime.now()
    time_difference = current_time - article_dt
    
    return time_difference.days < days

def get_main_story_links(main_url, category):
    """æ­¥é©Ÿ 1: å¾ä¸»é æŠ“å–æ‰€æœ‰ä¸»è¦æ•…äº‹é€£çµ"""
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    story_links = []
    
    try:
        print(f"ğŸ” æ­£åœ¨æŠ“å– {category} é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµ...")
        driver.get(main_url)
        
        # ç­‰å¾…é é¢è¼‰å…¥ - æ‰¾åˆ°æ‰€æœ‰ c-wiz å€å¡Š
        wait = WebDriverWait(driver, 15)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'c-wiz[jsrenderer="jeGyVb"]')))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        c_wiz_blocks = soup.find_all("c-wiz", {"jsrenderer": "jeGyVb"})
        
        print(f"âœ… æ‰¾åˆ° {len(c_wiz_blocks)} å€‹ c-wiz å€å¡Š")
        
        # å¾æ¯å€‹ c-wiz å€å¡Šä¸­æ‰¾åˆ°ä¸»è¦æ•…äº‹é€£çµ
        for i, block in enumerate(c_wiz_blocks, start=1):
            story_link = block.find("a", class_="jKHa4e")  # ä¸»è¦æ•…äº‹é€£çµ
            
            if story_link:
                href = story_link.get("href")
                title = story_link.text.strip()
                
                if href:
                    # è™•ç†ç›¸å°é€£çµ
                    if href.startswith("./"):
                        full_link = "https://news.google.com" + href[1:]
                    else:
                        full_link = "https://news.google.com" + href
                    
                    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–¼Supabaseä¸­
                    should_skip, skip_reason, is_old_story = check_story_exists_in_supabase(full_link, category)
                    
                    if should_skip:
                        print(f"â­ï¸  è·³éæ•…äº‹ {i}: [{category}] {title}")
                        print(f"   åŸå› : {skip_reason}")
                        continue
                    
                    print(f"   è™•ç†æ•…äº‹ {i}: {href}")
                    print(f"   ğŸ“‹ æª¢æŸ¥çµæœ: {skip_reason}")
                    
                    # ç‚ºæ¯å€‹æ•…äº‹ç”Ÿæˆ UUID
                    story_id = str(uuid.uuid4())
                    
                    story_links.append({
                        "index": i,
                        "story_id": story_id,  # ä½¿ç”¨ UUID
                        "title": title,
                        "url": full_link,
                        "category": category,  # æ–°å¢ï¼šå°‡åˆ†é¡è³‡è¨ŠåŠ å…¥
                        "is_old_story": is_old_story  # æ–°å¢ï¼šæ¨™è¨˜æ˜¯å¦ç‚ºèˆŠæ•…äº‹
                    })
                    
                    print(f"{i}. ğŸ“° [{category}] {title}")
                    print(f"   ğŸ†” æ•…äº‹ID: {story_id}")
                    print(f"   ğŸ”— {full_link}")
                    if is_old_story:
                        print(f"   âš ï¸ èˆŠæ•…äº‹ï¼šåªæœƒæŠ“å–3å¤©å…§çš„æ–°è")
        
        print(f"\nğŸ“Š ç¸½å…±æ”¶é›†åˆ° {len(story_links)} å€‹ {category} é ˜åŸŸéœ€è¦è™•ç†çš„ä¸»è¦æ•…äº‹é€£çµ")
        
    except Exception as e:
        print(f"âŒ æŠ“å–ä¸»è¦æ•…äº‹é€£çµæ™‚å‡ºéŒ¯: {e}")
    finally:
        driver.quit()
    
    return story_links

def get_article_links_from_story(story_info):
    """æ­¥é©Ÿ 2: é€²å…¥æ¯å€‹æ•…äº‹é é¢ï¼Œæ‰¾å‡ºæ‰€æœ‰ article ä¸‹çš„æ–‡ç« é€£çµå’Œç›¸é—œä¿¡æ¯"""
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    article_links = []
    
    try:
        print(f"\nğŸ” æ­£åœ¨è™•ç†æ•…äº‹ {story_info['index']}: [{story_info['category']}] {story_info['title']}")
        print(f"   ğŸ†” æ•…äº‹ID: {story_info['story_id']}")
        if story_info.get('is_old_story', False):
            print(f"   ğŸ“… èˆŠæ•…äº‹æ¨¡å¼ï¼šåªæŠ“å–3å¤©å…§çš„æ–°èæ–‡ç« ")
        
        driver.get(story_info['url'])
        time.sleep(random.randint(3, 6))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # æ‰¾åˆ°æ‰€æœ‰ article tag class="MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc"
        article_elements = soup.find_all("article", class_="MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc")
        
        print(f"   âœ… æ‰¾åˆ° {len(article_elements)} å€‹ article å…ƒç´ ")
        
        filtered_count = 0
        processed_count = 0
        
        for j, article in enumerate(article_elements, start=1):
            if processed_count >= 20:
                break  # æœ€å¤šåªæŠ“å– 20 ç¯‡æ–‡ç« 

            # åœ¨ article ä¸‹é¢æ‰¾ h4 tag class="ipQwMb ekueJc RD0gLb"
            h4_element = article.find("h4", class_="ipQwMb ekueJc RD0gLb")
            
            if h4_element:
                # åœ¨ h4 ä¸‹é¢æ‰¾ a tag class="DY5T1d RZIKme"
                link = h4_element.find("a", class_="DY5T1d RZIKme")
                
                if link:
                    href = link.get("href")
                    link_text = link.text.strip()
                    
                    # æ‰¾åª’é«”ä¾†æº a tag class="wEwyrc"
                    media_element = article.find("a", class_="wEwyrc")
                    media = media_element.text.strip() if media_element else "æœªçŸ¥ä¾†æº"

                    if media == "MSN":
                        continue

                    # æ‰¾æ™‚é–“ class="WW6dff uQIVzc Sksgp slhocf"
                    time_element = article.find(class_="WW6dff uQIVzc Sksgp slhocf")
                    article_datetime = time_element.get("datetime") if time_element and time_element.get("datetime") else "æœªçŸ¥æ™‚é–“"
                    
                    # å¦‚æœæ˜¯èˆŠæ•…äº‹ï¼Œæª¢æŸ¥æ–‡ç« æ˜¯å¦åœ¨3å¤©å…§
                    if story_info.get('is_old_story', False):
                        if not is_article_within_days(article_datetime, days=3):
                            filtered_count += 1
                            print(f"     {j}. â­ï¸ è·³éèˆŠæ–‡ç« : {link_text}")
                            print(f"        ğŸ“… ç™¼å¸ƒæ™‚é–“: {article_datetime}")
                            continue
                    
                    if href:
                        # è™•ç†ç›¸å°é€£çµ
                        if href.startswith("./"):
                            full_href = "https://news.google.com" + href[1:]
                        else:
                            full_href = "https://news.google.com" + href
                        
                        article_links.append({
                            "story_id": story_info['story_id'],  # ä½¿ç”¨ story_id è€Œé story_index
                            "story_title": story_info['title'],
                            "story_category": story_info['category'],  # æ•…äº‹åˆ†é¡
                            "story_url": story_info['url'],
                            "article_index": processed_count + 1,  # ä½¿ç”¨è™•ç†è¨ˆæ•¸å™¨
                            "article_title": link_text,
                            "article_url": full_href,
                            "media": media,  # åª’é«”ä¾†æº
                            "article_datetime": article_datetime,  # æ–°å¢ï¼šä¿å­˜æ–‡ç« æ™‚é–“
                        })
                        
                        processed_count += 1
                        print(f"     {processed_count}. ğŸ“„ {link_text}")
                        print(f"        ğŸ¢ åª’é«”: {media}")
                        print(f"        ğŸ“… æ™‚é–“: {article_datetime}")
                        print(f"        ğŸ”— {full_href}")
                    else:
                        print(f"     {j}. âŒ h4 å…ƒç´ ä¸­æ²’æœ‰æ‰¾åˆ°å°æ‡‰çš„ a tag")
                else:
                    print(f"     {j}. âŒ h4 å…ƒç´ ä¸­æ²’æœ‰æ‰¾åˆ°å°æ‡‰çš„ a tag")
            else:
                print(f"     {j}. âŒ article å…ƒç´ ä¸­æ²’æœ‰æ‰¾åˆ° h4 tag")
        
        if story_info.get('is_old_story', False) and filtered_count > 0:
            print(f"   ğŸ“Š èˆŠæ•…äº‹éæ¿¾çµ±è¨ˆ: è·³é {filtered_count} ç¯‡è¶…é3å¤©çš„æ–‡ç« ï¼Œè™•ç† {processed_count} ç¯‡3å¤©å…§æ–‡ç« ")
        
    except Exception as e:
        print(f"âŒ è™•ç†æ•…äº‹æ™‚å‡ºéŒ¯: {e}")
    finally:
        driver.quit()
    
    return article_links

def get_final_content(article_info, driver):
    """æ­¥é©Ÿ 3: è·³è½‰åˆ°åŸå§‹ç¶²ç«™ä¸¦æŠ“å–å…§å®¹ã€åœ–ç‰‡å’Œæ™‚é–“"""

    MAX_RETRIES = 2
    TIMEOUT = 15
    
    for attempt in range(MAX_RETRIES):
        try:
            print(f"   å˜—è©¦ç¬¬ {attempt + 1} æ¬¡è¨ªå•...")
            
            # è¨­ç½®é¡µé¢åŠ è½½è¶…æ™‚
            driver.set_page_load_timeout(TIMEOUT)
            
            # ä½¿ç”¨ try-except å¤„ç†é¡µé¢åŠ è½½è¶…æ—¶
            try:
                driver.refresh()
                print(f"   åˆ·æ–°é é¢: {article_info['article_url']}")
                driver.get(article_info['article_url'])
            except TimeoutException:
                driver.refresh()
                print(f"   âš ï¸ é é¢åŠ è¼‰è¶…æ™‚ï¼Œä½†ç¹¼çºŒå˜—è©¦ç²å–å…§å®¹...")
                if attempt < MAX_RETRIES - 1:
                    print(f"   ğŸ”„ {TIMEOUT//2} ç§’å¾Œé‡è©¦...")
                    time.sleep(TIMEOUT//2)
                    continue
                else:
                    return None
            
            # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿå°è¯•è·å–å·²åŠ è½½çš„å†…å®¹
            except WebDriverException as e:
                driver.refresh()
                print(f"   âŒ WebDriver éŒ¯èª¤: {e}")
                if attempt < MAX_RETRIES - 1:
                    print(f"   ğŸ”„ {TIMEOUT//2} ç§’å¾Œé‡è©¦...")
                    time.sleep(TIMEOUT//2)
                    continue
                else:
                    return None
            
            # ç­‰å¾…ä¸€å°æ®µæ™‚é–“è®“é é¢æ¸²æŸ“
            time.sleep(random.randint(4, 6))
            
            # å–å¾—è·³è½‰å¾Œçš„çœŸå¯¦ç¶²å€
            try:
                # æª¢æŸ¥æ˜¯å¦ç‚ºéœ€è¦è·³éçš„ URL
                skip_patterns = [
                    "https://www.gamereactor.cn/video",
                    "https://wantrich.chinatimes.com",
                    "https://taongafarm.site", 
                    "https://www.cmoney.tw",
                    "https://www.cw.com.tw",
                    "https://www.msn.com/",
                    "https://cn.wsj.com/",
                    "https://about.pts.org.tw/pr/latestnews",
                    "https://www.chinatimes.com",
                    "https://newtalk.tw",
                    "https://sports.ltn.com.tw",
                    "https://video.ltn.com.tw",
                    "https://def.ltn.com.tw",
                    "https://www.upmedia.mg",
                    "http://www.aastocks.com",
                    "https://news.futunn.com",
                    "https://ec.ltn.com.tw/",
                    "https://health.ltn.com.tw",
                    "https://www.taiwannews",
                    "https://www.ftvnews.com.tw",
                    "https://tw.nextapple.com",
                    "https://talk.ltn.com.tw",
                    "https://www.mobile01.com/"
                ]
                final_url = driver.current_url
                print(f"   æœ€çµ‚ç¶²å€: {final_url}")
                if(final_url.startswith("https://www.google.com/sorry/index?continue=https://news.google.com/read")):
                    driver.refresh()
                    time.sleep(random.randint(2, 4))
                    final_url = driver.current_url
                elif any(final_url.startswith(pattern) for pattern in skip_patterns):
                    print(f"   â­ï¸  è·³éé€£çµ: {final_url}")
                    return None
                
            except WebDriverException:
                print(f"   âš ï¸ ç„¡æ³•ç²å–ç•¶å‰ URLï¼Œä½¿ç”¨åŸå§‹ URL")
                final_url = article_info['article_url']
                return None
            
            # å–å¾— HTML åŸå§‹ç¢¼ä¸¦äº¤çµ¦ BeautifulSoup
            try:
                html = driver.page_source
                soup = BeautifulSoup(html, "html.parser")
            except WebDriverException:
                print(f"   âŒ ç„¡æ³•ç²å–é é¢æºç¢¼")
                if attempt < MAX_RETRIES - 1:
                    print(f"   ğŸ”„ {TIMEOUT//2} ç§’å¾Œé‡è©¦...")
                    time.sleep(TIMEOUT//2)
                    continue
                else:
                    return None

            # æ¸…ç†å…§å®¹
            content_to_clean = None

            # ç¬¬ä¸€å„ªå…ˆï¼šå°‹æ‰¾ article æ¨™ç±¤
            article_tag = soup.find('article')
            if article_tag and article_info['media'] != 'Now æ–°è':
                content_to_clean = str(article_tag)
            elif soup.find('artical'):
                article_tag = soup.find('artical')
                content_to_clean = str(article_tag)
            else:
                # ç¬¬äºŒå„ªå…ˆï¼šå°‹æ‰¾ç‰¹å®š id çš„ div æ¨™ç±¤
                target_ids = [
                    'text ivu-mt', 'content-box', 'text', 'boxTitle', 
                    'news-detail-content', 'story', 'article-content__editor', 'article-body', 
                    'artical-content', 'article_text', 'newsText'
                ]
                
                div_by_id = None
                for target_id in target_ids:
                    div_by_id = soup.find('div', id=target_id)
                    if div_by_id:
                        break
                
                if div_by_id:
                    content_to_clean = str(div_by_id)
                else:
                    # ç¬¬ä¸‰å„ªå…ˆï¼šå°‹æ‰¾ç‰¹å®š class çš„ div æ¨™ç±¤
                    target_classes = ['text boxTitle','text ivu-mt', 'paragraph', 'atoms', 
                                      'news-box-text border', 'newsLeading', 'text']

                    div_by_class = None
                    for target_class in target_classes:
                        div_by_class = soup.find('div', class_=target_class)
                        if div_by_class:
                            break
                    
                    if div_by_class:
                        content_to_clean = str(div_by_class)
                    else:
                        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ body
                        if soup.body:
                            content_to_clean = str(soup.body)

            # å¦‚æœæœ‰æ‰¾åˆ°å…§å®¹ï¼Œé€²è¡Œæ¸…ç†
            if content_to_clean:
                # é‡æ–°è§£ææ‰¾åˆ°çš„å…§å®¹
                content_soup = BeautifulSoup(content_to_clean, "html.parser")
                
                # æ’é™¤ç‰¹å®šçš„ div æ¨™ç±¤
                excluded_divs = content_soup.find_all('div', class_='paragraph moreArticle')
                for div in excluded_divs:
                    div.decompose()
                
                # æ’é™¤ç‰¹å®šçš„ p æ¨™ç±¤
                excluded_p_classes = [
                    'mb-module-gap read-more-vendor break-words leading-[1.4] text-px20 lg:text-px18 lg:leading-[1.8] text-batcave __web-inspector-hide-shortcut__',
                    'mb-module-gap read-more-editor break-words leading-[1.4] text-px20 lg:text-px18 lg:leading-[1.8] text-batcave'
                ]
                
                for p_class in excluded_p_classes:
                    excluded_ps = content_soup.find_all('p', class_=p_class)
                    for p in excluded_ps:
                        p.decompose()
                
                # æœ€çµ‚æ¸…ç†
                body_content = str(content_soup)
                body_content = body_content.replace("\x00", "").replace("\r", "").replace("\n", "")
                body_content = body_content.replace('"', '\\"')
            else:
                body_content = ""
                print(f"   âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„å…§å®¹")
                
            # ç”Ÿæˆæ–‡ç«  ID
            article_id = str(uuid.uuid4())

            if("æ‚¨çš„ç¶²è·¯å·²é­åˆ°åœæ­¢è¨ªå•æœ¬ç¶²ç«™çš„æ¬Šåˆ©ã€‚" in body_content or "æˆ‘å€‘çš„ç³»çµ±åµæ¸¬åˆ°æ‚¨çš„é›»è…¦ç¶²è·¯é€å‡ºçš„æµé‡æœ‰ç•°å¸¸æƒ…æ³ã€‚" in body_content):
                print(f"   âš ï¸ æ–‡ç«  {article_id} è¢«å°é–ï¼Œç„¡æ³•è¨ªå•")
                return None

            return {
                "story_id": article_info['story_id'],  # ä½¿ç”¨ story_id è€Œé story_index
                "story_title": article_info['story_title'],
                "story_category": article_info['story_category'],  # æ–°å¢ï¼šä¿å­˜åˆ†é¡
                "story_url": article_info['story_url'],
                "id": article_id,
                "article_index": article_info['article_index'],
                "article_title": article_info['article_title'],
                "google_news_url": article_info['article_url'],
                "final_url": final_url,
                "media": article_info.get('media', 'æœªçŸ¥ä¾†æº'),  # æ·»åŠ åª’é«”ä¾†æº
                "content": body_content,
            }
            
        except Exception as e:
            print(f"   âŒ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—: {e}")
            if attempt < MAX_RETRIES - 1:
                print(f"   ğŸ”„ {TIMEOUT//2} ç§’å¾Œé‡è©¦...")
                time.sleep(TIMEOUT//2)
            else:
                print(f"   ğŸ’€ å·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„è©²æ–‡ç« ")
    
    return None

def create_robust_driver():
    """å‰µå»ºä¸€å€‹æ›´ç©©å¥çš„ WebDriver"""
    # Chrome options è¨­å®š
    chrome_options = Options()
    # chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-web-security")
    chrome_options.add_argument("--disable-features=VizDisplayCompositor")
    chrome_options.add_argument("--page-load-strategy=eager")

    # ç”¨æˆ¶ä»£ç†
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36")

    # é˜²æ­¢è¢«è­˜åˆ¥ç‚ºè‡ªå‹•åŒ–
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)

    # å»£å‘Šå’Œè¿½è¹¤é˜»æ“‹
    chrome_options.add_argument("--disable-background-timer-throttling")
    chrome_options.add_argument("--disable-backgrounding-occluded-windows")
    chrome_options.add_argument("--disable-renderer-backgrounding")
    chrome_options.add_argument("--disable-features=TranslateUI")
    chrome_options.add_argument("--disable-ipc-flooding-protection")

    # åœ–ç‰‡å’Œåª’é«”å„ªåŒ–
    chrome_options.add_argument("--disable-background-media")
    chrome_options.add_argument("--disable-background-downloads")
    chrome_options.add_argument("--aggressive-cache-discard")
    chrome_options.add_argument("--disable-sync")

    # ç¶²è·¯å„ªåŒ–
    chrome_options.add_argument("--disable-default-apps")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-plugins")
    chrome_options.add_argument("--disable-notifications")
    chrome_options.add_argument("--disable-popup-blocking")

    # è¨˜æ†¶é«”å’Œæ•ˆèƒ½å„ªåŒ–
    chrome_options.add_argument("--memory-pressure-off")
    chrome_options.add_argument("--max_old_space_size=4096")
    chrome_options.add_argument("--single-process")
    chrome_options.add_argument("--no-zygote")

    # é˜»æ“‹ç‰¹å®šå…§å®¹é¡å‹
    prefs = {
        "profile.default_content_setting_values": {
            "notifications": 2,  # é˜»æ“‹é€šçŸ¥
            "plugins": 2,        # é˜»æ“‹æ’ä»¶
            "popups": 2,         # é˜»æ“‹å½ˆå‡ºçª—å£
            "geolocation": 2,    # é˜»æ“‹ä½ç½®è«‹æ±‚
            "media_stream": 2,   # é˜»æ“‹æ”åƒé ­/éº¥å…‹é¢¨
        },
        "profile.managed_default_content_settings": {
            "images": 2,         # 1=å…è¨±, 2=é˜»æ“‹åœ–ç‰‡
        },
        "profile.default_content_settings": {
            "popups": 2
        }
    }
    chrome_options.add_experimental_option("prefs", prefs)
    
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return driver
    except Exception as e:
        print(f"âŒ å‰µå»º WebDriver å¤±æ•—: {e}")
        raise

# def save_to_supabase(grouped_articles):
#     """å°‡æŠ“å–çš„æ–°èæ•¸æ“šä¿å­˜åˆ°Supabase"""
#     if not supabase:
#         print("âš ï¸ Supabaseæœªè¨­å®šï¼Œè·³éæ•¸æ“šä¸Šå‚³")
#         return
    
#     try:
#         print("ğŸ’¾ æ­£åœ¨å°‡æ•¸æ“šä¸Šå‚³åˆ°Supabase...")
        
#         # å‡è¨­æ‚¨æœ‰å…©å€‹è¡¨ï¼š'news_stories' å’Œ 'news_articles'
#         for story in grouped_articles:
#             # ä¸Šå‚³æ•…äº‹è¨˜éŒ„
#             story_data = {
#                 'story_id': story['story_id'],
#                 'story_title': story['story_title'],
#                 'story_url': story['story_url'],
#                 'category': story['category'],
#                 'crawl_date': story['crawl_date']
#             }
            
#             # æª¢æŸ¥æ•…äº‹æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨å‰‡æ›´æ–°ï¼Œå¦å‰‡æ’å…¥
#             existing_story = supabase.table('stories').select('*').eq('story_id', story['story_id']).execute()
            
#             if existing_story.data:
#                 # æ›´æ–°ç¾æœ‰è¨˜éŒ„
#                 supabase.table('stories').update(story_data).eq('story_id', story['story_id']).execute()
#                 print(f"âœ… æ›´æ–°æ•…äº‹: {story['story_title']}")
#             else:
#                 # æ’å…¥æ–°è¨˜éŒ„
#                 supabase.table('stories').insert(story_data).execute()
#                 print(f"âœ… æ–°å¢æ•…äº‹: {story['story_title']}")
            
#             # ä¸Šå‚³æ–‡ç« è¨˜éŒ„
#             for article in story['articles']:
#                 article_data = {
#                     'article_id': article['article_id'],
#                     'story_id': story['story_id'],
#                     'article_title': article['article_title'],
#                     'article_url': article['article_url'],
#                     'google_news_url': article['google_news_url'],
#                     'media': article['media'],
#                     'content': article['content'],
#                     'crawl_date': story['crawl_date']
#                 }
                
#                 # æª¢æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨
#                 existing_article = supabase.table('news_articles').select('*').eq('article_id', article['article_id']).execute()
                
#                 if existing_article.data:
#                     # æ›´æ–°ç¾æœ‰è¨˜éŒ„
#                     supabase.table('news_articles').update(article_data).eq('article_id', article['article_id']).execute()
#                 else:
#                     # æ’å…¥æ–°è¨˜éŒ„
#                     supabase.table('news_articles').insert(article_data).execute()
        
#         print("âœ… æ‰€æœ‰æ•¸æ“šå·²æˆåŠŸä¸Šå‚³åˆ°Supabase")
        
#     except Exception as e:
#         print(f"âŒ ä¸Šå‚³åˆ°Supabaseæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def main():
    """ä¸»åŸ·è¡Œå‡½å¼"""
    # å®šç¾©è¦çˆ¬å–çš„æ–°èé ˜åŸŸ
    topic_sources = [
        {
            "url": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFZ4ZERBU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
            "category": "Politics"
        },
        {
            "url": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFptTXpJU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
            "category": "Taiwan News"
        },
        # {
        #     "url": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "International News"
        # },
        # {
        #     "url": "https://news.google.com/topics/CAAqLAgKIiZDQkFTRmdvSkwyMHZNR1ptZHpWbUVnVjZhQzFVVnhvQ1ZGY29BQVAB?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "Science & Technology"
        # },
        # {
        #     "url": "https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSkwyMHZNREUwWkhONEVnVjZhQzFVVnlnQVAB?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "Lifestyle & Consumer"
        # },
        # {
        #     "url": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "Sports"
        # },
        # {
        #     "url": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "Entertainment"
        # },
        # {
        #     "url": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "Business & Finance"
        # },
        # {
        #     "url": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "Health & Wellness"
        # }
    ]
    
    # å»ºç«‹å¿…è¦çš„è³‡æ–™å¤¾
    os.makedirs("json", exist_ok=True)
    
    all_article_links = []
    skip_stats = defaultdict(int)  # çµ±è¨ˆè·³éçš„æ•…äº‹æ•¸é‡
    
    # === æ­¥é©Ÿ 1 & 2: éæ­·æ‰€æœ‰ä¸»é¡Œé ˜åŸŸ ===
    print("=== é–‹å§‹çˆ¬å–å¤šå€‹æ–°èé ˜åŸŸ ===")
    
    for topic in topic_sources:
        main_url = topic["url"]
        category = topic["category"]
        
        print(f"\n=== è™•ç†ä¸»é¡Œï¼š{category} ===")
        
        # æ­¥é©Ÿ 1: æ‰¾åˆ°è©²é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµï¼ˆå·²åŒ…å«Supabaseé‡è¤‡æª¢æŸ¥ï¼‰
        story_links = get_main_story_links(main_url, category)
        
        if not story_links:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ° {category} é ˜åŸŸéœ€è¦è™•ç†çš„ä¸»è¦æ•…äº‹é€£çµ")
            continue
        
        # æ­¥é©Ÿ 2: å¾æ¯å€‹æ•…äº‹æŠ“å–æ–‡ç« é€£çµ
        print(f"\n=== å¾ {category} é ˜åŸŸçš„æ¯å€‹æ•…äº‹æŠ“å–æ–‡ç« é€£çµ ===")
        
        for story in story_links[:8]:  # å¯è¦–éœ€æ±‚ä¿®æ”¹æ•¸é‡
            article_links = get_article_links_from_story(story)
            all_article_links.extend(article_links)
            time.sleep(2)
    
    if not all_article_links:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•éœ€è¦è™•ç†çš„æ–‡ç« é€£çµ")
        return
    
    # === æ­¥é©Ÿ 3: æŠ“å–æ–‡ç« å…§å®¹å’Œåœ–ç‰‡ ===
    print("\n=== æ­¥é©Ÿ 3: æŠ“å–æ–‡ç« å…§å®¹å’Œåœ–ç‰‡ ===")
    final_articles = []

    # å‰µå»ºæ›´ç©©å¥çš„ driver
    try:
        driver = create_robust_driver()
        
        # å…ˆè¨ªå• Google News ä¸»é 
        driver.get("https://news.google.com/")
        time.sleep(2)

        # åŒ¯å…¥ cookiesï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            with open("cookies.json", "r", encoding="utf-8") as f:
                cookies = json.load(f)
            for cookie in cookies:
                if 'sameSite' in cookie:
                    cookie.pop('sameSite')
                try:
                    driver.add_cookie(cookie)
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•æ·»åŠ  cookie: {e}")
        except FileNotFoundError:
            print("âš ï¸  cookies.json æª”æ¡ˆä¸å­˜åœ¨ï¼Œç¹¼çºŒåŸ·è¡Œ...")

        # è™•ç†æ–‡ç« 
        successful_count = 0
        failed_count = 0
        
        for i, article in enumerate(all_article_links, start=1):  
            print(f"\nè™•ç†æ–‡ç«  {i}/{len(all_article_links)} (æˆåŠŸ: {successful_count}, å¤±æ•—: {failed_count})")
            print(f"   [{article.get('story_category', 'æœªçŸ¥')}] {article['article_title']}")
            
            content = get_final_content(article, driver)
            
            if content and content.get('content'):
                final_articles.append(content)
                successful_count += 1
                print(f"âœ… æˆåŠŸæŠ“å–: [{content['story_category']}] {content['article_title']}")
            else:
                failed_count += 1
                print(f"âŒ ç„¡æ³•å–å¾—å…§å®¹")
                
            # é©ç•¶çš„å»¶é²ï¼Œé¿å…è¢«å°
            time.sleep(random.randint(2, 4))

    except Exception as e:
        print(f"âŒ WebDriver åˆå§‹åŒ–æˆ–é‹è¡Œå¤±æ•—: {e}")
        return
    finally:
        try:
            driver.quit()
            print("ğŸ”§ WebDriver å·²å®‰å…¨é—œé–‰")
        except:
            pass

    if not final_articles:
        print("âŒ æ²’æœ‰æˆåŠŸæŠ“å–åˆ°ä»»ä½•æ–‡ç« å…§å®¹")
        return

    print(f"\nğŸ“Š æŠ“å–çµ±è¨ˆ: æˆåŠŸ {successful_count} ç¯‡, å¤±æ•— {failed_count} ç¯‡")

    # === ä¾åˆ†é¡å’Œ story_id åˆ†çµ„ ===
    category_grouped = defaultdict(lambda: defaultdict(list))

    for item in final_articles:
        category = item["story_category"]
        story_id = item["story_id"]
        
        category_grouped[category][story_id].append({
            "article_id": item["id"],
            "article_title": item["article_title"],
            "article_index": item["article_index"],
            "google_news_url": item["google_news_url"],
            "article_url": item["final_url"],
            "media": item["media"],
            "content": item["content"],
        })

    # === ç‚ºæ¯å€‹åˆ†é¡å»ºç«‹åˆ†çµ„æ–‡ç« ä¸¦å„²å­˜ JSON ===
    all_grouped_articles = []
    category_stats = defaultdict(int)
    
    for category, stories in category_grouped.items():
        category_articles = []
        
        for story_id, articles in stories.items():
            # å¾ final_articles ä¸­æ‰¾åˆ°å°æ‡‰çš„æ•…äº‹è³‡è¨Š
            sample_article = next((x for x in final_articles if x["story_id"] == story_id), None)
            if sample_article:
                story_url = sample_article["story_url"]
            else:
                story_url = ""

            # å°‡æ–‡ç« ä¾ç…§ article_index æ’åº
            articles_sorted = sorted(articles, key=lambda x: x["article_index"])
            
            # ä½¿ç”¨ç¬¬ä¸€ç¯‡æ–‡ç« çš„æ¨™é¡Œä½œç‚º story_title
            story_title = articles_sorted[0]["article_title"] if articles_sorted else ""
                
            category_articles.append({
                "story_id": story_id,
                "story_title": story_title,
                "story_url": story_url,
                "crawl_date": dt.datetime.now().strftime("%Y/%m/%d %H:%M"),
                "category": category,
                "articles": articles_sorted
            })
        
        # ä¹ŸåŠ å…¥ç¸½é«”åˆ—è¡¨
        all_grouped_articles.extend(category_articles)

    # === å„²å­˜ç¸½é«” JSONï¼ˆåŸæœ¬çš„åŠŸèƒ½ä¿ç•™ï¼‰ ===
    with open("json/final_news.json", "w", encoding="utf-8") as f:
        json.dump(all_grouped_articles, f, ensure_ascii=False, indent=2)

    # === ä¸Šå‚³åˆ° Supabase ===
    # if supabase:
    #     save_to_supabase(all_grouped_articles)

    # === çµ±è¨ˆè³‡è¨Š ===
    print(f"\nğŸ‰ å®Œæˆï¼ç¸½å…±æˆåŠŸæŠ“å– {len(final_articles)} ç¯‡æ–‡ç« ")
    print("ğŸ“Š å„é ˜åŸŸæ–‡ç« æ•¸é‡çµ±è¨ˆ:")
    for category, count in category_stats.items():
        print(f"   {category}: {count} ç¯‡")
    print("ğŸ“ å„²å­˜æª”æ¡ˆ:")
    print("   - json/final_news.jsonï¼ˆæ‰€æœ‰åˆ†é¡åˆä½µï¼‰")
    for category in category_stats.keys():
        print(f"   - json/final_news_{category}.jsonï¼ˆ{category} åˆ†é¡ç¨ç«‹æª”æ¡ˆï¼‰")

if __name__ == "__main__":
    main()