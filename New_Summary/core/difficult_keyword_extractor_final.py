"""
å›°é›£é—œéµå­—æå–å™¨ - å¯å­˜å…¥è³‡æ–™åº«ç‰ˆæœ¬
å¾ Supabase single_news è¡¨è®€å–è³‡æ–™ï¼Œæå–å›°é›£é—œéµå­—ä¸¦ç”Ÿæˆè§£é‡‹ï¼Œå¯å­˜å…¥è³‡æ–™åº«

ç”¨æ³•:
  python difficult_keyword_extractor_final.py [limit]

è«‹åœ¨ word_analysis_system/.env è¨­å®š GEMINI_API_KEYã€SUPABASE_URL èˆ‡ SUPABASE_KEY
"""

import os
import json
import time
import sys
import uuid
import logging
from typing import List, Dict, Any, Set, Optional
from dotenv import load_dotenv
from tqdm import tqdm
from google import genai
from google.genai import types

logger = logging.getLogger(__name__)


class DiffKeywordConfig:
    """å›°é›£é—œéµå­—è™•ç†å™¨è¨­å®š"""
    
    # API è¨­å®š
    API_CONFIG = {
        'model_name': 'gemini-2.5-flash-lite',
        'call_delay_seconds': 1,  # API å‘¼å«é–“éš”
        'max_retries': 3,
    }
    
    # è™•ç†è¨­å®š
    PROCESSING_CONFIG = {
        'explanation_word_limit': 50,  # è§£é‡‹å­—æ•¸é™åˆ¶
        'default_limit': None,  # é è¨­è®€å–ç­†æ•¸é™åˆ¶
    }
    
    # è³‡æ–™åº«è¨­å®š
    DB_CONFIG = {
        'table_name': 'single_news',
        'select_fields': ['story_id', 'news_title', 'ultra_short', 'short', 'long'],
        'primary_content_field': 'long',  # ä¸»è¦ç”¨æ–¼æå–é—œéµå­—çš„æ¬„ä½
        'title_field': 'news_title',
        'term_map_table': 'term_map',
        'term_map_fields': ['story_id', 'term_id'],
        'term_table': 'term',
        'term_fields': ['term_id', 'term', 'definition', 'example'],
    }
    
    # è¼¸å‡ºè¨­å®š
    OUTPUT_CONFIG = {
        'save_to_file': False,
        'output_filename': 'difficult_keywords_output.json',
        'terminal_width': 80,
    }


class DiffKeywordProcessor:
    """å›°é›£é—œéµå­—æå–èˆ‡è§£é‡‹çš„æ ¸å¿ƒé¡åˆ¥"""

    def __init__(self):
        """åˆå§‹åŒ–å›°é›£é—œéµå­—è™•ç†å™¨"""
        self.client = None
        self.supabase_client = None
        self.api_config = DiffKeywordConfig.API_CONFIG
        self.proc_config = DiffKeywordConfig.PROCESSING_CONFIG
        self.db_config = DiffKeywordConfig.DB_CONFIG
        self._setup_model()
        self._setup_supabase()

    def _setup_model(self):
        """è¼‰å…¥ç’°å¢ƒè®Šæ•¸ä¸¦åˆå§‹åŒ– Gemini Client"""
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise EnvironmentError("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GEMINI_API_KEYï¼Œè«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š")

        try:
            # ä½¿ç”¨æ–°ç‰ˆ google-genai client
            self.client = genai.Client(api_key=api_key)
            logger.info(f"âœ“ Gemini Client åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹ {self.api_config['model_name']}")
        except Exception as e:
            logger.error(f"âœ— åˆå§‹åŒ– Gemini Client æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

    def is_ready(self) -> bool:
        """æª¢æŸ¥æ¨¡å‹å’Œè³‡æ–™åº«é€£ç·šæ˜¯å¦å·²æˆåŠŸåˆå§‹åŒ–"""
        return self.client is not None and self.supabase_client is not None

    def _call_gemini(self, prompt: str) -> Dict[str, Any]:
        """å‘¼å« Gemini API ä¸¦è™•ç†å›è¦†ï¼ˆæ–°ç‰ˆ SDKï¼‰"""
        for attempt in range(self.api_config['max_retries']):
            try:
                response = self.client.models.generate_content(
                    model=self.api_config['model_name'],
                    contents=[{"role": "user", "parts": [{"text": prompt}]}],
                    config=types.GenerateContentConfig()
                )
                cleaned_text = self._clean_response_text(response.text)
                return json.loads(cleaned_text)
            except json.JSONDecodeError as e:
                logger.warning(f"âœ— JSON è§£æéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{self.api_config['max_retries']}): {e}")
                if attempt == self.api_config['max_retries'] - 1:
                    logger.error(f"åŸå§‹å›è¦†: {response.text}")
                    return {}
            except Exception as e:
                logger.warning(f"âœ— API å‘¼å«æ™‚ç™¼ç”ŸéŒ¯èª¤ (å˜—è©¦ {attempt + 1}/{self.api_config['max_retries']}): {e}")
                if attempt == self.api_config['max_retries'] - 1:
                    return {}
                time.sleep(2)
        return {}

    def _setup_supabase(self):
        """è¼‰å…¥ç’°å¢ƒè®Šæ•¸ä¸¦åˆå§‹åŒ– Supabase é€£ç·š"""
        load_dotenv()
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise EnvironmentError("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° SUPABASE_URL æˆ– SUPABASE_KEYï¼Œè«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š")
        
        try:
            from supabase import create_client
            self.supabase_client = create_client(supabase_url, supabase_key)
            logger.info(f"âœ“ Supabase é€£ç·š ({supabase_url}) åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âœ— åˆå§‹åŒ– Supabase æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            logger.error("è«‹ç¢ºèªå·²å®‰è£ supabase-pyï¼špip install supabase-py postgrest-py")
            raise

    def _clean_response_text(self, text: str) -> str:
        """æ¸…ç† Gemini å›è¦†ä¸­çš„ markdown JSON æ¨™ç±¤"""
        cleaned_text = text.strip()
        # æª¢æŸ¥ä¸¦ç§»é™¤é–‹é ­çš„ markdown æ¨™ç±¤
        if cleaned_text.startswith("```json"):
            cleaned_text = cleaned_text[7:]
        elif cleaned_text.startswith("```"):
            cleaned_text = cleaned_text[3:]
        
        # æª¢æŸ¥ä¸¦ç§»é™¤çµå°¾çš„ markdown æ¨™ç±¤
        if cleaned_text.endswith("```json"):
            cleaned_text = cleaned_text[:-7]
        elif cleaned_text.endswith("```"):
            cleaned_text = cleaned_text[:-3]
            
        return cleaned_text.strip()

    def fetch_combined_data(self, limit: Optional[int] = None, story_ids: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """è®€å–ä¸¦åˆä½µ single_news å’Œ term_map è³‡æ–™"""
        logger.info("=== è®€å–åˆä½µè³‡æ–™ ===")
        
        # è®€å– single_news è³‡æ–™
        logger.info("è®€å– single_news è³‡æ–™...")
        try:
            table_name = self.db_config['table_name']
            fields = ','.join(self.db_config['select_fields'])
            
            query = self.supabase_client.table(table_name).select(fields)
            if story_ids:
                query = query.in_('story_id', story_ids)
            
            if limit:
                query = query.limit(limit)
                logger.info(f"é™åˆ¶è®€å–å‰ {limit} ç­†")
            else:
                logger.info("è®€å–æ‰€æœ‰è³‡æ–™")
            
            resp = query.execute()
            
            if getattr(resp, 'error', None):
                logger.error(f"è®€å– {table_name} å¤±æ•—: {resp.error}")
                return []
            
            news_data = resp.data or []
            logger.info(f"æˆåŠŸè®€å– {len(news_data)} ç­†æ–°èè³‡æ–™")
            
        except Exception as e:
            logger.error(f"è®€å–æ–°èè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
        
        # è®€å– term_map è³‡æ–™
        logger.info("è®€å– term_map è³‡æ–™...")
        try:
            # ç›´æ¥æŸ¥è©¢ term_mapï¼Œç„¶å¾Œå†æŸ¥è©¢ term
            term_map_resp = self.supabase_client.table(self.db_config['term_map_table']).select('story_id,term_id').execute()
            
            if getattr(term_map_resp, 'error', None):
                print(f"è®€å– {self.db_config['term_map_table']} å¤±æ•—: {term_map_resp.error}")
                term_map = {}
            else:
                term_map_data = term_map_resp.data or []
                print(f"æˆåŠŸè®€å– {len(term_map_data)} ç­† term_map è³‡æ–™")
                
                # ç²å–æ‰€æœ‰ term_id å°æ‡‰çš„ term åç¨±
                term_ids = list(set([row.get('term_id') for row in term_map_data if row.get('term_id')]))
                if term_ids:
                    terms_resp = self.supabase_client.table(self.db_config['term_table']).select('term_id,term').in_('term_id', term_ids).execute()
                    
                    if getattr(terms_resp, 'error', None):
                        print(f"è®€å– term è¡¨å¤±æ•—: {terms_resp.error}")
                        term_id_to_term = {}
                    else:
                        term_data = terms_resp.data or []
                        term_id_to_term = {row['term_id']: row['term'] for row in term_data if row.get('term_id') and row.get('term')}
                else:
                    term_id_to_term = {}
                
                # çµ„ç¹”æˆ story_id -> terms çš„å­—å…¸
                term_map = {}
                for row in term_map_data:
                    story_id = row.get('story_id')
                    term_id = row.get('term_id')
                    term_name = term_id_to_term.get(term_id)

                    if story_id and term_name:
                        if story_id not in term_map:
                            term_map[story_id] = []
                        term_map[story_id].append(term_name)

                logger.info(f"çµ„ç¹” term_map: {len(term_map)} å€‹ä¸åŒçš„ story_id")
            
        except Exception as e:
            logger.error(f"è®€å– term_map è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            term_map = {}
        
        # åˆä½µè³‡æ–™
        combined_data = []
        for news in news_data:
            story_id = news.get('story_id')
            existing_terms = term_map.get(story_id, [])
            
            # å°‡ existing_terms æ·»åŠ åˆ°æ–°èè³‡æ–™ä¸­
            news_with_terms = news.copy()
            news_with_terms['existing_terms'] = existing_terms
            combined_data.append(news_with_terms)
        
        logger.info(f"åˆä½µå®Œæˆ: {len(combined_data)} ç­†æ–°èè³‡æ–™")
        return combined_data

    def extract_keywords_from_text(self, text: str, title: str) -> List[str]:
        """å¾å–®ç¯‡æ–‡æœ¬ä¸­æå–å›°é›£é—œéµå­—"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„çŸ¥è­˜ç·¨è¼¯ï¼Œæ“…é•·ç‚ºé«˜ä¸­ç”Ÿè®€è€…è§£é‡‹è¤‡é›œæ¦‚å¿µã€‚
        è«‹å¾ä»¥ä¸‹æ–°èå…§å®¹ä¸­ï¼Œ**ç¯©é¸**å‡ºå°é«˜ä¸­ç”Ÿè€Œè¨€å¯èƒ½éœ€è¦é¡å¤–æŸ¥è©¢ã€æŸ¥å­—å…¸çš„è©å½™ã€‚

        **ç¯©é¸æ¨™æº–ï¼šé«˜ä¸­ç”Ÿéœ€è¦é¡å¤–æŸ¥è©¢çš„è©å½™**
        **é¸å‡ºé«˜ä¸­ç”Ÿåœ¨èª²å ‚ä¸Šè¼ƒå°‘æ¥è§¸ï¼Œéœ€è¦é¡å¤–æŸ¥è©¢æ‰èƒ½å®Œå…¨ç†è§£çš„è©å½™**
        
        å¿…é ˆç¬¦åˆä»¥ä¸‹æ¢ä»¶ä¹‹ä¸€çš„è©å½™ï¼š
        - å°ˆæ¥­è¡“èªï¼šé†«å­¸å°ˆæ¥­è¡“èªï¼ˆå¦‚ã€Œå…ç–«ç³»çµ±ã€ã€ã€ŒåŸºå› çªè®Šã€ï¼‰ã€æ³•å¾‹å°ˆæ¥­è¡“èªï¼ˆå¦‚ã€Œè‘—ä½œæ¬Šæ³•ã€ã€ã€Œæ™ºæ…§è²¡ç”¢æ¬Šã€ï¼‰ã€å·¥ç¨‹æŠ€è¡“è¡“èªï¼ˆå¦‚ã€ŒåŠå°é«”ã€ã€ã€Œäººå·¥æ™ºæ…§ã€ï¼‰
        - å­¸è¡“æ¦‚å¿µï¼šéœ€è¦é€²ä¸€æ­¥å­¸ç¿’æ‰èƒ½ç†è§£çš„æ¦‚å¿µï¼ˆå¦‚ã€Œé€šè²¨è†¨è„¹ã€ã€ã€Œæº«å®¤æ•ˆæ‡‰ã€ã€ã€Œé‡å­ç‰©ç†ã€ï¼‰
        - åœ‹éš›çµ„ç¹”æˆ–å°ˆæ¥­æ©Ÿæ§‹ï¼šè¼ƒæ–°æˆ–è¤‡é›œçš„çµ„ç¹”ï¼ˆå¦‚ã€Œä¸–ç•Œè²¿æ˜“çµ„ç¹”ã€ã€ã€Œåœ‹éš›è²¨å¹£åŸºé‡‘ã€ã€ã€ŒWHOã€ã€ã€ŒUNESCOã€ï¼‰
        - æ–°èˆˆç§‘æŠ€è¡“èªï¼šè¿‘å¹´å‡ºç¾çš„ç§‘æŠ€è©å½™ï¼ˆå¦‚ã€Œå€å¡Šéˆã€ã€ã€Œè™›æ“¬å¯¦å¢ƒã€ã€ã€Œæ©Ÿå™¨å­¸ç¿’ã€ã€ã€Œç‰©è¯ç¶²ã€ï¼‰
        - é‡‘èç¶“æ¿Ÿè¡“èªï¼šé«˜ä¸­ç¶“æ¿Ÿèª²ç¨‹è¼ƒå°‘æ¶‰åŠçš„æ¦‚å¿µï¼ˆå¦‚ã€Œé‡åŒ–å¯¬é¬†ã€ã€ã€ŒåŒ¯ç‡æ“ç¸±ã€ã€ã€ŒæœŸè²¨å¸‚å ´ã€ï¼‰
        - åœ‹éš›æ”¿æ²»è¡“èªï¼šéœ€è¦èƒŒæ™¯çŸ¥è­˜çš„æ”¿æ²»æ¦‚å¿µï¼ˆå¦‚ã€Œå¤šé‚Šä¸»ç¾©ã€ã€ã€Œåœ°ç·£æ”¿æ²»ã€ã€ã€Œåˆ¶è£æªæ–½ã€ï¼‰
        - è¤‡é›œçš„ç¤¾æœƒç§‘å­¸æ¦‚å¿µï¼šå¦‚ã€Œç¤¾æœƒæµå‹•ã€ã€ã€Œæ–‡åŒ–å¤šå…ƒä¸»ç¾©ã€ã€ã€Œå¯æŒçºŒç™¼å±•ã€
        
        **ä¸è¦æå–çš„è©å½™ï¼š**
        - é«˜ä¸­ç”Ÿæ—¥å¸¸æœƒä½¿ç”¨çš„è©å½™ï¼ˆå¦‚ã€Œå­¸ç¿’ã€ã€ã€Œå·¥ä½œã€ã€ã€Œç”Ÿæ´»ã€ï¼‰
        - é«˜ä¸­èª²ç¨‹ä¸­å¸¸è¦‹çš„åŸºç¤æ¦‚å¿µï¼ˆå¦‚ã€Œæ°‘ä¸»ã€ã€ã€Œè‡ªç”±ã€ã€ã€Œç’°ä¿ã€ã€ã€Œç§‘æŠ€ã€ï¼‰
        - ç°¡å–®çš„åœ°åã€äººåã€å…¬å¸åï¼ˆé™¤éæ˜¯éœ€è¦ç‰¹æ®ŠèƒŒæ™¯çŸ¥è­˜çš„ï¼‰
        - æ—¥å¸¸æ–°èä¸­å¸¸è¦‹çš„è©å½™ï¼ˆå¦‚ã€Œæ”¿åºœã€ã€ã€Œç¶“æ¿Ÿã€ã€ã€Œç¤¾æœƒã€ã€ã€Œç™¼å±•ã€ï¼‰
        - åŸºç¤æ•¸å­¸ã€ç§‘å­¸æ¦‚å¿µï¼ˆå¦‚ã€Œç™¾åˆ†æ¯”ã€ã€ã€Œæº«åº¦ã€ã€ã€Œé‡åŠ›ã€ï¼‰
        - ä¸€èˆ¬å•†æ¥­è©å½™ï¼ˆå¦‚ã€Œå…¬å¸ã€ã€ã€Œå¸‚å ´ã€ã€ã€Œæ¶ˆè²»è€…ã€ï¼‰

        **åˆ¤æ–·åŸå‰‡ï¼š**
        - å¦‚æœä¸€å€‹é«˜ä¸­ç”Ÿåœ¨æ—¥å¸¸å°è©±æˆ–åŸºç¤èª²ç¨‹ä¸­èƒ½ç†è§£ï¼Œå°±ä¸è¦é¸
        - å¦‚æœéœ€è¦é¡å¤–æŸ¥è©¢è³‡æ–™æˆ–å­—å…¸æ‰èƒ½å®Œå…¨ç†è§£å…¶å«ç¾©å’Œé‡è¦æ€§ï¼Œå°±è¦é¸
        - å¦‚æœæ˜¯å°ˆæ¥­é ˜åŸŸçš„å…¥é–€æ¦‚å¿µï¼Œä¸”é«˜ä¸­èª²ç¨‹ä¸å¸¸æ¶‰åŠï¼Œå°±è¦é¸

        æ¨™é¡Œï¼š{title}
        å…§å®¹ï¼š{text}

        è«‹ç¯©é¸å‡ºé«˜ä¸­ç”Ÿéœ€è¦é¡å¤–æŸ¥è©¢çš„å°ˆæ¥­è¡“èªæˆ–è¤‡é›œæ¦‚å¿µã€‚
        è«‹åš´æ ¼ä»¥ JSON æ ¼å¼å›å‚³ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {{"keywords": ["é—œéµå­—1", "é—œéµå­—2", "..."]}}
        """
        result = self._call_gemini(prompt)
        time.sleep(self.api_config['call_delay_seconds'])
        return result.get('keywords', [])

    def get_word_explanation(self, word: str) -> Dict[str, Any]:
        """ç‚ºå–®ä¸€è©å½™ç”¢ç”Ÿè§£é‡‹å’Œå¯¦éš›æ‡‰ç”¨å¯¦ä¾‹"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä½çŸ¥è­˜æ·µåšçš„è©å…¸ç·¨çº‚å°ˆå®¶ï¼Œæ“…é•·ç”¨å…·é«”å¯¦ä¾‹èªªæ˜æ¦‚å¿µã€‚
        é‡å°ä»¥ä¸‹è©å½™ï¼Œè«‹æä¾›ç´„ {self.proc_config['explanation_word_limit']} å­—çš„ã€Œåè©è§£é‡‹ã€å’Œã€Œæ‡‰ç”¨å¯¦ä¾‹ã€ã€‚

        è¦è§£é‡‹çš„è©å½™æ˜¯ï¼šã€Œ{word}ã€

        ã€Œæ‡‰ç”¨å¯¦ä¾‹ã€éƒ¨åˆ†ï¼Œè«‹ä¸è¦ç”¨å®Œæ•´çš„å¥å­é€ å¥ã€‚è«‹ç›´æ¥åˆ—å‡ºè©²è©å½™æœƒè¢«ä½¿ç”¨åˆ°çš„å…·é«”å ´æ™¯ã€æŠ€è¡“æˆ–ç”¢å“ã€‚
        æ ¼å¼è«‹åƒé€™æ¨£ï¼Œåˆ—èˆ‰å¹¾å€‹å¯¦éš›ä¾‹å­ï¼š
        - **ç¯„ä¾‹è¼¸å…¥ï¼š** äººå·¥æ™ºæ…§
        - **æœŸæœ›çš„æ‡‰ç”¨å¯¦ä¾‹è¼¸å‡ºï¼š** èªéŸ³åŠ©æ‰‹ï¼ˆå¦‚ Siriã€Alexaï¼‰ã€æ¨è–¦ç³»çµ±ã€è‡ªå‹•é§•é§›æ±½è»Šã€é†«ç™‚å½±åƒåˆ†æã€‚

        è«‹åš´æ ¼ä¾ç…§ä»¥ä¸‹ JSON æ ¼å¼å›å‚³ï¼Œä¸è¦æœ‰ä»»ä½• markdown æ¨™ç±¤æˆ–èªªæ˜æ–‡å­—ï¼š
        {{
            "term": "{word}",
            "definition": "ï¼ˆåœ¨æ­¤å¡«å¯«ç°¡æ½”çš„åè©è§£é‡‹ï¼‰",
            "examples": [
                {{
                    "title": "æ‡‰ç”¨å¯¦ä¾‹",
                    "text": "ï¼ˆåœ¨æ­¤æ¢åˆ—å¼å¡«å¯«å…·é«”çš„æ‡‰ç”¨å ´æ™¯æˆ–ç”¢å“ï¼Œè€Œéé€ å¥ï¼‰"
                }}
            ]
        }}
        """
        result = self._call_gemini(prompt)
        time.sleep(self.api_config['call_delay_seconds'])
        return result

    def get_word_explanation_from_context(self, word: str, context: str, title: str = "") -> Dict[str, Any]:
        """æ ¹æ“šæ–‡ç« å…§å®¹ç‚ºè©å½™ç”¢ç”Ÿè§£é‡‹ï¼ˆæŒ‰ç…§æŒ‡å®šæ ¼å¼ï¼‰"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ•™è‚²ç·¨è¼¯ï¼Œæ“…é•·ç‚ºé«˜ä¸­ç”Ÿè§£é‡‹è¤‡é›œæ¦‚å¿µã€‚
        
        è«‹æ ¹æ“šä»¥ä¸‹æ–°èå…§å®¹ï¼Œç‚ºæŒ‡å®šçš„è©å½™ã€Œ{word}ã€æä¾›é©åˆé«˜ä¸­ç”Ÿç†è§£çš„è©³ç´°è§£é‡‹ã€‚
        
        æ–°èæ¨™é¡Œï¼š{title}
        æ–°èå…§å®¹ï¼š{context}
        
        è¦è§£é‡‹çš„è©å½™ï¼šã€Œ{word}ã€
        
        è«‹æä¾›ï¼š
        1. è©³ç´°è§£é‡‹ï¼ˆ100-150å­—ï¼‰ï¼šç”¨é«˜ä¸­ç”Ÿèƒ½ç†è§£çš„èªè¨€èªªæ˜è©²è©å½™çš„å«ç¾©ã€èƒŒæ™¯ã€é‡è¦æ€§ç­‰
        2. ç›¸é—œæ‡‰ç”¨ä¾‹å­ï¼ˆ3-5å€‹ï¼‰ï¼šåˆ—å‡ºè©²è©å½™çš„å…·é«”æ‡‰ç”¨å ´æ™¯ã€ç›¸é—œç”¢å“æˆ–å¯¦éš›ä¾‹å­ï¼Œè®“é«˜ä¸­ç”Ÿå®¹æ˜“ç†è§£å’Œè¨˜æ†¶
        
        **èªè¨€è¦æ±‚ï¼š**
        - é¿å…éæ–¼è‰±æ·±çš„å°ˆæ¥­è¡“èª
        - ç”¨å…·é«”çš„ä¾‹å­å’Œé¡æ¯”ä¾†èªªæ˜æŠ½è±¡æ¦‚å¿µ
        - è§£é‡‹è¦æ¸…æ™°æ˜“æ‡‚ï¼Œé©åˆé«˜ä¸­ç”Ÿé–±è®€ç†è§£
        
        è«‹åš´æ ¼ä¾ç…§ä»¥ä¸‹ JSON æ ¼å¼å›å‚³ï¼Œä¸è¦æœ‰ä»»ä½• markdown æ¨™ç±¤æˆ–èªªæ˜æ–‡å­—ï¼š
        {{
            "term": "{word}",
            "explanation": "é©åˆé«˜ä¸­ç”Ÿç†è§£çš„è©³ç´°è§£é‡‹ï¼ˆ100-150å­—ï¼‰",
            "examples": [
                "å…·é«”æ‡‰ç”¨ä¾‹å­1",
                "å…·é«”æ‡‰ç”¨ä¾‹å­2", 
                "å…·é«”æ‡‰ç”¨ä¾‹å­3",
                "å…·é«”æ‡‰ç”¨ä¾‹å­4",
                "å…·é«”æ‡‰ç”¨ä¾‹å­5"
            ]
        }}
        
        æ‡‰ç”¨ä¾‹å­æ ¼å¼è¦æ±‚ï¼š
        - æ¯å€‹ä¾‹å­è¦å…·é«”æ˜ç¢ºï¼Œä½¿ç”¨é«˜ä¸­ç”Ÿç†Ÿæ‚‰çš„äº‹ç‰©
        - å¯ä»¥åŒ…å«çŸ¥åå“ç‰Œã€æ—¥å¸¸æ‡‰ç”¨ã€å­¸æ ¡å¯èƒ½æ¥è§¸åˆ°çš„æƒ…å¢ƒ
        - ä¾‹å­è¦èˆ‡è©²è©å½™ç›´æ¥ç›¸é—œä¸”å®¹æ˜“ç†è§£
        - æä¾›3-5å€‹ä¾‹å­ï¼ˆå¯ä»¥å°‘æ–¼5å€‹ï¼Œä½†è‡³å°‘3å€‹ï¼‰
        
        ç¯„ä¾‹æ ¼å¼ï¼š
        - term: "äººå·¥æ™ºæ…§"
        - explanation: "äººå·¥æ™ºæ…§æ˜¯è®“é›»è…¦æ¨¡æ“¬äººé¡æ€è€ƒå’Œå­¸ç¿’èƒ½åŠ›çš„æŠ€è¡“ã€‚å°±åƒäººé¡èƒ½å¤ å­¸ç¿’ã€æ¨ç†å’Œåšæ±ºå®šä¸€æ¨£ï¼Œäººå·¥æ™ºæ…§è®“æ©Ÿå™¨ä¹Ÿèƒ½åŸ·è¡Œé€™äº›ä»»å‹™..."
        - examples: ["æ‰‹æ©Ÿçš„èªéŸ³åŠ©æ‰‹Siri", "Netflixçš„é›»å½±æ¨è–¦ç³»çµ±", "Googleç¿»è­¯", "è‡ªå‹•é§•é§›æ±½è»Š", "æ£‹é¡éŠæˆ²AI"]
        """
        result = self._call_gemini(prompt)
        time.sleep(self.api_config['call_delay_seconds'])
        return result
    
    def test_word_explanation_by_story_id(self, story_id: str, save_to_db: bool = False):
        """æ¸¬è©¦åŠŸèƒ½ï¼šæ ¹æ“šæŒ‡å®šçš„ story_id ç”Ÿæˆè©å½™è§£é‡‹
        
        Args:
            story_id: è¦è™•ç†çš„æ–°è ID
            save_to_db: æ˜¯å¦å°‡çµæœå­˜å…¥è³‡æ–™åº«
        """
        print(f"\n=== æ¸¬è©¦è©å½™è§£é‡‹åŠŸèƒ½ (story_id: {story_id}) ===")
        if save_to_db:
            print("ğŸ”„ æ¨¡å¼ï¼šç”Ÿæˆè©å½™è§£é‡‹ä¸¦å­˜å…¥è³‡æ–™åº«")
        else:
            print("ğŸ“‹ æ¨¡å¼ï¼šåƒ…ç”Ÿæˆè©å½™è§£é‡‹é è¦½")
        
        # 1. å¾ single_news è¡¨è®€å–æŒ‡å®š story_id çš„è³‡æ–™
        try:
            table_name = self.db_config['table_name']
            fields = ','.join(self.db_config['select_fields'])
            
            resp = self.supabase_client.table(table_name).select(fields).eq('story_id', story_id).execute()
            
            if getattr(resp, 'error', None):
                print(f"âœ— è®€å–æ–°èå¤±æ•—: {resp.error}")
                return
            
            news_data = resp.data
            if not news_data:
                print(f"âœ— æ‰¾ä¸åˆ° story_id: {story_id}")
                return
            
            news = news_data[0]
            title = news.get(self.db_config['title_field'], 'æœªçŸ¥æ¨™é¡Œ')
            content = news.get(self.db_config['primary_content_field'], '')
            
            if not content:
                print(f"âœ— story_id {story_id} çš„å…§å®¹ç‚ºç©º")
                return
            
            print(f"âœ“ æˆåŠŸè®€å–æ–°è: {title[:50]}...")
            print(f"âœ“ å…§å®¹é•·åº¦: {len(content)} å­—")
            
        except Exception as e:
            print(f"âœ— è®€å–æ–°èè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return
        
        # 2. æå–å›°é›£é—œéµå­—
        print(f"\n--- æ­¥é©Ÿ 1: æå–å›°é›£é—œéµå­— ---")
        keywords = self.extract_keywords_from_text(content, title)
        
        if not keywords:
            print("âœ— æœªæ‰¾åˆ°å›°é›£é—œéµå­—")
            return
        
        print(f"âœ“ æ‰¾åˆ° {len(keywords)} å€‹å›°é›£é—œéµå­—:")
        for i, keyword in enumerate(keywords, 1):
            print(f"  {i}. {keyword}")
        
        # 3. ç‚ºæ¯å€‹é—œéµå­—ç”ŸæˆåŸºæ–¼æ–‡æ„çš„è§£é‡‹
        print(f"\n--- æ­¥é©Ÿ 2: ç”Ÿæˆè©å½™è§£é‡‹ ---")
        explanations = {}
        
        for keyword in keywords:
            print(f"\næ­£åœ¨ç‚ºã€Œ{keyword}ã€ç”Ÿæˆè§£é‡‹...")
            explanation = self.get_word_explanation_from_context(keyword, content, title)
            
            if explanation and explanation.get('term'):
                explanations[keyword] = explanation
                print(f"âœ“ æˆåŠŸç”Ÿæˆè§£é‡‹")
            else:
                print(f"âœ— æœªèƒ½ç”Ÿæˆè§£é‡‹")
        
        # 4. è¼¸å‡ºçµæœ
        print(f"\n" + "=" * 80)
        print(f"æ¸¬è©¦çµæœæ‘˜è¦")
        print("=" * 80)
        print(f"æ–°èæ¨™é¡Œ: {title}")
        print(f"story_id: {story_id}")
        print(f"æå–é—œéµå­—æ•¸: {len(keywords)}")
        print(f"æˆåŠŸè§£é‡‹æ•¸: {len(explanations)}")
        
        # è©³ç´°é¡¯ç¤ºæ¯å€‹è©å½™çš„è§£é‡‹
        print(f"\n" + "="*60)
        print("ç”Ÿæˆçš„è©å½™è§£é‡‹")
        print("="*60)
        
        for i, (word, explanation) in enumerate(explanations.items(), 1):
            term = explanation.get('term', word)
            explanation_text = explanation.get('explanation', 'ç„¡è§£é‡‹')
            examples = explanation.get('examples', [])
            
            print(f"\nã€è©å½™ {i}ã€‘")
            print(f"{term}")
            print(f"{explanation_text}")
            if examples:
                examples_text = "ã€".join(examples)
                print(f"{examples_text}")
            print("-" * 40)
        
        # 5. å¦‚æœé¸æ“‡å­˜å…¥è³‡æ–™åº«ï¼ŒåŸ·è¡Œè³‡æ–™åº«æ“ä½œ
        if save_to_db and explanations:
            print(f"\n--- æ­¥é©Ÿ 3: å­˜å…¥è³‡æ–™åº« ---")
            
            # æº–å‚™ term_map è³‡æ–™
            new_combinations = []
            for keyword in keywords:
                new_combinations.append({
                    'story_id': story_id,
                    'term': keyword
                })
            
            # æº–å‚™ term è³‡æ–™
            new_terms = []
            for word, explanation in explanations.items():
                explanation_text = explanation.get('explanation', '')
                examples = explanation.get('examples', [])
                examples_text = "ã€".join(examples) if examples else ""  # å°‡ä¾‹å­åˆ—è¡¨è½‰ç‚ºå­—ä¸²
                
                new_terms.append({
                    'term_id': str(uuid.uuid4()),  # ç”Ÿæˆå”¯ä¸€çš„ term_id
                    'term': word,
                    'definition': explanation_text,
                    'example': examples_text  # å°‡ä¾‹å­åˆ—è¡¨å­˜å…¥ example æ¬„ä½
                })
            
            # æª¢æŸ¥ç¾æœ‰è³‡æ–™ä¸¦åŸ·è¡Œæ’å…¥
            print("æª¢æŸ¥ç¾æœ‰è³‡æ–™...")
            
            # æª¢æŸ¥ term_map é‡è¤‡æ€§
            existing_term_map = self._check_existing_single_term_map(story_id, keywords)
            filtered_combinations = [combo for combo in new_combinations 
                                   if (combo['story_id'], combo['term']) not in existing_term_map]
            
            # æª¢æŸ¥ term é‡è¤‡æ€§
            existing_terms = self._check_existing_single_terms(list(explanations.keys()), explanations, {'story_id': story_id})
            filtered_terms = [term for term in new_terms 
                            if term['term'] not in existing_terms]
            
            print(f"æº–å‚™æ’å…¥ term_map: {len(filtered_combinations)} ç­†")
            print(f"æº–å‚™æ’å…¥ term: {len(filtered_terms)} ç­†")
            
            # åŸ·è¡Œæ’å…¥
            if filtered_terms:
                term_success = self.insert_term_data(filtered_terms)
            else:
                term_success = True
                print("æ‰€æœ‰è©å½™å·²å­˜åœ¨æ–¼ term è¡¨ä¸­")
            
            if filtered_combinations:
                term_map_success = self.insert_term_map_data(filtered_combinations)
            else:
                term_map_success = True
                print("æ‰€æœ‰çµ„åˆå·²å­˜åœ¨æ–¼ term_map è¡¨ä¸­")
            
            # é¡¯ç¤ºæœ€çµ‚çµæœ
            if term_success and term_map_success:
                print("âœ… è³‡æ–™åº«å„²å­˜å®Œæˆï¼")
            else:
                print("âŒ éƒ¨åˆ†è³‡æ–™å„²å­˜å¤±æ•—")
        
        return explanations
    
    def _check_existing_single_term_map(self, story_id: str, keywords: List[str]) -> Set:
        """æª¢æŸ¥å–®ä¸€ story_id çš„ç¾æœ‰ term_map çµ„åˆ"""
        try:
            table_name = self.db_config['term_map_table']
            resp = self.supabase_client.table(table_name).select('story_id,term').eq('story_id', story_id).execute()
            
            if getattr(resp, 'error', None):
                print(f"è®€å–ç¾æœ‰ term_map å¤±æ•—: {resp.error}")
                return set()
            
            existing_combinations = set()
            for row in resp.data or []:
                story_id_val = row.get('story_id')
                term = row.get('term')
                if story_id_val and term:
                    existing_combinations.add((story_id_val, term))
            
            return existing_combinations
            
        except Exception as e:
            print(f"æª¢æŸ¥ç¾æœ‰ term_map æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return set()
    
    def _check_existing_single_terms(self, keywords: List[str], explanations: Dict = None, story_context: Dict = None) -> Set:
        """æª¢æŸ¥å–®ä¸€è©å½™åˆ—è¡¨çš„ç¾æœ‰ termï¼Œä¸¦æ ¹æ“šæ–‡æ„åˆ¤æ–·æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ"""
        try:
            table_name = self.db_config['term_table']
            
            # ä½¿ç”¨ in_ æŸ¥è©¢æª¢æŸ¥å¤šå€‹è©å½™ï¼ŒåŒæ™‚ç²å–å®Œæ•´å®šç¾©è³‡è¨Š
            resp = self.supabase_client.table(table_name).select('term,definition,example').in_('term', keywords).execute()
            
            if getattr(resp, 'error', None):
                print(f"è®€å–ç¾æœ‰ term å¤±æ•—: {resp.error}")
                return set()
            
            existing_terms = set()
            for row in resp.data or []:
                term = row.get('term')
                existing_definition = row.get('definition', '')
                
                if term and explanations and term in explanations:
                    # å¦‚æœæœ‰æ–°è§£é‡‹ï¼Œæª¢æŸ¥æ–‡æ„æ˜¯å¦ç›¸ç¬¦
                    new_explanation = explanations[term]
                    is_contextually_appropriate = self._check_definition_context_match(
                        term, existing_definition, new_explanation, story_context
                    )
                    
                    if is_contextually_appropriate:
                        existing_terms.add(term)
                        print(f"è©å½™ '{term}' ç¾æœ‰å®šç¾©ç¬¦åˆæ–‡æ„ï¼Œå°‡é‡ç”¨")
                    else:
                        print(f"è©å½™ '{term}' ç¾æœ‰å®šç¾©ä¸ç¬¦æ–‡æ„ï¼Œå°‡é‡æ–°ç”Ÿæˆ")
                        # ä¸åŠ å…¥ existing_termsï¼Œé€™æ¨£æœƒè¢«è¦–ç‚ºæ–°è©å½™é‡æ–°ç”Ÿæˆ
                elif term:
                    # æ²’æœ‰æ–°è§£é‡‹çš„æƒ…æ³ï¼Œç›´æ¥åŠ å…¥ç¾æœ‰è©å½™
                    existing_terms.add(term)
            
            return existing_terms
            
        except Exception as e:
            print(f"æª¢æŸ¥ç¾æœ‰ term æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return set()

    def insert_term_map_data(self, new_combinations: List[Dict[str, str]]) -> bool:
        """å°‡æ–°çš„ term_map çµ„åˆæ’å…¥è³‡æ–™åº« (ä½¿ç”¨ term_id)"""
        if not new_combinations:
            print("æ²’æœ‰ term_map è³‡æ–™éœ€è¦æ’å…¥")
            return True
        
        print("\n=== é–‹å§‹æ’å…¥ term_map è³‡æ–™ ===")
        print(f"æº–å‚™æ’å…¥ {len(new_combinations)} ç­†è³‡æ–™åˆ° {self.db_config['term_map_table']} è¡¨")
        
        success_count = 0
        error_count = 0
        
        try:
            table_name = self.db_config['term_map_table']
            
            # æ‰¹æ¬¡æ’å…¥
            batch_size = 100  # æ¯æ‰¹æ’å…¥100ç­†
            for i in range(0, len(new_combinations), batch_size):
                batch = new_combinations[i:i + batch_size]
                
                try:
                    resp = self.supabase_client.table(table_name).insert(batch).execute()
                    
                    if getattr(resp, 'error', None):
                        print(f"æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥å¤±æ•—: {resp.error}")
                        error_count += len(batch)
                    else:
                        batch_success = len(batch)
                        success_count += batch_success
                        print(f"âœ“ æ‰¹æ¬¡ {i//batch_size + 1}: æˆåŠŸæ’å…¥ {batch_success} ç­†")
                
                except Exception as e:
                    print(f"âœ— æ‰¹æ¬¡ {i//batch_size + 1} ç™¼ç”ŸéŒ¯èª¤: {e}")
                    error_count += len(batch)
        
        except Exception as e:
            print(f"âœ— æ’å…¥ term_map æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
        
        print("\nterm_map æ’å…¥çµæœ:")
        print(f"  æˆåŠŸ: {success_count} ç­†")
        print(f"  å¤±æ•—: {error_count} ç­†")
        
        return error_count == 0

    def insert_term_data(self, new_terms: List[Dict[str, str]]) -> Dict[str, str]:
        """
        å°‡æ–°çš„é—œéµå­—å®šç¾©æ’å…¥ term è¡¨
        è¿”å› term -> term_id çš„æ˜ å°„
        """
        if not new_terms:
            print("æ²’æœ‰ term è³‡æ–™éœ€è¦æ’å…¥")
            return {}
        
        print("\n=== é–‹å§‹æ’å…¥ term è³‡æ–™ ===")
        print(f"æº–å‚™æ’å…¥ {len(new_terms)} ç­†è³‡æ–™åˆ° {self.db_config['term_table']} è¡¨")
        
        success_count = 0
        error_count = 0
        term_to_id_map = {}
        
        try:
            table_name = self.db_config['term_table']
            
            # ç‚ºæ¯å€‹è©å½™ç”Ÿæˆ term_id
            terms_with_id = []
            for term_data in new_terms:
                term_id = str(uuid.uuid4())
                term_with_id = {
                    'term_id': term_id,
                    'term': term_data['term'],
                    'definition': term_data['definition'],
                    'example': term_data.get('example', '')
                }
                terms_with_id.append(term_with_id)
                term_to_id_map[term_data['term']] = term_id
            
            # æ‰¹æ¬¡æ’å…¥
            batch_size = 50  # term è¡¨è³‡æ–™è¼ƒå¤§ï¼Œæ¯æ‰¹æ’å…¥50ç­†
            for i in range(0, len(terms_with_id), batch_size):
                batch = terms_with_id[i:i + batch_size]
                
                try:
                    resp = self.supabase_client.table(table_name).insert(batch).execute()
                    
                    if getattr(resp, 'error', None):
                        print(f"æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥å¤±æ•—: {resp.error}")
                        error_count += len(batch)
                        # ç§»é™¤å¤±æ•—çš„è©å½™æ˜ å°„
                        for failed_term in batch:
                            term_to_id_map.pop(failed_term['term'], None)
                    else:
                        batch_success = len(batch)
                        success_count += batch_success
                        print(f"âœ“ æ‰¹æ¬¡ {i//batch_size + 1}: æˆåŠŸæ’å…¥ {batch_success} ç­†")
                
                except Exception as e:
                    print(f"âœ— æ‰¹æ¬¡ {i//batch_size + 1} ç™¼ç”ŸéŒ¯èª¤: {e}")
                    error_count += len(batch)
                    # ç§»é™¤å¤±æ•—çš„è©å½™æ˜ å°„
                    for failed_term in batch:
                        term_to_id_map.pop(failed_term['term'], None)
        
        except Exception as e:
            print(f"âœ— æ’å…¥ term æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {}
        
        print("\nterm æ’å…¥çµæœ:")
        print(f"  æˆåŠŸ: {success_count} ç­†")
        print(f"  å¤±æ•—: {error_count} ç­†")
        print(f"  ç”Ÿæˆæ˜ å°„: {len(term_to_id_map)} å€‹")
        
        return term_to_id_map

    def check_existing_term_combinations(self, story_keywords: Dict, term_to_id_map: Dict[str, str]) -> List[Dict[str, str]]:
        """æª¢æŸ¥ä¸¦æº–å‚™éœ€è¦æ’å…¥åˆ° term_map çš„æ–°çµ„åˆ (ä½¿ç”¨ term_id)"""
        print("\n=== æª¢æŸ¥ term_map é‡è¤‡æ€§ ===")
        
        # å…ˆå–å¾—ç¾æœ‰çš„æ‰€æœ‰ term_map çµ„åˆ
        try:
            table_name = self.db_config['term_map_table']
            query = self.supabase_client.table(table_name).select('story_id,term_id')
            resp = query.execute()
            
            if getattr(resp, 'error', None):
                print(f"è®€å– {table_name} å¤±æ•—: {resp.error}")
                return []
            
            existing_combinations = set()
            for row in resp.data or []:
                story_id = row.get('story_id')
                term_id = row.get('term_id')
                if story_id and term_id:
                    existing_combinations.add((story_id, term_id))
            
            print(f"ç¾æœ‰ term_map çµ„åˆæ•¸é‡: {len(existing_combinations)}")
            
        except Exception as e:
            print(f"è®€å–ç¾æœ‰ term_map è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
        
        # æª¢æŸ¥å“ªäº›çµ„åˆæ˜¯æ–°çš„
        new_combinations = []
        
        for story_id, story_data in story_keywords.items():
            new_keywords = story_data.get("new_keywords", [])
            
            for keyword in new_keywords:
                term_id = term_to_id_map.get(keyword)
                if term_id:
                    combination = (story_id, term_id)
                    if combination not in existing_combinations:
                        new_combinations.append({
                            'story_id': story_id,
                            'term_id': term_id
                        })
                else:
                    print(f"è­¦å‘Š: æ‰¾ä¸åˆ°è©å½™ '{keyword}' çš„ term_id")
        
        print(f"æº–å‚™æ’å…¥çš„æ–°çµ„åˆæ•¸é‡: {len(new_combinations)}")
        return new_combinations
    
    def _check_definition_context_match(self, word: str, existing_definition: str, new_explanation: Dict, story_context: Dict = None) -> bool:
        """
        ä½¿ç”¨ AI åˆ¤æ–·ç¾æœ‰å®šç¾©æ˜¯å¦ç¬¦åˆç•¶å‰æ–‡æ„
        
        Args:
            word: è¦æª¢æŸ¥çš„è©å½™
            existing_definition: è³‡æ–™åº«ä¸­ç¾æœ‰çš„å®šç¾©
            new_explanation: æ–°ç”Ÿæˆçš„è§£é‡‹å…§å®¹
            story_context: æ•…äº‹ä¸Šä¸‹æ–‡ï¼ˆå¯é¸ï¼‰
        
        Returns:
            bool: True è¡¨ç¤ºç¾æœ‰å®šç¾©ç¬¦åˆæ–‡æ„ï¼ŒFalse è¡¨ç¤ºéœ€è¦é‡æ–°ç”Ÿæˆ
        """
        new_definition = new_explanation.get('definition', '')
        
        # å¦‚æœæ²’æœ‰æ–°å®šç¾©æˆ–ç¾æœ‰å®šç¾©ï¼Œç›´æ¥è¿”å› False
        if not new_definition or not existing_definition:
            return False
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„èªè¨€å­¸å°ˆå®¶ï¼Œæ“…é•·åˆ¤æ–·è©å½™åœ¨ä¸åŒèªå¢ƒä¸‹çš„å«ç¾©æ˜¯å¦ç›¸ç¬¦ã€‚
        
        è«‹æ¯”è¼ƒä»¥ä¸‹å…©å€‹é—œæ–¼è©å½™ã€Œ{word}ã€çš„å®šç¾©ï¼Œåˆ¤æ–·å®ƒå€‘æ˜¯å¦æŒ‡å‘ç›¸åŒçš„æ¦‚å¿µå’Œå«ç¾©ï¼š
        
        ã€ç¾æœ‰å®šç¾©ã€‘
        {existing_definition}
        
        ã€æ–°æ–‡æ„å®šç¾©ã€‘
        {new_definition}
        
        æ¯”è¼ƒè¦é»ï¼š
        1. æ ¸å¿ƒæ¦‚å¿µæ˜¯å¦ç›¸åŒï¼ˆä¾‹å¦‚ã€ŒIPã€å¯èƒ½æŒ‡ã€Œæ™ºæ…§è²¡ç”¢æ¬Šã€æˆ–ã€Œç¶²éš›ç¶²è·¯å”å®šã€ï¼Œé€™æ˜¯ä¸åŒæ¦‚å¿µï¼‰
        2. æ‡‰ç”¨é ˜åŸŸæ˜¯å¦ä¸€è‡´ï¼ˆä¾‹å¦‚ã€Œé›²ç«¯ã€åœ¨æ°£è±¡å­¸å’Œè³‡è¨Šç§‘æŠ€ä¸­å«ç¾©ä¸åŒï¼‰
        3. æŠ€è¡“å±¤é¢æ˜¯å¦åŒ¹é…ï¼ˆä¾‹å¦‚ã€Œæ¼”ç®—æ³•ã€åœ¨æ•¸å­¸å’Œç¨‹å¼è¨­è¨ˆä¸­çš„å…·é«”å«ç¾©ï¼‰
        4. ä½¿ç”¨èªå¢ƒæ˜¯å¦ç›¸ç¬¦ï¼ˆä¾‹å¦‚ã€Œå¹³å°ã€åœ¨å»ºç¯‰å’Œç¶²è·¯æœå‹™ä¸­çš„æ„æ€ï¼‰
        
        åˆ¤æ–·æ¨™æº–ï¼š
        - å¦‚æœå…©å€‹å®šç¾©æŒ‡å‘åŒä¸€å€‹æ¦‚å¿µï¼Œåªæ˜¯è¡¨é”æ–¹å¼ç•¥æœ‰ä¸åŒ â†’ ç›¸ç¬¦
        - å¦‚æœå…©å€‹å®šç¾©æŒ‡å‘å®Œå…¨ä¸åŒçš„æ¦‚å¿µæˆ–é ˜åŸŸ â†’ ä¸ç›¸ç¬¦
        - å¦‚æœä¸€å€‹å®šç¾©æ¯”å¦ä¸€å€‹æ›´å…·é«”æˆ–æ›´å»£æ³›ï¼Œä½†æ ¸å¿ƒæ¦‚å¿µç›¸åŒ â†’ ç›¸ç¬¦
        - å¦‚æœå­˜åœ¨æ­§ç¾©ä½†ä¸»è¦å«ç¾©ç›¸åŒ â†’ ç›¸ç¬¦
        
        è«‹åš´æ ¼ä»¥ JSON æ ¼å¼å›å‚³ï¼š
        {{
            "is_match": true/false,
            "reason": "ç°¡çŸ­èªªæ˜åˆ¤æ–·ç†ç”±ï¼ˆä¸è¶…é50å­—ï¼‰"
        }}
        """
        
        try:
            result = self._call_gemini(prompt)
            is_match = result.get('is_match', False)
            reason = result.get('reason', 'ç„¡æ³•åˆ¤æ–·')
            
            print(f"   AI åˆ¤æ–·çµæœ: {'ç›¸ç¬¦' if is_match else 'ä¸ç›¸ç¬¦'} - {reason}")
            return is_match
            
        except Exception as e:
            print(f"   AI åˆ¤æ–·æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ï¼Œé è¨­ç‚ºä¸ç›¸ç¬¦")
            return False
        
    def check_existing_terms(self, word_explanations: Dict, story_context: Dict = None) -> tuple[List[Dict[str, str]], Dict[str, str]]:
        """
        æª¢æŸ¥ä¸¦æº–å‚™éœ€è¦æ’å…¥åˆ° term è¡¨çš„æ–°é—œéµå­—å®šç¾©
        æ ¹æ“šæ–‡æ„æ¯”å°ç¾æœ‰å®šç¾©ï¼Œå¦‚æœä¸ç¬¦åˆå°±é‡æ–°ç”Ÿæˆ
        è¿”å›: (æ–°è©å½™åˆ—è¡¨, è©å½™åˆ°term_idçš„æ˜ å°„)
        """
        print("\n=== æª¢æŸ¥ term è¡¨é‡è¤‡æ€§èˆ‡æ–‡æ„ç¬¦åˆåº¦ ===")
        
        # å…ˆå–å¾—ç¾æœ‰çš„æ‰€æœ‰ term å®Œæ•´è³‡æ–™
        try:
            table_name = self.db_config['term_table']
            query = self.supabase_client.table(table_name).select('term,term_id,definition,example')
            resp = query.execute()
            
            if getattr(resp, 'error', None):
                print(f"è®€å– {table_name} å¤±æ•—: {resp.error}")
                return [], {}
            
            existing_terms = {}  # term -> {'term_id': str, 'definition': str, 'example': str}
            for row in resp.data or []:
                term = row.get('term')
                term_id = row.get('term_id')
                definition = row.get('definition', '')
                example = row.get('example', '')
                if term and term_id:
                    existing_terms[term] = {
                        'term_id': term_id,
                        'definition': definition,
                        'example': example
                    }
            
            print(f"ç¾æœ‰ term è¡¨ä¸­çš„é—œéµå­—æ•¸é‡: {len(existing_terms)}")
            
        except Exception as e:
            print(f"è®€å–ç¾æœ‰ term è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return [], {}
        
        # æª¢æŸ¥å“ªäº›é—œéµå­—æ˜¯æ–°çš„æˆ–éœ€è¦é‡æ–°ç”Ÿæˆ
        new_terms = []
        term_to_id_map = {}
        
        for word, explanation in word_explanations.items():
            if word in existing_terms:
                # è©å½™å·²å­˜åœ¨ï¼Œæª¢æŸ¥å®šç¾©æ˜¯å¦ç¬¦åˆæ–‡æ„
                existing_data = existing_terms[word]
                existing_definition = existing_data['definition']
                
                print(f"\næª¢æŸ¥è©å½™ '{word}' çš„æ–‡æ„ç¬¦åˆåº¦...")
                
                # ä½¿ç”¨ AI æ¯”å°ç¾æœ‰å®šç¾©èˆ‡ç•¶å‰æ–‡æ„æ˜¯å¦ç›¸ç¬¦
                is_contextually_appropriate = self._check_definition_context_match(
                    word, existing_definition, explanation, story_context
                )
                
                if is_contextually_appropriate:
                    # ç¾æœ‰å®šç¾©ç¬¦åˆæ–‡æ„ï¼Œä½¿ç”¨ç¾æœ‰çš„ term_id
                    term_to_id_map[word] = existing_data['term_id']
                    print(f"âœ“ è©å½™ '{word}' ç¾æœ‰å®šç¾©ç¬¦åˆæ–‡æ„ï¼Œä½¿ç”¨ç¾æœ‰ term_id: {existing_data['term_id']}")
                else:
                    # ç¾æœ‰å®šç¾©ä¸ç¬¦åˆæ–‡æ„ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆä¸¦æ’å…¥æ–°çš„å®šç¾©
                    term_id = str(uuid.uuid4())
                    
                    # å¾è§£é‡‹ä¸­æå–å®šç¾©å’Œæ‡‰ç”¨
                    definition = explanation.get('definition', '')
                    examples = explanation.get('examples', [])
                    example_text = examples[0].get('text', '') if examples else ''
                    
                    new_terms.append({
                        'term': word,
                        'definition': definition,
                        'example': example_text
                    })
                    
                    term_to_id_map[word] = term_id
                    print(f"âš  è©å½™ '{word}' ç¾æœ‰å®šç¾©ä¸ç¬¦æ–‡æ„ï¼Œå°‡ç”Ÿæˆæ–°å®šç¾© (æ–° term_id: {term_id})")
                    print(f"   åŸå®šç¾©: {existing_definition[:50]}...")
                    print(f"   æ–°å®šç¾©: {definition[:50]}...")
            else:
                # å…¨æ–°çš„è©å½™ï¼Œéœ€è¦æ’å…¥
                term_id = str(uuid.uuid4())
                
                # å¾è§£é‡‹ä¸­æå–å®šç¾©å’Œæ‡‰ç”¨
                definition = explanation.get('definition', '')
                examples = explanation.get('examples', [])
                example_text = examples[0].get('text', '') if examples else ''
                
                new_terms.append({
                    'term': word,
                    'definition': definition,
                    'example': example_text
                })
                
                term_to_id_map[word] = term_id
                print(f"+ æ–°è©å½™ '{word}' æº–å‚™æ’å…¥ (term_id: {term_id})")
        
        print(f"\næº–å‚™æ’å…¥çš„æ–°é—œéµå­—æ•¸é‡: {len(new_terms)}")
        print(f"ç¸½è©å½™æ˜ å°„æ•¸é‡: {len(term_to_id_map)}")
        return new_terms, term_to_id_map

    def run(self, limit: Optional[int] = None, story_ids: Optional[List[str]] = None):
        """åŸ·è¡Œå®Œæ•´çš„å›°é›£é—œéµå­—æå–æµç¨‹"""
        if not self.is_ready():
            logger.error("âœ— ç³»çµ±æœªå°±ç·’ï¼Œç„¡æ³•åŸ·è¡Œ")
            return

        logger.info("=" * 80)
        logger.info("  å›°é›£é—œéµå­—æå–ç³»çµ± - å¯å­˜å…¥è³‡æ–™åº«ç‰ˆæœ¬")
        logger.info("=" * 80)

        # 1. è®€å–ä¸¦åˆä½µ Supabase single_news å’Œ term_map è³‡æ–™
        news_data = self.fetch_combined_data(limit, story_ids)
        if not news_data:
            logger.warning("æœªå–å¾—ä»»ä½•è³‡æ–™")
            return

        # 2. æå–æ‰€æœ‰é—œéµå­—ï¼Œä¸¦æ ¹æ“š story_id çµ„ç¹”
        logger.info("=== éšæ®µä¸€ï¼šå¾æ–°èä¸­æå–å›°é›£é—œéµå­— ===")
        story_keywords = {}
        all_keywords: Set[str] = set()
        
        content_field = self.db_config['primary_content_field']
        title_field = self.db_config['title_field']
        
        for news in tqdm(news_data, desc="è™•ç†æ–°è"):
            story_id = news.get('story_id')
            if story_id is None:
                continue

            title = news.get(title_field, 'æœªçŸ¥æ¨™é¡Œ')
            content = news.get(content_field, '')
            existing_terms = news.get('existing_terms', [])
            
            if not content:
                print(f"âš  story_id {story_id} çš„ {content_field} æ¬„ä½ç‚ºç©ºï¼Œè·³é")
                continue
            
            # æå–é—œéµå­—
            keywords = self.extract_keywords_from_text(content, title)
            
            # åˆä½µæ–°æå–çš„é—œéµå­—å’Œç¾æœ‰çš„ terms
            all_story_keywords = list(set(keywords + existing_terms))
            
            # æ›´æ–°ç¸½é—œéµå­—é›†åˆ
            all_keywords.update(all_story_keywords)
            
            # å°‡é—œéµå­—åŠ å…¥å°æ‡‰çš„ story_id
            story_keywords[story_id] = {
                "title": title,
                "keywords": all_story_keywords,
                "new_keywords": keywords,
                "existing_terms": existing_terms
            }

        unique_keywords = sorted(list(all_keywords))
        logger.info(f"âœ“ éšæ®µä¸€å®Œæˆï¼šå…±æå– {len(unique_keywords)} å€‹ä¸é‡è¤‡é—œéµå­—ã€‚")

        # 3. ç‚ºé—œéµå­—ç”Ÿæˆè§£é‡‹
        logger.info("=== éšæ®µäºŒï¼šç‚ºé—œéµå­—ç”Ÿæˆè§£é‡‹èˆ‡ç¯„ä¾‹ ===")
        word_explanations = {}
        for word in tqdm(unique_keywords, desc="ç”Ÿæˆè©å½™è§£é‡‹"):
            explanation = self.get_word_explanation(word)
            if explanation and "term" in explanation:
                word_explanations[word] = explanation
            else:
                logger.warning(f"âš  æœªèƒ½æˆåŠŸè§£é‡‹è©å½™ï¼š'{word}'")
        
        logger.info(f"âœ“ éšæ®µäºŒå®Œæˆï¼šå…±æˆåŠŸè§£é‡‹ {len(word_explanations)} å€‹è©å½™ã€‚")

        # 4. æª¢æŸ¥ä¸¦æº–å‚™æ’å…¥è³‡æ–™
        logger.info("=== éšæ®µä¸‰ï¼šæª¢æŸ¥é‡è¤‡æ€§ä¸¦æº–å‚™æ’å…¥ ===")
        # æº–å‚™æ•…äº‹ä¸Šä¸‹æ–‡è³‡è¨Šä¾› AI åˆ¤æ–·ä½¿ç”¨
        story_context = {
            'story_keywords': story_keywords,
            'total_stories': len(story_keywords)
        }
        
        new_terms, term_to_id_map = self.check_existing_terms(word_explanations, story_context)
        
        # 5. åŸ·è¡Œè³‡æ–™åº«æ’å…¥
        logger.info("=== éšæ®µå››ï¼šåŸ·è¡Œè³‡æ–™åº«æ’å…¥ ===")
        
        # å…ˆæ’å…¥ term è¡¨ï¼ˆé—œéµå­—å®šç¾©ï¼‰
        if new_terms:
            term_id_map_from_insert = self.insert_term_data(new_terms)
            # åˆä½µæ–°æ’å…¥çš„è©å½™ term_id æ˜ å°„
            term_to_id_map.update(term_id_map_from_insert)
            term_success = len(term_id_map_from_insert) > 0
        else:
            print("æ²’æœ‰æ–°è©å½™éœ€è¦æ’å…¥åˆ° term è¡¨")
            term_success = True
        
        # ç„¶å¾Œæº–å‚™ term_map é—œè¯
        new_combinations = self.check_existing_term_combinations(story_keywords, term_to_id_map)
        
        # å†æ’å…¥ term_map è¡¨ï¼ˆstory_id å’Œ term_id çš„é—œè¯ï¼‰
        term_map_success = self.insert_term_map_data(new_combinations)

        # 6. é¡¯ç¤ºæœ€çµ‚çµæœ
        logger.info("=" * 80)
        logger.info("  åŸ·è¡Œå®Œæˆæ‘˜è¦")
        logger.info("=" * 80)
        logger.info(f"âœ“ è™•ç†æ–°èæ•¸é‡: {len(story_keywords)}")
        logger.info(f"âœ“ ä¸é‡è¤‡é—œéµå­—: {len(unique_keywords)}")
        logger.info(f"âœ“ æˆåŠŸè§£é‡‹è©å½™: {len(word_explanations)}")
        
        if new_terms:
            status = "âœ“ æˆåŠŸ" if term_success else "âœ— å¤±æ•—"
            logger.info(f"{status} æ’å…¥ term è¡¨: {len(new_terms)} å€‹æ–°é—œéµå­—")
        
        if new_combinations:
            status = "âœ“ æˆåŠŸ" if term_map_success else "âœ— å¤±æ•—"
            logger.info(f"{status} æ’å…¥ term_map è¡¨: {len(new_combinations)} ç­†æ–°çµ„åˆ")
        
        logger.info("=" * 80)


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    print("=" * 80)
    print("  å›°é›£é—œéµå­—æå–ç³»çµ± - å¯å­˜å…¥è³‡æ–™åº«ç‰ˆæœ¬")
    print("=" * 80)
    
    # è§£ææŒ‡ä»¤åˆ—åƒæ•¸
    if len(sys.argv) > 1:
        first_arg = sys.argv[1]
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ¸¬è©¦æ¨¡å¼
        if first_arg == "test" and len(sys.argv) > 2:
            story_id = sys.argv[2]
            
            # æª¢æŸ¥æ˜¯å¦è¦å­˜å…¥è³‡æ–™åº«
            save_to_db = False
            if len(sys.argv) > 3 and sys.argv[3] == "--save":
                save_to_db = True
                print(f"âœ“ æ¸¬è©¦æ¨¡å¼: ä½¿ç”¨ story_id = {story_id} (å­˜å…¥è³‡æ–™åº«)")
            else:
                print(f"âœ“ æ¸¬è©¦æ¨¡å¼: ä½¿ç”¨ story_id = {story_id} (åƒ…é è¦½)")
            
            try:
                processor = DiffKeywordProcessor()
                if processor.is_ready():
                    processor.test_word_explanation_by_story_id(story_id, save_to_db)
                else:
                    print("âœ— ç³»çµ±æœªå°±ç·’ï¼Œç„¡æ³•åŸ·è¡Œæ¸¬è©¦")
            except EnvironmentError as e:
                print(f"âœ— ç’°å¢ƒéŒ¯èª¤ï¼š{e}")
                print("è«‹æª¢æŸ¥æ‚¨çš„ .env è¨­å®šæª”ã€‚")
            except Exception as e:
                print(f"âœ— ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š{e}")
            
            return
        
        # ä¸€èˆ¬æ¨¡å¼ï¼šè§£æ limit åƒæ•¸
        try:
            limit = int(first_arg)
            print(f"âœ“ è¨­å®šè®€å–é™åˆ¶: {limit} ç­†")
        except ValueError:
            print("âš  ç„¡æ•ˆçš„ limit åƒæ•¸ï¼Œå°‡è®€å–æ‰€æœ‰è³‡æ–™")
            limit = None
    else:
        limit = None
        print("âœ“ ä¸€èˆ¬æ¨¡å¼: è™•ç†æ‰€æœ‰è³‡æ–™")
    
    try:
        # åˆå§‹åŒ–ä¸¦åŸ·è¡Œè™•ç†å™¨
        processor = DiffKeywordProcessor()
        if processor.is_ready():
            processor.run(limit)
        
    except EnvironmentError as e:
        print(f"âœ— ç’°å¢ƒéŒ¯èª¤ï¼š{e}")
        print("è«‹æª¢æŸ¥æ‚¨çš„ .env è¨­å®šæª”ã€‚")
        sys.exit(1)
    except Exception as e:
        print(f"âœ— ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š{e}")
        sys.exit(1)
        
    print("\n" + "=" * 80)
    print("ç³»çµ±åŸ·è¡Œå®Œç•¢ã€‚")
    print("=" * 80)


if __name__ == "__main__":
    main()
