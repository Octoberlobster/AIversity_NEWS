"""
æ™ºèƒ½æ–‡æœ¬åˆ†æèˆ‡è©å½™è§£é‡‹ç¨‹å¼
å¾JSONæª”æ¡ˆä¸­æå–æ–‡å­—ã€è­˜åˆ¥å›°é›£è©å½™ä¸¦è‡ªå‹•ç”Ÿæˆè§£é‡‹

ä¸»è¦åŠŸèƒ½ï¼š
1. å¾JSONæª”æ¡ˆè®€å–æ–‡å­—å…§å®¹
2. ä½¿ç”¨AIè­˜åˆ¥å›°é›£è©å½™
3. è‡ªå‹•ç”Ÿæˆè©å½™è§£é‡‹
4. è¼¸å‡ºå®Œæ•´çµæœ
"""

import json
import os
import re
import time
from typing import List, Dict, Set, Optional, Union
import google.generativeai as genai
from dotenv import load_dotenv

class TextAnalyzer:
    """æ–‡æœ¬åˆ†æå™¨ - ç”¨æ–¼å¾JSONä¸­æå–æ–‡å­—ä¸¦è­˜åˆ¥å›°é›£è©å½™"""
    
    def __init__(self):
        self.difficult_words = set()
        self.stopwords = self._load_stopwords()
    
    def _load_stopwords(self) -> Set[str]:
        """è¼‰å…¥åœç”¨è©åˆ—è¡¨"""
        # å¸¸è¦‹çš„ä¸­æ–‡åœç”¨è©
        default_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€å€‹',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'èªª', 'è¦', 'å»', 'ä½ ', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'é€™', 'é‚„', 'å¯ä»¥', 'å‡º', 'ä¾†', 'ä»–', 'å¥¹', 'å®ƒ', 'é€™å€‹', 'é‚£å€‹', 'å› ç‚º',
            'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å¾Œ', 'å¦‚æœ', 'é€™æ¨£', 'é‚£æ¨£', 'ä»€éº¼', 'æ€éº¼', 'ç‚ºä»€éº¼',
            'ç­‰', 'ç­‰ç­‰', 'ä»¥åŠ', 'ä»¥åŠ', 'ä¸¦ä¸”', 'æˆ–è€…', 'è€Œä¸”', 'å¯èƒ½', 'æ‡‰è©²', 'å¿…é ˆ',
            'å·²ç¶“', 'é‚„æ˜¯', 'æˆ–æ˜¯', 'å¦å‰‡', 'é›–ç„¶', 'ä¸é', 'åªæ˜¯', 'è€Œå·²', 'è€Œå·²',
            'ã€', 'ï¼Œ', 'ã€‚', 'ï¼Ÿ', 'ï¼', 'ï¼š', 'ï¼›', '"', '"', ''', ''', 'ï¼ˆ', 'ï¼‰',
            'ã€', 'ã€‘', 'ã€Š', 'ã€‹', 'ã€ˆ', 'ã€‰', 'â€¦', 'â€”â€”', 'ï¼', 'Â·'
        }
        
        # å˜—è©¦å¾æª”æ¡ˆè¼‰å…¥åœç”¨è©ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        stopwords_file = "Stopwords-zhTW.txt"
        if os.path.exists(stopwords_file):
            try:
                with open(stopwords_file, 'r', encoding='utf-8') as f:
                    file_stopwords = {line.strip() for line in f if line.strip()}
                default_stopwords.update(file_stopwords)
                print(f"å·²è¼‰å…¥åœç”¨è©æª”æ¡ˆï¼š{stopwords_file}")
            except Exception as e:
                print(f"è¼‰å…¥åœç”¨è©æª”æ¡ˆå¤±æ•—ï¼š{e}")
        
        return default_stopwords
    
    def extract_text_from_json(self, json_data: Union[Dict, List, str], max_depth: int = 10) -> List[str]:
        """
        å¾JSONè³‡æ–™ä¸­éæ­¸æå–æ‰€æœ‰æ–‡å­—å…§å®¹
        
        Args:
            json_data: JSONè³‡æ–™
            max_depth: æœ€å¤§éæ­¸æ·±åº¦
            
        Returns:
            æå–çš„æ–‡å­—åˆ—è¡¨
        """
        texts = []
        
        def _extract_recursive(data, depth=0):
            if depth > max_depth:
                return
            
            if isinstance(data, str):
                # æ¸…ç†æ–‡å­—ä¸¦æ·»åŠ åˆ°åˆ—è¡¨
                cleaned_text = self._clean_text(data)
                if cleaned_text and len(cleaned_text) > 2:  # éæ¿¾å¤ªçŸ­çš„æ–‡å­—
                    texts.append(cleaned_text)
            
            elif isinstance(data, dict):
                for key, value in data.items():
                    # ä¹Ÿæå–éµåä¸­çš„æ–‡å­—
                    if isinstance(key, str):
                        cleaned_key = self._clean_text(key)
                        if cleaned_key and len(cleaned_key) > 1:
                            texts.append(cleaned_key)
                    _extract_recursive(value, depth + 1)
            
            elif isinstance(data, list):
                for item in data:
                    _extract_recursive(item, depth + 1)
        
        _extract_recursive(json_data)
        return texts
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡å­—ï¼Œç§»é™¤å¤šé¤˜çš„ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦"""
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ¨™ç±¤
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ç§»é™¤ç´”æ•¸å­—å’Œç´”è‹±æ–‡ï¼ˆæ ¹æ“šéœ€æ±‚èª¿æ•´ï¼‰
        if re.match(r'^[0-9\s\-\.]+$', text) or re.match(r'^[a-zA-Z\s]+$', text):
            return ""
        
        return text
    
    def identify_difficult_words(self, texts: List[str], model) -> List[str]:
        """
        ä½¿ç”¨AIè­˜åˆ¥å›°é›£è©å½™
        
        Args:
            texts: æ–‡å­—åˆ—è¡¨
            model: Geminiæ¨¡å‹
            
        Returns:
            å›°é›£è©å½™åˆ—è¡¨
        """
        # åˆä½µæ‰€æœ‰æ–‡å­—
        combined_text = " ".join(texts)
        
        # å¦‚æœæ–‡å­—å¤ªé•·ï¼Œæˆªå–å‰é¢éƒ¨åˆ†
        if len(combined_text) > 8000:
            combined_text = combined_text[:8000] + "..."
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ä¸­æ–‡èªè¨€åˆ†æå¸«ã€‚è«‹å¾ä»¥ä¸‹æ–‡å­—ä¸­è­˜åˆ¥å‡ºå¯èƒ½å°ä¸€èˆ¬è®€è€…ä¾†èªªæ¯”è¼ƒå›°é›£ã€å°ˆæ¥­æˆ–ä¸å¸¸è¦‹çš„è©å½™ã€‚

        è«‹éµå¾ªä»¥ä¸‹æ¨™æº–ï¼š
        1. å°ˆæ¥­è¡“èªï¼ˆå¦‚ï¼šæ³•å¾‹ã€é†«å­¸ã€ç§‘æŠ€ç­‰é ˜åŸŸçš„å°ˆæœ‰åè©ï¼‰
        2. ä¸å¸¸ç”¨çš„è©å½™æˆ–æˆèª
        3. å¤–ä¾†èªéŸ³è­¯è©
        4. ç¸®å¯«æˆ–ç°¡ç¨±
        5. é•·åº¦åœ¨2-8å€‹å­—çš„è©å½™

        è«‹æ’é™¤ï¼š
        - å¸¸è¦‹çš„æ—¥å¸¸ç”¨èª
        - äººåã€åœ°åï¼ˆé™¤éæ˜¯å°ˆæ¥­è¡“èªï¼‰
        - ç´”æ•¸å­—æˆ–æ—¥æœŸ
        - å–®å€‹å­—ç¬¦

        è«‹ä»¥JSONæ ¼å¼å›å‚³å›°é›£è©å½™åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {{"difficult_words": ["è©å½™1", "è©å½™2", "è©å½™3"]}}

        è¦åˆ†æçš„æ–‡å­—ï¼š
        {combined_text}
        """
        
        try:
            response = model.generate_content(prompt)
            cleaned_text = response.text.strip()
            
            # æ¸…ç†å›æ‡‰
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text[7:].strip()
            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3].strip()
            
            result = json.loads(cleaned_text)
            difficult_words = result.get("difficult_words", [])
            
            # éæ¿¾åœç”¨è©å’Œé‡è¤‡è©å½™
            filtered_words = []
            for word in difficult_words:
                if (word not in self.stopwords and 
                    len(word) >= 2 and 
                    len(word) <= 8 and
                    word not in filtered_words):
                    filtered_words.append(word)
            
            return filtered_words[:20]  # é™åˆ¶æœ€å¤š20å€‹è©å½™
            
        except Exception as e:
            print(f"è­˜åˆ¥å›°é›£è©å½™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return []

class SmartTextProcessor:
    """æ™ºèƒ½æ–‡æœ¬è™•ç†å™¨ - æ•´åˆæ–‡æœ¬åˆ†æå’Œè©å½™è§£é‡‹åŠŸèƒ½"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.analyzer = TextAnalyzer()
        self.api_key = api_key
        self.model = None
        self._setup_model()
    
    def _setup_model(self):
        """è¨­ç½®Geminiæ¨¡å‹"""
        if not self.api_key:
            load_dotenv()
            self.api_key = os.getenv("GEMINI_API_KEY")
        
        if not self.api_key:
            raise ValueError("éŒ¯èª¤ï¼šè«‹è¨­å®š GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸æˆ–æä¾› api_key åƒæ•¸")
        
        try:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-1.5-pro-latest')
            print("âœ… Gemini API åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"åˆå§‹åŒ– Gemini æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def process_json_file(self, 
                         json_file: str, 
                         output_file: Optional[str] = None,
                         verbose: bool = True) -> Dict:
        """
        è™•ç†JSONæª”æ¡ˆçš„å®Œæ•´æµç¨‹
        
        Args:
            json_file: è¼¸å…¥çš„JSONæª”æ¡ˆè·¯å¾‘
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°éç¨‹
            
        Returns:
            åŒ…å«å›°é›£è©å½™è§£é‡‹çš„å®Œæ•´çµæœ
        """
        if verbose:
            print(f"ğŸ“– é–‹å§‹è™•ç†æª”æ¡ˆï¼š{json_file}")
        
        # æ­¥é©Ÿ1ï¼šè®€å–JSONæª”æ¡ˆ
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            if verbose:
                print("âœ… JSONæª”æ¡ˆè®€å–æˆåŠŸ")
        except Exception as e:
            raise FileNotFoundError(f"è®€å–JSONæª”æ¡ˆå¤±æ•—ï¼š{e}")
        
        # æ­¥é©Ÿ2ï¼šæå–æ–‡å­—å…§å®¹
        if verbose:
            print("ğŸ” æ­£åœ¨æå–æ–‡å­—å…§å®¹...")
        texts = self.analyzer.extract_text_from_json(json_data)
        if verbose:
            print(f"âœ… å·²æå– {len(texts)} æ®µæ–‡å­—å…§å®¹")
        
        if not texts:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡å­—å…§å®¹")
            return {"texts": [], "difficult_words": [], "explanations": {"terms": []}}
        
        # æ­¥é©Ÿ3ï¼šè­˜åˆ¥å›°é›£è©å½™
        if verbose:
            print("ğŸ§  æ­£åœ¨è­˜åˆ¥å›°é›£è©å½™...")
        difficult_words = self.analyzer.identify_difficult_words(texts, self.model)
        if verbose:
            print(f"âœ… è­˜åˆ¥å‡º {len(difficult_words)} å€‹å›°é›£è©å½™ï¼š{difficult_words}")
        
        if not difficult_words:
            print("â„¹ï¸ æœªè­˜åˆ¥å‡ºå›°é›£è©å½™")
            return {"texts": texts, "difficult_words": [], "explanations": {"terms": []}}
        
        # æ­¥é©Ÿ4ï¼šç”Ÿæˆè©å½™è§£é‡‹
        if verbose:
            print("ğŸ“ æ­£åœ¨ç”Ÿæˆè©å½™è§£é‡‹...")
        explanations = self._explain_words(difficult_words, verbose)
        
        # æ­¥é©Ÿ5ï¼šçµ„åˆæœ€çµ‚çµæœ
        final_result = {
            "source_file": json_file,
            "processing_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "extracted_texts_count": len(texts),
            "extracted_texts": texts[:5],  # åªä¿ç•™å‰5æ®µä½œç‚ºæ¨£æœ¬
            "difficult_words_count": len(difficult_words),
            "difficult_words": difficult_words,
            "explanations": explanations
        }
        
        # æ­¥é©Ÿ6ï¼šå„²å­˜çµæœï¼ˆå¦‚æœæŒ‡å®šäº†è¼¸å‡ºæª”æ¡ˆï¼‰
        if output_file:
            self._save_result(final_result, output_file, verbose)
        
        if verbose:
            print("ğŸ‰ è™•ç†å®Œæˆï¼")
        
        return final_result
    
    def _explain_words(self, words: List[str], verbose: bool = True) -> Dict:
        """ç”Ÿæˆè©å½™è§£é‡‹"""
        terms_list = []
        
        prompt_template = """
        ä½ æ˜¯ä¸€ä½çŸ¥è­˜æ·µåšçš„è©å…¸ç·¨çº‚å°ˆå®¶ã€‚
        é‡å°ä»¥ä¸‹æä¾›çš„è©å½™ï¼Œè«‹æä¾›ç´„50å­—çš„ã€Œåè©è§£é‡‹ã€å’Œã€Œæ‡‰ç”¨ç¯„ä¾‹ã€ã€‚
        è«‹åš´æ ¼ä¾ç…§ä»¥ä¸‹ JSON æ ¼å¼å›å‚³ï¼Œä¸è¦åŒ…å«ä»»ä½• markdown æ¨™ç±¤æˆ–é¡å¤–çš„èªªæ˜æ–‡å­—ã€‚

        æ ¼å¼ç¯„ä¾‹ï¼š
        {{
          "definition": "é€™è£¡æ”¾ç°¡æ½”æ˜ç­çš„åè©è§£é‡‹ã€‚",
          "example_text": "é€™æ˜¯ä¸€å€‹æ‡‰ç”¨ç¯„ä¾‹ã€‚\\n\\né€™æ˜¯å¦ä¸€å€‹æ‡‰ç”¨ç¯„ä¾‹ã€‚\\n\\nç¬¬ä¸‰å€‹æ‡‰ç”¨ç¯„ä¾‹ã€‚"
        }}

        è¦è§£é‡‹çš„è©å½™æ˜¯ï¼šã€Œ{word}ã€
        
        æ³¨æ„ï¼š
        1. definition æ‡‰è©²æ˜¯ç°¡æ½”çš„å®šç¾©èªªæ˜
        2. example_text æ‡‰è©²åŒ…å«2-3å€‹æ‡‰ç”¨ç¯„ä¾‹ï¼Œä½¿ç”¨ \\n\\n åˆ†éš”
        3. ç¯„ä¾‹æ‡‰è©²å±•ç¤ºè©²è©å½™åœ¨å¯¦éš›èªå¢ƒä¸­çš„ä½¿ç”¨æ–¹å¼
        """

        for i, word in enumerate(words, 1):
            if verbose:
                print(f"  ğŸ“ è§£é‡‹è©å½™ ({i}/{len(words)})ï¼šã€Œ{word}ã€...")
            
            prompt = prompt_template.format(word=word)
            
            try:
                response = self.model.generate_content(prompt)
                
                # æ¸…ç†å›æ‡‰æ–‡å­—
                cleaned_text = response.text.strip()
                if cleaned_text.startswith("```json"):
                    cleaned_text = cleaned_text[7:].strip()
                if cleaned_text.endswith("```"):
                    cleaned_text = cleaned_text[:-3].strip()
                
                # è§£æJSONå›æ‡‰
                result = json.loads(cleaned_text)
                
                # è½‰æ›ç‚ºç›®æ¨™æ ¼å¼
                term_data = {
                    "term": word,
                    "definition": f"ï¼ˆç¤ºä¾‹ï¼‰{result['definition']}",
                    "examples": [
                        {
                            "title": "æ‡‰ç”¨ä¾‹å­",
                            "text": result["example_text"]
                        }
                    ]
                }
                
                terms_list.append(term_data)
                
                # å»¶é²ä»¥é¿å…è¶…éAPIé€Ÿç‡é™åˆ¶
                if i < len(words):
                    time.sleep(1)
                
            except json.JSONDecodeError:
                if verbose:
                    print(f"    âš ï¸ è§£æè©å½™ã€Œ{word}ã€çš„å›æ‡‰å¤±æ•—ï¼Œè·³é")
                continue
            except Exception as e:
                if verbose:
                    print(f"    âš ï¸ è§£é‡‹è©å½™ã€Œ{word}ã€æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                continue
        
        return {"terms": terms_list}
    
    def _save_result(self, result: Dict, output_file: str, verbose: bool = True):
        """å„²å­˜çµæœåˆ°æª”æ¡ˆ"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            if verbose:
                print(f"ğŸ’¾ çµæœå·²å„²å­˜åˆ°ï¼š{output_file}")
        except Exception as e:
            print(f"âŒ å„²å­˜æª”æ¡ˆå¤±æ•—ï¼š{e}")

def main():
    """ä¸»ç¨‹å¼"""
    import sys
    
    print("ğŸš€ æ™ºèƒ½æ–‡æœ¬åˆ†æèˆ‡è©å½™è§£é‡‹ç¨‹å¼")
    print("=" * 50)
    
    # æª¢æŸ¥å‘½ä»¤åˆ—åƒæ•¸
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
        output_file = sys.argv[2] if len(sys.argv) > 2 else None
    else:
        # äº’å‹•æ¨¡å¼
        input_file = input("è«‹è¼¸å…¥JSONæª”æ¡ˆè·¯å¾‘ï¼š").strip()
        output_file = input("è«‹è¼¸å…¥è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆç›´æ¥æŒ‰Enterè·³éï¼‰ï¼š").strip()
        if not output_file:
            output_file = None
    
    if not input_file:
        print("âŒ è«‹æä¾›JSONæª”æ¡ˆè·¯å¾‘")
        return
    
    try:
        # å‰µå»ºè™•ç†å™¨ä¸¦åŸ·è¡Œ
        processor = SmartTextProcessor()
        result = processor.process_json_file(input_file, output_file)
        
        # é¡¯ç¤ºçµæœæ‘˜è¦
        print("\nğŸ“Š è™•ç†çµæœæ‘˜è¦ï¼š")
        print("=" * 30)
        print(f"ä¾†æºæª”æ¡ˆï¼š{result['source_file']}")
        print(f"è™•ç†æ™‚é–“ï¼š{result['processing_date']}")
        print(f"æå–æ–‡å­—æ®µæ•¸ï¼š{result['extracted_texts_count']}")
        print(f"è­˜åˆ¥å›°é›£è©å½™æ•¸ï¼š{result['difficult_words_count']}")
        print(f"æˆåŠŸè§£é‡‹è©å½™æ•¸ï¼š{len(result['explanations']['terms'])}")
        
        if result['difficult_words']:
            print(f"\nğŸ”¤ å›°é›£è©å½™ï¼š{', '.join(result['difficult_words'])}")
        
        if not output_file:
            print("\nğŸ“ å®Œæ•´çµæœï¼š")
            print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    main()
