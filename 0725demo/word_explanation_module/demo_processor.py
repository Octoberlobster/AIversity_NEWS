#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æœ¬è™•ç†æ¼”ç¤ºè…³æœ¬
å±•ç¤ºå¦‚ä½•å¾JSONä¸­æå–å…§å®¹ä¸¦è­˜åˆ¥å›°é›£è©å½™ï¼ˆæ¨¡æ“¬ç‰ˆæœ¬ï¼‰
"""

import json
import re
from typing import List, Dict, Set

class DemoTextAnalyzer:
    """æ¼”ç¤ºç‰ˆæ–‡æœ¬åˆ†æå™¨"""
    
    def __init__(self):
        self.stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€å€‹',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'èªª', 'è¦', 'å»', 'ä½ ', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'é€™', 'é‚„', 'å¯ä»¥', 'å‡º', 'ä¾†', 'ä»–', 'å¥¹', 'å®ƒ', 'é€™å€‹', 'é‚£å€‹', 'å› ç‚º',
            'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å¾Œ', 'å¦‚æœ', 'é€™æ¨£', 'é‚£æ¨£', 'ä»€éº¼', 'æ€éº¼', 'ç‚ºä»€éº¼',
            'ç­‰', 'ç­‰ç­‰', 'ä»¥åŠ', 'ä¸¦ä¸”', 'æˆ–è€…', 'è€Œä¸”', 'å¯èƒ½', 'æ‡‰è©²', 'å¿…é ˆ',
            'å·²ç¶“', 'é‚„æ˜¯', 'æˆ–æ˜¯', 'å¦å‰‡', 'é›–ç„¶', 'ä¸é', 'åªæ˜¯', 'è€Œå·²',
            'ã€', 'ï¼Œ', 'ã€‚', 'ï¼Ÿ', 'ï¼', 'ï¼š', 'ï¼›', '"', '"', ''', ''', 'ï¼ˆ', 'ï¼‰'
        }
    
    def extract_text_from_json(self, json_data) -> List[str]:
        """å¾JSONä¸­æå–æ–‡å­—"""
        texts = []
        
        def _extract(data):
            if isinstance(data, str):
                cleaned = self._clean_text(data)
                if cleaned and len(cleaned) > 2:
                    texts.append(cleaned)
            elif isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(key, str):
                        cleaned_key = self._clean_text(key)
                        if cleaned_key and len(cleaned_key) > 1:
                            texts.append(cleaned_key)
                    _extract(value)
            elif isinstance(data, list):
                for item in data:
                    _extract(item)
        
        _extract(json_data)
        return texts
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡å­—"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        # éæ¿¾ç´”æ•¸å­—æˆ–ç´”è‹±æ–‡
        if re.match(r'^[0-9\s\-\.]+$', text) or re.match(r'^[a-zA-Z\s]+$', text):
            return ""
        
        return text
    
    def identify_difficult_words_demo(self, texts: List[str]) -> List[str]:
        """æ¨¡æ“¬è­˜åˆ¥å›°é›£è©å½™"""
        # é å®šç¾©çš„å›°é›£è©å½™æ¨¡å¼
        tech_terms = [
            'äººå·¥æ™ºæ…§', 'æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’', 'ç¥ç¶“ç¶²è·¯', 'è‡ªç„¶èªè¨€è™•ç†',
            'å€å¡Šéˆ', 'æ™ºèƒ½åˆç´„', 'åˆ†æ•£å¼å¸³æœ¬', 'å»ä¸­å¿ƒåŒ–', 'é‡å­è¨ˆç®—',
            'é‡å­ç³¾çº', 'é‡å­ç–ŠåŠ ', 'å·ç©ç¥ç¶“ç¶²è·¯', 'é†«ç™‚è¨ºæ–·', 'å½±åƒåˆ†æ'
        ]
        
        combined_text = " ".join(texts)
        found_words = []
        
        for term in tech_terms:
            if term in combined_text and term not in found_words:
                found_words.append(term)
        
        return found_words
    
    def generate_demo_explanations(self, words: List[str]) -> Dict:
        """ç”Ÿæˆæ¼”ç¤ºç”¨çš„è©å½™è§£é‡‹"""
        explanations = {
            'äººå·¥æ™ºæ…§': {
                'definition': 'æ¨¡æ“¬äººé¡æ™ºèƒ½çš„è¨ˆç®—æ©Ÿç³»çµ±ï¼Œèƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºèƒ½çš„ä»»å‹™ã€‚',
                'examples': 'äººå·¥æ™ºæ…§æŠ€è¡“è¢«å»£æ³›æ‡‰ç”¨æ–¼è‡ªå‹•é§•é§›æ±½è»Šä¸­ã€‚\n\né†«é™¢ä½¿ç”¨äººå·¥æ™ºæ…§ä¾†å”åŠ©è¨ºæ–·ç–¾ç—…ã€‚\n\næ™ºèƒ½æ‰‹æ©Ÿçš„èªéŸ³åŠ©æ‰‹å°±æ˜¯äººå·¥æ™ºæ…§çš„æ‡‰ç”¨ã€‚'
            },
            'æ©Ÿå™¨å­¸ç¿’': {
                'definition': 'ä¸€ç¨®äººå·¥æ™ºæ…§çš„åˆ†æ”¯ï¼Œè®“è¨ˆç®—æ©Ÿèƒ½å¤ å¾æ•¸æ“šä¸­è‡ªå‹•å­¸ç¿’å’Œæ”¹é€²ã€‚',
                'examples': 'æ©Ÿå™¨å­¸ç¿’ç®—æ³•å¯ä»¥é æ¸¬è‚¡ç¥¨åƒ¹æ ¼è¶¨å‹¢ã€‚\n\né›»å•†å¹³å°åˆ©ç”¨æ©Ÿå™¨å­¸ç¿’æ¨è–¦å•†å“çµ¦ç”¨æˆ¶ã€‚\n\næ©Ÿå™¨å­¸ç¿’æŠ€è¡“å¹«åŠ©éŠ€è¡Œè­˜åˆ¥ä¿¡ç”¨å¡è©é¨™ã€‚'
            },
            'æ·±åº¦å­¸ç¿’': {
                'definition': 'åŸºæ–¼äººå·¥ç¥ç¶“ç¶²è·¯çš„æ©Ÿå™¨å­¸ç¿’æ–¹æ³•ï¼Œèƒ½å¤ è™•ç†è¤‡é›œçš„æ¨¡å¼è­˜åˆ¥ä»»å‹™ã€‚',
                'examples': 'æ·±åº¦å­¸ç¿’åœ¨åœ–åƒè­˜åˆ¥é ˜åŸŸå–å¾—çªç ´æ€§é€²å±•ã€‚\n\nèªéŸ³è­˜åˆ¥ç³»çµ±å¤§å¤šæ¡ç”¨æ·±åº¦å­¸ç¿’æŠ€è¡“ã€‚\n\næ·±åº¦å­¸ç¿’æ¨¡å‹èƒ½å¤ ç”Ÿæˆé€¼çœŸçš„äººå·¥åœ–åƒã€‚'
            },
            'ç¥ç¶“ç¶²è·¯': {
                'definition': 'æ¨¡ä»¿ç”Ÿç‰©ç¥ç¶“å…ƒçµæ§‹çš„è¨ˆç®—æ¨¡å‹ï¼Œæ˜¯æ·±åº¦å­¸ç¿’çš„åŸºç¤æ¶æ§‹ã€‚',
                'examples': 'å·ç©ç¥ç¶“ç¶²è·¯å°ˆé–€ç”¨æ–¼è™•ç†åœ–åƒæ•¸æ“šã€‚\n\nå¾ªç’°ç¥ç¶“ç¶²è·¯é©åˆè™•ç†åºåˆ—æ•¸æ“šã€‚\n\nç¥ç¶“ç¶²è·¯éœ€è¦å¤§é‡æ•¸æ“šä¾†è¨“ç·´æ¨¡å‹ã€‚'
            },
            'å€å¡Šéˆ': {
                'definition': 'ä¸€ç¨®åˆ†æ•£å¼æ•¸æ“šåº«æŠ€è¡“ï¼Œé€šéå¯†ç¢¼å­¸ç¢ºä¿æ•¸æ“šçš„å®‰å…¨æ€§å’Œä¸å¯ç¯¡æ”¹æ€§ã€‚',
                'examples': 'æ¯”ç‰¹å¹£æ˜¯å€å¡ŠéˆæŠ€è¡“çš„ç¬¬ä¸€å€‹æ‡‰ç”¨ã€‚\n\nä¾›æ‡‰éˆç®¡ç†å¯ä»¥åˆ©ç”¨å€å¡Šéˆè¿½è¹¤å•†å“ä¾†æºã€‚\n\nå€å¡ŠéˆæŠ€è¡“èƒ½å¤ æé«˜é‡‘èäº¤æ˜“çš„é€æ˜åº¦ã€‚'
            },
            'é‡å­è¨ˆç®—': {
                'definition': 'åˆ©ç”¨é‡å­åŠ›å­¸åŸç†é€²è¡Œè¨ˆç®—çš„æ–°å‹è¨ˆç®—æ–¹å¼ï¼Œå…·æœ‰å¼·å¤§çš„ä¸¦è¡Œè™•ç†èƒ½åŠ›ã€‚',
                'examples': 'é‡å­è¨ˆç®—æ©Ÿèƒ½å¤ å¿«é€Ÿç ´è§£å‚³çµ±åŠ å¯†ç®—æ³•ã€‚\n\nè£½è—¥å…¬å¸ä½¿ç”¨é‡å­è¨ˆç®—ä¾†æ¨¡æ“¬åˆ†å­çµæ§‹ã€‚\n\né‡å­è¨ˆç®—åœ¨å„ªåŒ–å•é¡Œä¸Šå…·æœ‰å·¨å¤§å„ªå‹¢ã€‚'
            }
        }
        
        terms_list = []
        for word in words:
            if word in explanations:
                exp = explanations[word]
                term_data = {
                    "term": word,
                    "definition": f"ï¼ˆç¤ºä¾‹ï¼‰{exp['definition']}",
                    "examples": [
                        {
                            "title": "æ‡‰ç”¨ä¾‹å­",
                            "text": exp['examples']
                        }
                    ]
                }
                terms_list.append(term_data)
        
        return {"terms": terms_list}

def demo_process_json(json_file: str):
    """æ¼”ç¤ºè™•ç†JSONæª”æ¡ˆçš„æµç¨‹"""
    print(f"ğŸš€ é–‹å§‹æ¼”ç¤ºè™•ç†ï¼š{json_file}")
    print("=" * 50)
    
    # æ­¥é©Ÿ1ï¼šè®€å–JSON
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print("âœ… JSONæª”æ¡ˆè®€å–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è®€å–å¤±æ•—ï¼š{e}")
        return
    
    # æ­¥é©Ÿ2ï¼šå‰µå»ºåˆ†æå™¨
    analyzer = DemoTextAnalyzer()
    
    # æ­¥é©Ÿ3ï¼šæå–æ–‡å­—
    print("\nğŸ” æ­£åœ¨æå–æ–‡å­—å…§å®¹...")
    texts = analyzer.extract_text_from_json(data)
    print(f"âœ… å·²æå– {len(texts)} æ®µæ–‡å­—å…§å®¹")
    
    print("\nğŸ“ æå–çš„æ–‡å­—å…§å®¹æ¨£æœ¬ï¼š")
    for i, text in enumerate(texts[:5], 1):
        print(f"  {i}. {text[:60]}{'...' if len(text) > 60 else ''}")
    
    # æ­¥é©Ÿ4ï¼šè­˜åˆ¥å›°é›£è©å½™
    print("\nğŸ§  æ­£åœ¨è­˜åˆ¥å›°é›£è©å½™...")
    difficult_words = analyzer.identify_difficult_words_demo(texts)
    print(f"âœ… è­˜åˆ¥å‡º {len(difficult_words)} å€‹å›°é›£è©å½™")
    
    print("\nğŸ”¤ å›°é›£è©å½™åˆ—è¡¨ï¼š")
    for i, word in enumerate(difficult_words, 1):
        print(f"  {i}. {word}")
    
    # æ­¥é©Ÿ5ï¼šç”Ÿæˆè§£é‡‹
    print("\nğŸ“– æ­£åœ¨ç”Ÿæˆè©å½™è§£é‡‹...")
    explanations = analyzer.generate_demo_explanations(difficult_words)
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(explanations['terms'])} å€‹è©å½™è§£é‡‹")
    
    # æ­¥é©Ÿ6ï¼šé¡¯ç¤ºçµæœ
    print("\n" + "=" * 50)
    print("ğŸ“Š è™•ç†å®Œæˆï¼è©å½™è§£é‡‹çµæœï¼š")
    print("=" * 50)
    
    for term in explanations['terms']:
        print(f"\nğŸ“š ã€{term['term']}ã€‘")
        print(f"å®šç¾©ï¼š{term['definition']}")
        print("ç¯„ä¾‹ï¼š")
        examples = term['examples'][0]['text'].split('\n\n')
        for i, example in enumerate(examples, 1):
            if example.strip():
                print(f"  {i}. {example.strip()}")
    
    # æ­¥é©Ÿ7ï¼šå„²å­˜çµæœ
    output_file = "demo_result.json"
    final_result = {
        "source_file": json_file,
        "extracted_texts_count": len(texts),
        "difficult_words_count": len(difficult_words),
        "difficult_words": difficult_words,
        "explanations": explanations
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ çµæœå·²å„²å­˜è‡³ï¼š{output_file}")
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ“– æ™ºèƒ½æ–‡æœ¬è™•ç†æ¼”ç¤ºç¨‹å¼")
    print("æ­¤ç‰ˆæœ¬ä¸éœ€è¦APIé‡‘é‘°ï¼Œä½¿ç”¨é è¨­çš„è©å½™è­˜åˆ¥å’Œè§£é‡‹")
    print("=" * 60)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç¯„ä¾‹æª”æ¡ˆ
    sample_file = "sample_tech_news.json"
    if not os.path.exists(sample_file):
        print("âš ï¸ æœªæ‰¾åˆ°ç¯„ä¾‹æª”æ¡ˆï¼Œæ­£åœ¨å‰µå»º...")
        # é‡æ–°å‰µå»ºç¯„ä¾‹æª”æ¡ˆ
        sample_data = {
            "title": "äººå·¥æ™ºæ…§èˆ‡æ©Ÿå™¨å­¸ç¿’çš„æ‡‰ç”¨",
            "content": "æ·±åº¦å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹é‡è¦åˆ†æ”¯ï¼Œå®ƒåˆ©ç”¨ç¥ç¶“ç¶²è·¯ä¾†æ¨¡æ“¬äººè…¦çš„å·¥ä½œæ–¹å¼ã€‚åœ¨é†«ç™‚è¨ºæ–·é ˜åŸŸï¼Œå·ç©ç¥ç¶“ç¶²è·¯èƒ½å¤ å”åŠ©é†«å¸«é€²è¡Œå½±åƒåˆ†æã€‚è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“å‰‡å¯ä»¥å¹«åŠ©é›»è…¦ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ã€‚",
            "articles": [
                {
                    "title": "å€å¡ŠéˆæŠ€è¡“çš„ç™¼å±•è¶¨å‹¢",
                    "summary": "å€å¡Šéˆæ˜¯ä¸€ç¨®åˆ†æ•£å¼å¸³æœ¬æŠ€è¡“ï¼Œå…·æœ‰å»ä¸­å¿ƒåŒ–ã€ä¸å¯ç¯¡æ”¹çš„ç‰¹æ€§ã€‚æ™ºèƒ½åˆç´„æ˜¯å€å¡Šéˆä¸Šå¯è‡ªå‹•åŸ·è¡Œçš„ç¨‹å¼ç¢¼ã€‚"
                },
                {
                    "title": "é‡å­è¨ˆç®—çš„æœªä¾†å±•æœ›", 
                    "summary": "é‡å­è¨ˆç®—åˆ©ç”¨é‡å­åŠ›å­¸çš„ç‰¹æ€§ä¾†è™•ç†è³‡è¨Šï¼Œé‡å­ç³¾çºå’Œé‡å­ç–ŠåŠ æ˜¯é‡å­è¨ˆç®—çš„æ ¸å¿ƒæ¦‚å¿µã€‚"
                }
            ]
        }
        
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²å‰µå»ºç¯„ä¾‹æª”æ¡ˆï¼š{sample_file}")
    
    # åŸ·è¡Œæ¼”ç¤º
    demo_process_json(sample_file)

if __name__ == "__main__":
    import os
    main()
