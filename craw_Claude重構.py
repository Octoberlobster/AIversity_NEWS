import feedparser
import time
import datetime
import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urlparse, parse_qs
import schedule
from supabase import create_client, Client
import uuid
import os
import logging
from typing import Dict, List, Optional, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç¦ç”¨ SSL è­¦å‘Š
requests.packages.urllib3.disable_warnings()

@dataclass
class NewsItem:
    """æ–°èé …ç›®è³‡æ–™é¡åˆ¥"""
    source: str
    title: str
    url: str
    date: str
    content: str = ""
    image: str = ""
    sourcecle_id: str = ""

class DatabaseManager:
    """è³‡æ–™åº«ç®¡ç†é¡åˆ¥"""
    
    def __init__(self):
        self.supabase_url = os.getenv("API_KEY_URL")
        self.supabase_key = os.getenv("API_KEY_supa")
        self.client: Client = create_client(self.supabase_url, self.supabase_key)
        
        # æ¬„ä½å°æ‡‰è¡¨
        self.key_map = {
            "title": "title", "Title": "title",
            "url": "url", "URL": "url",
            "date": "date", "Date": "date",
            "content": "content", "translatedcontent": "content", "Content": "content",
            "source": "sourcecle_media", "Source": "sourcecle_media",
            "image": "image", "Image": "image",
        }
    
    def normalize_key(self, key: str) -> str:
        """æ¨™æº–åŒ–éµå"""
        return self.key_map.get(key.lower(), key.lower())
    
    def transform_item(self, item: Dict) -> Dict:
        """è½‰æ›å–®å€‹é …ç›®"""
        if isinstance(item, dict):
            transformed = {self.normalize_key(k): v for k, v in item.items()}
            transformed["sourcecle_id"] = self._generate_uuid(item.get("sourcecle_id", ""))
            return transformed
        return item
    
    def _generate_uuid(self, val: str) -> str:
        """ç”Ÿæˆæˆ–é©—è­‰ UUID"""
        try:
            return str(uuid.UUID(val))
        except:
            return str(uuid.uuid4())
    
    def save_to_database(self, news_data: List[NewsItem]) -> None:
        """å„²å­˜è³‡æ–™åˆ°è³‡æ–™åº«"""
        try:
            converted_data = [self.transform_item(item.__dict__) for item in news_data[:400]]
            
            for item in converted_data:
                self.client.table("cleaned_news").upsert(
                    item, on_conflict="title"
                ).execute()
            
            logger.info(f"âœ… è³‡æ–™è™•ç†å®Œæˆï¼Œå…±è™•ç† {len(converted_data)} ç­†")
        except Exception as e:
            logger.error(f"âŒ åŒ¯å…¥å¤±æ•—ï¼š{e}")

class ContentExtractor(ABC):
    """å…§å®¹æå–å™¨æŠ½è±¡åŸºé¡"""
    
    @abstractmethod
    def extract_content(self, source_data: str) -> str:
        """æå–æ–°èå…§å®¹"""
        pass
    
    @abstractmethod
    def extract_image(self, news_url: str) -> str:
        """æå–æ–°èåœ–ç‰‡"""
        pass

class UDNContentExtractor(ContentExtractor):
    """UDN å…§å®¹æå–å™¨"""
    
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
            'Connection': 'close',
        }
        self.excluded_keywords = [
            "å»¶ä¼¸é–±è®€", "è²¬ä»»ç·¨è¼¯", "æ ¸ç¨¿ç·¨è¼¯", "â–ª", "ä¸ç”¨æŠ½", "ã€", 
            "ä¸€æ‰‹æŒæ¡ç¶“æ¿Ÿè„ˆå‹•", "æœ¬ç¶²ç«™ä¹‹æ–‡å­—", "APP", "é»æˆ‘ä¸‹è¼‰APP", 
            "æ´»å‹•è¾¦æ³•", "è«‹ç¹¼çºŒå¾€ä¸‹é–±è®€", "åœ–ï¼", "è³‡æ–™ç…§ï¼", "ä¸­å¤®ç¤¾è¨˜è€…",
            "è¨˜è€…å®‹å¥ç”Ÿ/æ”å½±", "ï¼æ”å½±", "ï¼ç¿»æ”", "ï¼å°å—å ±å°", "ï¼å°ä¸­å ±å°",
            "ï¼ç¶œåˆå ±å°", "ï¼æ–°åŒ—å ±å°", "ï¼é«˜é›„å ±å°", "ï¼å½°åŒ–å ±å°", "â–²",
            "åœ–æ–‡ï¼CTWANT", "ï¼ç·¨è­¯", "ï¼æ¡ƒåœ’å ±å°", "ï¼å°æ±å ±å°", "ï¼å°åŒ—å ±å°",
            "ç¤ºæ„åœ–", "ï¼ˆè·¯é€ï¼‰", "ï¼ˆä¸­äºŒè§£é¡æä¾›ï¼‰", "åœ–å–è‡ª",
            "è³‡æ–™ç…§ï¼‰", "ç¿»æ”ï¼‰", "ï¼ˆç¾è¯ç¤¾ï¼‰", "ï¼ˆæœ¬å ±è³‡æ–™ç…§",
            "æ”ï¼‰", "æä¾›ï¼‰", "ï¼ˆå½­åšï¼‰", "ï¼ˆæ³•æ–°ç¤¾ï¼‰", "ï¼ˆåœ–æ“·è‡ª",
            "ï¼æ¾æ¹–å ±å°", "ï¼å®œè˜­å ±å°", "ï¼èŠ±è“®å ±å°", "ï¼å±æ±å ±å°",
            "ï¼å—æŠ•å ±å°", "ï¼åŸºéš†å ±å°", "ï¼è‹—æ —å ±å°", "ï¼æ–°ç«¹å ±å°",
            "ï¼é›²æ—å ±å°", "ï¼å˜‰ç¾©å ±å°", "é¦–æ¬¡ä¸Šç¨¿", "ï¼ˆæ³•æ–°ç¤¾è³‡æ–™ç…§ï¼‰"
        ]
    
    def extract_content(self, news_url: str) -> str:
        """æå– UDN æ–°èå…§å®¹"""
        try:
            response = requests.get(news_url, headers=self.headers, timeout=10)
            response.encoding = "utf-8"
            
            logger.info(f"è¨ªå•æ–°è: {news_url}ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
            
            if response.status_code != 200:
                logger.warning("âŒ ç„¡æ³•è¨ªå•æ–°èå…§æ–‡")
                return ""
            
            return self._parse_content(response.text)
            
        except Exception as e:
            logger.error(f"âŒ æå–å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return ""
    
    def _parse_content(self, html_text: str) -> str:
        """è§£æ HTML å…§å®¹"""
        content_selectors = [
            {'tag': 'div', 'attrs': {'data-desc': 'å…§æ–‡'}},
            {'tag': 'div', 'attrs': {'data-desc': 'å…§å®¹é '}},
            {'tag': 'div', 'class_': 'text boxTitle boxText'},
            {'tag': 'div', 'class_': 'paragraph'},
            {'tag': 'section', 'class_': 'article-content__editor'},
            {'tag': 'div', 'class_': 'story'}
        ]
        
        content_sections = self._find_content_sections(html_text, content_selectors)
        
        if not content_sections:
            # å˜—è©¦å¾ç‰¹å®šä½ç½®é–‹å§‹è§£æ
            content_sections = self._find_content_sections(html_text[50000:], content_selectors)
        
        return self._extract_text_from_sections(content_sections)
    
    def _find_content_sections(self, html_text: str, selectors: List[Dict]) -> List:
        """å°‹æ‰¾å…§å®¹å€å¡Š"""
        try:
            soup = BeautifulSoup(html_text, 'html.parser')
            for selector in selectors:
                if 'attrs' in selector:
                    sections = soup.find_all(selector['tag'], attrs=selector['attrs'])
                else:
                    sections = soup.find_all(selector['tag'], class_=selector['class_'])
                if sections:
                    return sections
        except Exception as e:
            logger.error(f"âŒ è§£æ HTML æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []
    
    def _extract_text_from_sections(self, content_sections: List) -> str:
        """å¾å…§å®¹å€å¡Šæå–æ–‡å­—"""
        content = []
        
        for section in content_sections:
            paragraphs = section.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºçµæŸé—œéµå­—
                if any(kw in text for kw in self.excluded_keywords[:7]):
                    break
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºæ’é™¤å…§å®¹
                if self._should_exclude_paragraph(p, text):
                    continue
                
                content.append(text)
        
        return " ".join(content)
    
    def _should_exclude_paragraph(self, paragraph, text: str) -> bool:
        """åˆ¤æ–·æ®µè½æ˜¯å¦æ‡‰è©²æ’é™¤"""
        if paragraph.find('iframe'):
            return True
        
        return any(kw in text for kw in self.excluded_keywords[6:])
    
    def extract_image(self, news_url: str) -> str:
        """æå– UDN æ–°èåœ–ç‰‡"""
        return self._extract_image_from_url(news_url)
    
    def _extract_image_from_url(self, news_url: str) -> str:
        """å¾ URL æå–åœ–ç‰‡"""
        try:
            response = requests.get(news_url, headers=self.headers, timeout=10)
            response.encoding = "utf-8"
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text[30000:], 'html.parser')
                return self._find_main_image(soup)
                
        except Exception as e:
            logger.error(f"âŒ æå–åœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return ""
    
    def _find_main_image(self, soup) -> str:
        """å°‹æ‰¾ä¸»è¦åœ–ç‰‡"""
        # å°‹æ‰¾ç‰¹å®šçš„åœ–ç‰‡å®¹å™¨
        img_containers = [
            soup.find("div", class_="image-popup-vertical-fit"),
            soup.find("div", class_="fullPic")
        ]
        
        for container in img_containers:
            if container:
                img_url = self._extract_image_from_container(container)
                if img_url:
                    return img_url
        
        # åœ¨æ–‡ç« å…§å®¹ä¸­å°‹æ‰¾åœ–ç‰‡
        for p in soup.select('div.text.boxTitle.boxText p'):
            img = p.find('img')
            if img:
                img_url = img.get('data-src') or img.get('src')
                if img_url:
                    return img_url
        
        return ""
    
    def _extract_image_from_container(self, container) -> str:
        """å¾å®¹å™¨ä¸­æå–åœ–ç‰‡ URL"""
        if container.has_attr('href'):
            return container["href"]
        
        img_element = container.find('img')
        if img_element:
            return img_element.get('data-src') or img_element.get('src', '')
        
        return ""

class NewTalkContentExtractor(ContentExtractor):
    """NewTalk å…§å®¹æå–å™¨"""
    
    def extract_content(self, content_str: str) -> str:
        """æå– NewTalk æ–°èå…§å®¹"""
        content = []
        soup = BeautifulSoup(content_str, 'html.parser')
        
        if soup:
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                
                if any(kw in text for kw in ["å»¶ä¼¸é–±è®€", "è²¬ä»»ç·¨è¼¯", "æ ¸ç¨¿ç·¨è¼¯", "â–ª", "æœ¬ç¶²ç«™ä¹‹æ–‡å­—"]):
                    break
                elif 'ä¸­å¤®ç¤¾è¨˜è€…' in text or 'åœ–ç‚º' in text:
                    continue
                
                content.append(text)
        
        return " ".join(content)
    
    def extract_image(self, news_url: str) -> str:
        """æå– NewTalk æ–°èåœ–ç‰‡"""
        return UDNContentExtractor()._extract_image_from_url(news_url)

class NewsSource:
    """æ–°èä¾†æºé…ç½®"""
    
    SOURCES = {
        "UDN": "https://udn.com/news/rssfeed/",
        "ETtoday": "https://feeds.feedburner.com/ettoday/realtime",
        "NewTalk": "https://newtalk.tw/rss/all/",
        "CNA": [
            "https://feeds.feedburner.com/rsscna/politics",
            "https://feeds.feedburner.com/rsscna/intworld",
            "https://feeds.feedburner.com/rsscna/mainland",
            "https://feeds.feedburner.com/rsscna/finance",
            "https://feeds.feedburner.com/rsscna/technology",
            "https://feeds.feedburner.com/rsscna/lifehealth",
            "https://feeds.feedburner.com/rsscna/social",
            "https://feeds.feedburner.com/rsscna/local",
            "https://feeds.feedburner.com/rsscna/culture",
            "https://feeds.feedburner.com/rsscna/sport",
            "https://feeds.feedburner.com/rsscna/stars",
        ],
        "LTN": "https://news.ltn.com.tw/rss/all.xml",
    }

class NewsCrawler:
    """æ–°èçˆ¬èŸ²ä¸»é¡åˆ¥"""
    
    def __init__(self):
        self.db_manager = DatabaseManager()
        self.extractors = {
            "UDN": UDNContentExtractor(),
            "ETtoday": UDNContentExtractor(),
            "LTN": UDNContentExtractor(),
            "CNA": UDNContentExtractor(),
            "NewTalk": NewTalkContentExtractor(),
        }
    
    def crawl_news(self) -> None:
        """çˆ¬å–æ–°èä¸»å‡½æ•¸"""
        logger.info("é–‹å§‹çˆ¬å–æ–°è...")
        
        news_data = []
        time_filter = self._get_time_filter()
        
        for source, urls in NewsSource.SOURCES.items():
            if isinstance(urls, str):
                urls = [urls]
            
            for url in urls:
                news_items = self._process_rss_feed(source, url, time_filter)
                news_data.extend(news_items)
        
        # å„²å­˜åˆ°æª”æ¡ˆ
        self._save_to_json(news_data)
        
        # å„²å­˜åˆ°è³‡æ–™åº«
        self.db_manager.save_to_database(news_data)
        
        logger.info(f"çˆ¬å–å®Œæˆï¼Œå…±è™•ç† {len(news_data)} å‰‡æ–°è")
    
    def _get_time_filter(self) -> datetime.datetime:
        """å–å¾—æ™‚é–“éæ¿¾å™¨ï¼ˆä¸€å°æ™‚å‰ï¼‰"""
        current_time = datetime.datetime.utcnow() + datetime.timedelta(hours=8)
        return current_time - datetime.timedelta(hours=1)
    
    def _process_rss_feed(self, source: str, url: str, time_filter: datetime.datetime) -> List[NewsItem]:
        """è™•ç† RSS æº"""
        news_items = []
        
        try:
            feed = feedparser.parse(url)
            
            for entry in feed.entries:
                if "ç¾å ´ç›´æ’­ä¸­" in entry.get("title", ""):
                    logger.info(f"è·³éç¾å ´ç›´æ’­æ–°è: {entry.title}")
                    continue
                
                # è§£æç™¼å¸ƒæ™‚é–“
                publish_time = self._parse_publish_time(entry, source)
                if not publish_time or publish_time < time_filter:
                    continue
                
                # å»ºç«‹æ–°èé …ç›®
                news_item = self._create_news_item(source, entry, publish_time)
                if not news_item.content:
                    logger.warning(f"âš ï¸ ç„¡æ³•æå–å…§å®¹: {news_item.title}")
                    continue
                news_items.append(news_item)
                
                logger.info(f"è™•ç†æ–°è: {news_item.title}")
                
        except Exception as e:
            logger.error(f"è™•ç† RSS æº {url} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return news_items
    
    def _parse_publish_time(self, entry, source: str) -> Optional[datetime.datetime]:
        """è§£æç™¼å¸ƒæ™‚é–“"""
        dt = None
        
        if entry.get("published_parsed"):
            t = entry.published_parsed
            dt = datetime.datetime(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec)
        elif entry.get("published"):
            try:
                date_str = entry.published.replace(',', ', ')
                t_struct = time.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
                dt = datetime.datetime.fromtimestamp(time.mktime(t_struct))
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•è§£æ published å­—ä¸²: {entry.published}ï¼ŒéŒ¯èª¤: {e}")
        
        # æ™‚å€è£œæ­£
        if dt and source in ["CNA", "UDN", "NewTalk", "LTN"]:
            dt = dt + datetime.timedelta(hours=8)
            logger.info(f"ç™¼å¸ƒæ™‚é–“ï¼š{dt.strftime('%Y/%m/%d %H:%M')}")
        
        return dt
    
    def _create_news_item(self, source: str, entry, publish_time: datetime.datetime) -> NewsItem:
        """å»ºç«‹æ–°èé …ç›®"""
        # åŸºæœ¬è³‡è¨Š
        news_item = NewsItem(
            source=source,
            title=entry.title,
            url=entry.link,
            date=entry.get("pubDate", publish_time.strftime("%Y/%m/%d %H:%M"))
        )
        
        # æå–åœ–ç‰‡
        news_item.image = self._extract_image(entry, source)
        
        # æå–å…§å®¹
        news_item.content = self._extract_content(source, entry)
        
        return news_item
    
    def _extract_image(self, entry, source: str) -> str:
        """æå–åœ–ç‰‡ URL"""
        image_url = None
        
        # å¾ RSS ä¸­æå–åœ–ç‰‡
        if "enclosures" in entry and entry.enclosures:
            image_url = entry.enclosures[0].href
        elif "media_content" in entry:
            image_url = entry.media_content[0]["url"][5:]
        elif "description" in entry:
            soup = BeautifulSoup(entry.description, "html.parser")
            img_tag = soup.find("img")
            if img_tag:
                parsed_url = urlparse(img_tag["src"])
                query_params = parse_qs(parsed_url.query)
                image_url = query_params.get("u", [""])[0]
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œå˜—è©¦å¾æ–°èé é¢æå–
        if not image_url and source in self.extractors:
            image_url = self.extractors[source].extract_image(entry.link)
        
        return image_url or ""
    
    def _extract_content(self, source: str, entry) -> str:
        """æå–æ–°èå…§å®¹"""
        if source not in self.extractors:
            return ""
        
        extractor = self.extractors[source]
        
        if source == "NewTalk":
            return extractor.extract_content(entry.description)
        else:
            return extractor.extract_content(entry.link)
    
    def _save_to_json(self, news_data: List[NewsItem]) -> None:
        """å„²å­˜åˆ° JSON æª”æ¡ˆ"""
        timestamp = time.strftime("%Y_%m_%d_%H", time.localtime())
        json_path = f"json/Cle/{timestamp}.json"
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs("json", exist_ok=True)
        
        # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
        data_dict = [item.__dict__ for item in news_data]
        
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data_dict, f, ensure_ascii=False, indent=4)
        
        logger.info(f"æ–°èè³‡æ–™å·²å„²å­˜è‡³ {json_path}ï¼Œå…± {len(news_data)} å‰‡æ–°è")

def main():
    """ä¸»å‡½æ•¸"""
    crawler = NewsCrawler()
    
    # ç«‹å³åŸ·è¡Œä¸€æ¬¡
    logging.info("ğŸŸ¢ å•Ÿå‹•çˆ¬èŸ²ï¼Œç«‹å³åŸ·è¡Œä¸€æ¬¡")
    crawler.crawl_news()
    
    # è¨­å®šæ’ç¨‹ï¼ˆæ¯ 59 åˆ†é˜åŸ·è¡Œä¸€æ¬¡ï¼‰
    schedule.every(59).minutes.do(lambda: logging.info("ğŸ” æ’ç¨‹è§¸ç™¼") or crawler.crawl_news())
    
    # æŒçºŒé‹è¡Œæ’ç¨‹
    while True:
        schedule.run_pending()
        time.sleep(5)

if __name__ == "__main__":
    main()
