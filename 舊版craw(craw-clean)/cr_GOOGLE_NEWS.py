from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
import time
import datetime as dt
from datetime import datetime
import requests
from supabase import create_client, Client
import uuid
import os
import json
import random
import re
from urllib.parse import urljoin, urlparse
from collections import defaultdict

# === Supabase è¨­å®š ===
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
if SUPABASE_URL and SUPABASE_KEY:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Chrome options è¨­å®š
chrome_options = Options()
# chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36")

def get_main_story_links(main_url, category):
    """æ­¥é©Ÿ 1: å¾ä¸»é æŠ“å–æ‰€æœ‰ä¸»è¦æ•…äº‹é€£çµ"""
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    story_links = []
    
    try:
        print(f"ğŸ” æ­£åœ¨æŠ“å– {category} é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµ...")
        driver.get(main_url)
        
        # ç­‰å¾…é é¢è¼‰å…¥ - æ‰¾åˆ°æ‰€æœ‰ c-wiz å€å¡Š
        wait = WebDriverWait(driver, 15)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'c-wiz[jsrenderer="jeGyVb"]')))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        c_wiz_blocks = soup.find_all("c-wiz", {"jsrenderer": "jeGyVb"})
        
        print(f"âœ… æ‰¾åˆ° {len(c_wiz_blocks)} å€‹ c-wiz å€å¡Š")
        
        # å¾æ¯å€‹ c-wiz å€å¡Šä¸­æ‰¾åˆ°ä¸»è¦æ•…äº‹é€£çµ
        for i, block in enumerate(c_wiz_blocks, start=1):
            story_link = block.find("a", class_="jKHa4e")  # ä¸»è¦æ•…äº‹é€£çµ
            
            if story_link:
                href = story_link.get("href")
                title = story_link.text.strip()
                print(f"   è™•ç†æ•…äº‹ {i}: {href}")
                
                if href:
                    # è™•ç†ç›¸å°é€£çµ
                    if href.startswith("./"):
                        full_link = "https://news.google.com" + href[1:]
                    else:
                        full_link = "https://news.google.com" + href
                    
                    # ç‚ºæ¯å€‹æ•…äº‹ç”Ÿæˆ UUID
                    story_id = str(uuid.uuid4())
                    
                    story_links.append({
                        "index": i,
                        "story_id": story_id,  # ä½¿ç”¨ UUID
                        "title": title,
                        "url": full_link,
                        "category": category  # æ–°å¢ï¼šå°‡åˆ†é¡è³‡è¨ŠåŠ å…¥
                    })
                    
                    print(f"{i}. ğŸ“° [{category}] {title}")
                    print(f"   ğŸ†” æ•…äº‹ID: {story_id}")
                    print(f"   ğŸ”— {full_link}")
        
        print(f"\nğŸ“Š ç¸½å…±æ”¶é›†åˆ° {len(story_links)} å€‹ {category} é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµ")
        
    except Exception as e:
        print(f"âŒ æŠ“å–ä¸»è¦æ•…äº‹é€£çµæ™‚å‡ºéŒ¯: {e}")
    finally:
        driver.quit()
    
    return story_links

def get_article_links_from_story(story_info):
    """æ­¥é©Ÿ 2: é€²å…¥æ¯å€‹æ•…äº‹é é¢ï¼Œæ‰¾å‡ºæ‰€æœ‰ article ä¸‹çš„æ–‡ç« é€£çµå’Œç›¸é—œä¿¡æ¯"""
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    article_links = []
    
    try:
        print(f"\nğŸ” æ­£åœ¨è™•ç†æ•…äº‹ {story_info['index']}: [{story_info['category']}] {story_info['title']}")
        print(f"   ğŸ†” æ•…äº‹ID: {story_info['story_id']}")
        driver.get(story_info['url'])
        time.sleep(random.randint(3, 6))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # æ‰¾åˆ°æ‰€æœ‰ article tag class="MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc"
        article_elements = soup.find_all("article", class_="MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc")
        
        print(f"   âœ… æ‰¾åˆ° {len(article_elements)} å€‹ article å…ƒç´ ")
        
        for j, article in enumerate(article_elements, start=1):
            if j > 3:
                break  # æœ€å¤šåªæŠ“å– 5 ç¯‡æ–‡ç«     
            
            # åœ¨ article ä¸‹é¢æ‰¾ h4 tag class="ipQwMb ekueJc RD0gLb"
            h4_element = article.find("h4", class_="ipQwMb ekueJc RD0gLb")
            
            if h4_element:
                # åœ¨ h4 ä¸‹é¢æ‰¾ a tag class="DY5T1d RZIKme"
                link = h4_element.find("a", class_="DY5T1d RZIKme")
                
                if link:
                    href = link.get("href")
                    link_text = link.text.strip()
                    
                    # æ‰¾åª’é«”ä¾†æº a tag class="wEwyrc"
                    media_element = article.find("a", class_="wEwyrc")
                    media_source = media_element.text.strip() if media_element else "æœªçŸ¥ä¾†æº"
                    
                    # æ‰¾æ™‚é–“ class="WW6dff uQIVzc Sksgp slhocf"
                    time_element = article.find(class_="WW6dff uQIVzc Sksgp slhocf")
                    article_datetime = time_element.get("datetime") if time_element and time_element.get("datetime") else "æœªçŸ¥æ™‚é–“"
                    
                    if href:
                        # è™•ç†ç›¸å°é€£çµ
                        if href.startswith("./"):
                            full_href = "https://news.google.com" + href[1:]
                        else:
                            full_href = "https://news.google.com" + href
                        
                        article_links.append({
                            "story_id": story_info['story_id'],  # ä½¿ç”¨ story_id è€Œé story_index
                            "story_title": story_info['title'],
                            "story_category": story_info['category'],  # æ•…äº‹åˆ†é¡
                            "story_url": story_info['url'],
                            "article_index": j,
                            "article_title": link_text,
                            "article_url": full_href,
                            "media_source": media_source,  # åª’é«”ä¾†æº
                        })
                        
                        print(f"     {j}. ğŸ“„ {link_text}")
                        print(f"        ğŸ¢ åª’é«”: {media_source}")
                        print(f"        ğŸ”— {full_href}")
                else:
                    print(f"     {j}. âŒ h4 å…ƒç´ ä¸­æ²’æœ‰æ‰¾åˆ°å°æ‡‰çš„ a tag")
            else:
                print(f"     {j}. âŒ article å…ƒç´ ä¸­æ²’æœ‰æ‰¾åˆ° h4 tag")
        
    except Exception as e:
        print(f"âŒ è™•ç†æ•…äº‹æ™‚å‡ºéŒ¯: {e}")
    finally:
        driver.quit()
    
    return article_links

def get_final_content(article_info, driver):
    """æ­¥é©Ÿ 3: è·³è½‰åˆ°åŸå§‹ç¶²ç«™ä¸¦æŠ“å–å…§å®¹ã€åœ–ç‰‡å’Œæ™‚é–“"""
    try:
        driver.get(article_info['article_url'])
        time.sleep(random.randint(3, 6))
        
        # å–å¾—è·³è½‰å¾Œçš„çœŸå¯¦ç¶²å€
        final_url = driver.current_url
        print(f"   æœ€çµ‚ç¶²å€: {final_url}")
        
        # å–å¾— HTML åŸå§‹ç¢¼ä¸¦äº¤çµ¦ BeautifulSoup
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        # æ¸…ç†å…§å®¹
        content_to_clean = None

        # ç¬¬ä¸€å„ªå…ˆï¼šå°‹æ‰¾ article æ¨™ç±¤
        article_tag = soup.find('article')
        if article_tag:
            content_to_clean = str(article_tag)
        else:
            # ç¬¬äºŒå„ªå…ˆï¼šå°‹æ‰¾ç‰¹å®š id çš„ div æ¨™ç±¤
            target_ids = [
                'content-box', 'text', 'boxTitle', 'news-detail-content', 
                'story', 'article-content__editor', 'article-body', 
                'artical-content', 'article_text'
            ]
            
            div_by_id = None
            for target_id in target_ids:
                div_by_id = soup.find('div', id=target_id)
                if div_by_id:
                    break
            
            if div_by_id:
                content_to_clean = str(div_by_id)
            else:
                # ç¬¬ä¸‰å„ªå…ˆï¼šå°‹æ‰¾ç‰¹å®š class çš„ div æ¨™ç±¤
                target_classes = ['paragraph', 'atoms']
                
                div_by_class = None
                for target_class in target_classes:
                    div_by_class = soup.find('div', class_=target_class)
                    if div_by_class:
                        break
                
                if div_by_class:
                    content_to_clean = str(div_by_class)
                else:
                    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ body
                    if soup.body:
                        content_to_clean = str(soup.body)

        # å¦‚æœæœ‰æ‰¾åˆ°å…§å®¹ï¼Œé€²è¡Œæ¸…ç†
        if content_to_clean:
            # é‡æ–°è§£ææ‰¾åˆ°çš„å…§å®¹
            content_soup = BeautifulSoup(content_to_clean, "html.parser")
            
            # æ’é™¤ç‰¹å®šçš„ div æ¨™ç±¤
            excluded_divs = content_soup.find_all('div', class_='paragraph moreArticle')
            for div in excluded_divs:
                div.decompose()
            
            # æ’é™¤ç‰¹å®šçš„ p æ¨™ç±¤
            excluded_p_classes = [
                'mb-module-gap read-more-vendor break-words leading-[1.4] text-px20 lg:text-px18 lg:leading-[1.8] text-batcave __web-inspector-hide-shortcut__',
                'mb-module-gap read-more-editor break-words leading-[1.4] text-px20 lg:text-px18 lg:leading-[1.8] text-batcave'
            ]
            
            for p_class in excluded_p_classes:
                excluded_ps = content_soup.find_all('p', class_=p_class)
                for p in excluded_ps:
                    p.decompose()
            
            # æœ€çµ‚æ¸…ç†
            body_content = str(content_soup)
            body_content = body_content.replace("\x00", "").replace("\r", "").replace("\n", "")
            body_content = body_content.replace('"', '\\"')
        else:
            body_content = ""
            
        # ç”Ÿæˆæ–‡ç«  ID
        article_id = str(uuid.uuid4())
            
        return {
            "story_id": article_info['story_id'],  # ä½¿ç”¨ story_id è€Œé story_index
            "story_title": article_info['story_title'],
            "story_category": article_info['story_category'],  # æ–°å¢ï¼šä¿å­˜åˆ†é¡
            "id": article_id,
            "article_index": article_info['article_index'],
            "article_title": article_info['article_title'],
            "google_news_url": article_info['article_url'],
            "final_url": final_url,
            "media_source": article_info.get('media_source', 'æœªçŸ¥ä¾†æº'),  # æ·»åŠ åª’é«”ä¾†æº
            "content": body_content,
        }
        
    except Exception as e:
        print(f"     âŒ ç„¡æ³•å–å¾—å…§å®¹: {e}")
    
    return None

def main():
    """ä¸»åŸ·è¡Œå‡½å¼"""
    # å®šç¾©è¦çˆ¬å–çš„æ–°èé ˜åŸŸ
    topic_sources = [
        # {
        #     "url": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFZ4ZERBU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "æ”¿æ²»"
        # },
        # {
        #     "url": "https://news.google.com/topics/CAAqLAgKIiZDQkFTRmdvSkwyMHZNR1ptZHpWbUVnVjZhQzFVVnhvQ1ZGY29BQVAB?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "ç§‘å­¸èˆ‡ç§‘æŠ€"
        # },
        {
            "url": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
            "category": "é«”è‚²"
        },
        # {
        #     "url": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        #     "category": "å•†æ¥­"
        # }
        # å¯ä»¥ç¹¼çºŒæ·»åŠ æ›´å¤šé ˜åŸŸ...
        # {
        #     "url": "å…¶ä»– Google News ä¸»é¡Œ URL",
        #     "category": "å…¶ä»–é ˜åŸŸåç¨±"
        # }
    ]
    
    # å»ºç«‹å¿…è¦çš„è³‡æ–™å¤¾
    os.makedirs("json", exist_ok=True)
    
    all_article_links = []
    
    # === æ­¥é©Ÿ 1 & 2: éæ­·æ‰€æœ‰ä¸»é¡Œé ˜åŸŸ ===
    print("=== é–‹å§‹çˆ¬å–å¤šå€‹æ–°èé ˜åŸŸ ===")
    
    for topic in topic_sources:
        main_url = topic["url"]
        category = topic["category"]
        
        print(f"\n=== è™•ç†ä¸»é¡Œï¼š{category} ===")
        
        # æ­¥é©Ÿ 1: æ‰¾åˆ°è©²é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµ
        story_links = get_main_story_links(main_url, category)
        
        if not story_links:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ° {category} é ˜åŸŸçš„ä»»ä½•ä¸»è¦æ•…äº‹é€£çµ")
            continue
        
        # æ­¥é©Ÿ 2: å¾æ¯å€‹æ•…äº‹æŠ“å–æ–‡ç« é€£çµ
        print(f"\n=== å¾ {category} é ˜åŸŸçš„æ¯å€‹æ•…äº‹æŠ“å–æ–‡ç« é€£çµ ===")
        
        for story in story_links[:4]:  # å¯è¦–éœ€æ±‚ä¿®æ”¹æ•¸é‡
            article_links = get_article_links_from_story(story)
            all_article_links.extend(article_links)
            time.sleep(2)
    
    # å„²å­˜æ–‡ç« é€£çµï¼ˆéå¿…è¦ï¼Œå¯è¨»è§£ï¼‰
    # with open("json/article_links.json", "w", encoding="utf-8") as f:
    #     json.dump(all_article_links, f, ensure_ascii=False, indent=2)
    # print(f"\nâœ… æ‰€æœ‰æ–‡ç« é€£çµå·²å„²å­˜åˆ° article_links.json (ç¸½å…± {len(all_article_links)} å€‹)")

    # === æ­¥é©Ÿ 3: æŠ“å–æ–‡ç« å…§å®¹å’Œåœ–ç‰‡ ===
    print("\n=== æ­¥é©Ÿ 3: æŠ“å–æ–‡ç« å…§å®¹å’Œåœ–ç‰‡ ===")
    final_articles = []

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.get("https://news.google.com/")  # å¿…é ˆå…ˆè¨ªå•ä¸€æ¬¡æ‰èƒ½åŠ  cookie
    time.sleep(2)

    # åŒ¯å…¥ cookiesï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        with open("cookies.json", "r", encoding="utf-8") as f:
            cookies = json.load(f)
        for cookie in cookies:
            if 'sameSite' in cookie:
                cookie.pop('sameSite')
            driver.add_cookie(cookie)
    except FileNotFoundError:
        print("âš ï¸  cookies.json æª”æ¡ˆä¸å­˜åœ¨ï¼Œç¹¼çºŒåŸ·è¡Œ...")

    # è™•ç†æ–‡ç« 
    for i, article in enumerate(all_article_links, start=1):  
        print(f"\nè™•ç†æ–‡ç«  {i}/{len(all_article_links)}")
        print(f"   [{article.get('story_category', 'æœªçŸ¥')}] {article['article_title']}")
        
        content = get_final_content(article, driver)
        
        if content:
            final_articles.append(content)
            print(f"âœ… æˆåŠŸæŠ“å–: [{content['story_category']}] {content['article_title']}")
        else:
            print(f"âŒ ç„¡æ³•å–å¾—å…§å®¹")
            
        time.sleep(1)

    driver.quit()

    if not final_articles:
        print("âŒ æ²’æœ‰æˆåŠŸæŠ“å–åˆ°ä»»ä½•æ–‡ç« å…§å®¹")
        return

    # === ä¾ story_id åˆ†çµ„ä¸¦ä½¿ç”¨ç¬¬ä¸€ç¯‡æ–‡ç« æ¨™é¡Œä½œç‚º story_title ===
    grouped = defaultdict(list)

    for item in final_articles:
        grouped[item["story_id"]].append({
            "id": item["id"],
            "article_index": item["article_index"],
            "article_title": item["article_title"],
            "google_news_url": item["google_news_url"],
            "final_url": item["final_url"],
            "media_source": item["media_source"],
            "content": item["content"],
        })

    grouped_articles = []
    for story_id in grouped.keys():
        # å¾ final_articles ä¸­æ‰¾åˆ°å°æ‡‰çš„æ•…äº‹è³‡è¨Š
        sample_article = next((x for x in final_articles if x["story_id"] == story_id), None)
        if sample_article:
            story_category = sample_article["story_category"]
        else:
            story_category = "æœªçŸ¥"
        
        # å°‡æ–‡ç« ä¾ç…§ article_index æ’åº
        articles_sorted = sorted(grouped[story_id], key=lambda x: x["article_index"])
        
        # ä½¿ç”¨ç¬¬ä¸€ç¯‡æ–‡ç« çš„æ¨™é¡Œä½œç‚º story_title
        story_title = articles_sorted[0]["article_title"] if articles_sorted else ""
            
        grouped_articles.append({
            "story_id": story_id,  # ä½¿ç”¨ story_id è€Œé story_index
            "story_title": story_title,  # ä½¿ç”¨ç¬¬ä¸€ç¯‡æ–‡ç« çš„æ¨™é¡Œ
            "crawl_date": dt.datetime.now().strftime("%Y/%m/%d %H:%M"),  # çˆ¬å–æ™‚é–“
            "category": story_category,  # æ–°å¢ï¼šåˆ†é¡è³‡è¨Š
            "articles": articles_sorted
        })

    # === å„²å­˜åˆ†çµ„ JSON ===
    with open("json/final_news.json", "w", encoding="utf-8") as f:
        json.dump(grouped_articles, f, ensure_ascii=False, indent=2)

    # === çµ±è¨ˆè³‡è¨Š ===
    category_stats = defaultdict(int)
    for article in final_articles:
        category_stats[article['story_category']] += 1

    print(f"\nğŸ‰ å®Œæˆï¼ç¸½å…±æˆåŠŸæŠ“å– {len(final_articles)} ç¯‡æ–‡ç« ")
    print("ğŸ“Š å„é ˜åŸŸæ–‡ç« æ•¸é‡çµ±è¨ˆ:")
    for category, count in category_stats.items():
        print(f"   {category}: {count} ç¯‡")
    print("ğŸ“ å„²å­˜æª”æ¡ˆ:")
    print("   - New_Summary/data/final_news.jsonï¼ˆä¾ story_id åˆ†çµ„ï¼ŒåŒ…å«é ˜åŸŸåˆ†é¡ï¼‰")
    print("   - json/article_links.jsonï¼ˆæ‰€æœ‰æ–‡ç« é€£çµï¼‰")

if __name__ == "__main__":
    main()