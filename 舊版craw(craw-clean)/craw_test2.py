from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
import time
import datetime as dt
from datetime import datetime
import requests
from supabase import create_client, Client
import uuid
import os
import json
import random
import re
from urllib.parse import urljoin, urlparse
from collections import defaultdict

# === Supabase è¨­å®š ===
SUPABASE_URL = os.getenv("API_KEY_URL")
SUPABASE_SERVICE_KEY = os.getenv("API_KEY_supa")
if SUPABASE_URL and SUPABASE_SERVICE_KEY:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

# Chrome options è¨­å®š
chrome_options = Options()
# chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36")

def download_images_from_soup(soup, final_url, article_id, base_folder="images"):
    """å¾ BeautifulSoup ç‰©ä»¶ä¸­ä¸‹è¼‰åœ–ç‰‡"""
    images_info = []
    
    # å»ºç«‹åœ–ç‰‡è³‡æ–™å¤¾
    article_folder = os.path.join(base_folder, article_id)
    os.makedirs(article_folder, exist_ok=True)
    
    # å°‹æ‰¾åœ–ç‰‡æ¨™ç±¤
    img_tags = soup.find_all('img')
    
    # å¸¸è¦‹çš„æ–°èåœ–ç‰‡é¸æ“‡å™¨
    news_selectors = [
        'img[src*="upload"]',
        'article img',
        '.content img',
        '.article-content img',
        '.story img',
        'figure img',
        '.image img',
        '.photo img'
    ]
    
    # å˜—è©¦ç”¨æ›´ç²¾ç¢ºçš„é¸æ“‡å™¨æ‰¾æ–°èåœ–ç‰‡
    news_images = []
    for selector in news_selectors:
        found_imgs = soup.select(selector)
        news_images.extend(found_imgs)
    
    # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šçš„æ–°èåœ–ç‰‡ï¼Œä½¿ç”¨æ‰€æœ‰åœ–ç‰‡ä½†éæ¿¾æ›´åš´æ ¼
    if not news_images:
        news_images = img_tags
    
    print(f"   ğŸ–¼ï¸  æ‰¾åˆ° {len(news_images)} å€‹åœ–ç‰‡æ¨™ç±¤")
    
    downloaded_count = 0
    
    for i, img in enumerate(news_images):
        try:
            # ç²å–åœ–ç‰‡URL
            img_url = (img.get('src') or 
                      img.get('data-src') or 
                      img.get('data-lazy-src') or
                      img.get('data-original'))
            
            if not img_url or img_url.startswith('data:'):
                continue
            
            # è™•ç†ç›¸å°è·¯å¾‘
            img_url = urljoin(final_url, img_url)
            
            # éæ¿¾æ‰æ˜é¡¯çš„å»£å‘Šæˆ–å°åœ–ç‰‡
            if any(keyword in img_url.lower() for keyword in ['ad', 'banner', 'logo', 'icon', 'avatar']):
                continue
            
            # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸å±¬æ€§
            width = img.get('width')
            height = img.get('height')
            if width and height:
                try:
                    if int(width) < 200 or int(height) < 200:
                        continue
                except:
                    pass
            
            # ä¸‹è¼‰åœ–ç‰‡
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Referer': final_url
            }
            
            img_response = requests.get(img_url, headers=headers, timeout=10)
            img_response.raise_for_status()
            
            # æª¢æŸ¥å…§å®¹å¤§å°ï¼Œéæ¿¾å¤ªå°çš„åœ–ç‰‡
            if len(img_response.content) < 5000:  # å°æ–¼ 5KB
                continue
            
            # ç²å–æª”æ¡ˆå‰¯æª”å
            parsed_url = urlparse(img_url)
            filename = os.path.basename(parsed_url.path)
            
            if not filename or '.' not in filename:
                # æ ¹æ“š content-type æ±ºå®šå‰¯æª”å
                content_type = img_response.headers.get('content-type', '')
                if 'jpeg' in content_type or 'jpg' in content_type:
                    ext = '.jpg'
                elif 'png' in content_type:
                    ext = '.png'
                elif 'webp' in content_type:
                    ext = '.webp'
                else:
                    ext = '.jpg'
                filename = f"image_{i}{ext}"
            
            # å„²å­˜åœ–ç‰‡
            filepath = os.path.join(article_folder, filename)
            with open(filepath, 'wb') as f:
                f.write(img_response.content)
            
            # è¨˜éŒ„åœ–ç‰‡è³‡è¨Š
            images_info.append({
                "filename": filename,
                "url": img_url,
                "local_path": filepath,
                "size": len(img_response.content)
            })
            
            downloaded_count += 1
            print(f"     âœ… ä¸‹è¼‰åœ–ç‰‡: {filename}")
            
            # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
            time.sleep(0.3)
            
        except Exception as e:
            print(f"     âŒ ä¸‹è¼‰åœ–ç‰‡å¤±æ•—: {e}")
    
    print(f"   ğŸ“Š ç¸½å…±ä¸‹è¼‰äº† {downloaded_count} å¼µåœ–ç‰‡")
    return images_info

def get_main_story_links(main_url, category):
    """æ­¥é©Ÿ 1: å¾ä¸»é æŠ“å–æ‰€æœ‰ä¸»è¦æ•…äº‹é€£çµ"""
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    story_links = []
    
    try:
        print(f"ğŸ” æ­£åœ¨æŠ“å– {category} é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµ...")
        driver.get(main_url)
        
        # ç­‰å¾…é é¢è¼‰å…¥ - æ‰¾åˆ°æ‰€æœ‰ c-wiz å€å¡Š
        wait = WebDriverWait(driver, 15)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'c-wiz[jsrenderer="jeGyVb"]')))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        c_wiz_blocks = soup.find_all("c-wiz", {"jsrenderer": "jeGyVb"})
        
        print(f"âœ… æ‰¾åˆ° {len(c_wiz_blocks)} å€‹ c-wiz å€å¡Š")
        
        # å¾æ¯å€‹ c-wiz å€å¡Šä¸­æ‰¾åˆ°ä¸»è¦æ•…äº‹é€£çµ
        for i, block in enumerate(c_wiz_blocks, start=1):
            story_link = block.find("a", class_="jKHa4e")  # ä¸»è¦æ•…äº‹é€£çµ
            
            if story_link:
                href = story_link.get("href")
                title = story_link.text.strip()
                print(f"   è™•ç†æ•…äº‹ {i}: {href}")
                
                if href:
                    # è™•ç†ç›¸å°é€£çµ
                    if href.startswith("./"):
                        full_link = "https://news.google.com" + href[1:]
                    else:
                        full_link = "https://news.google.com" + href
                    
                    story_links.append({
                        "index": i,
                        "title": title,
                        "url": full_link,
                        "category": category  # æ–°å¢ï¼šå°‡åˆ†é¡è³‡è¨ŠåŠ å…¥
                    })
                    
                    print(f"{i}. ğŸ“° [{category}] {title}")
                    print(f"   ğŸ”— {full_link}")
        
        print(f"\nğŸ“Š ç¸½å…±æ”¶é›†åˆ° {len(story_links)} å€‹ {category} é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµ")
        
    except Exception as e:
        print(f"âŒ æŠ“å–ä¸»è¦æ•…äº‹é€£çµæ™‚å‡ºéŒ¯: {e}")
    finally:
        driver.quit()
    
    return story_links

def get_article_links_from_story(story_info):
    """æ­¥é©Ÿ 2: é€²å…¥æ¯å€‹æ•…äº‹é é¢ï¼Œæ‰¾å‡ºæ‰€æœ‰ a tag class="VDXfz" çš„ href"""
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    article_links = []
    
    try:
        print(f"\nğŸ” æ­£åœ¨è™•ç†æ•…äº‹ {story_info['index']}: [{story_info['category']}] {story_info['title']}")
        driver.get(story_info['url'])
        time.sleep(random.randint(3, 6))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # æ‰¾åˆ°æ‰€æœ‰ h4 tag class="ipQwMb ekueJc RD0gLb" 
        h4_elements = soup.find_all("h4", class_="ipQwMb ekueJc RD0gLb")
        
        print(f"   âœ… æ‰¾åˆ° {len(h4_elements)} å€‹ h4 å…ƒç´ ")
        
        for j, h4 in enumerate(h4_elements, start=1):
            # if j > 5:
            #     break  # æœ€å¤šåªæŠ“å– 5 ç¯‡æ–‡ç«     
            
            # åœ¨ h4 ä¸‹é¢æ‰¾ a tag class="DY5T1d RZIKme"
            link = h4.find("a", class_="DY5T1d RZIKme")
            
            if link:
                href = link.get("href")
                link_text = link.text.strip()
                
                if href:
                    # è™•ç†ç›¸å°é€£çµ
                    if href.startswith("./"):
                        full_href = "https://news.google.com" + href[1:]
                    else:
                        full_href = "https://news.google.com" + href
                    
                    article_links.append({
                        "story_index": story_info['index'],
                        "story_title": story_info['title'],
                        "story_category": story_info['category'],  # æ–°å¢ï¼šä¿å­˜æ•…äº‹åˆ†é¡
                        "article_index": j,
                        "article_title": link_text,
                        "article_url": full_href
                    })
                    
                    print(f"     {j}. ğŸ“„ {link_text}")
                    print(f"        ğŸ”— {full_href}")
            else:
                print(f"     {j}. âŒ h4 å…ƒç´ ä¸­æ²’æœ‰æ‰¾åˆ°å°æ‡‰çš„ a tag")
        
    except Exception as e:
        print(f"âŒ è™•ç†æ•…äº‹æ™‚å‡ºéŒ¯: {e}")
    finally:
        driver.quit()
    
    return article_links

def extract_publish_time(soup, final_url):
    """å¾ç¶²é ä¸­æå–ç™¼å¸ƒæ™‚é–“"""
    publish_time = None
    
    # å¸¸è¦‹çš„æ™‚é–“é¸æ“‡å™¨å’Œå±¬æ€§
    time_selectors = [
        # JSON-LD çµæ§‹åŒ–æ•¸æ“š
        'script[type="application/ld+json"]',
        # HTML5 time æ¨™ç±¤
        'time[datetime]',
        'time[pubdate]',
        # å¸¸è¦‹çš„æ™‚é–“ class
        '.publish-time',
        '.published-time',
        '.article-time',
        '.post-time',
        '.date',
        '.timestamp',
        '.article-date',
        '.publish-date',
        '.post-date',
        '.entry-date',
        # Meta æ¨™ç±¤
        'meta[property="article:published_time"]',
        'meta[name="publish_date"]',
        'meta[name="article:published_time"]',
        'meta[name="pubdate"]',
        'meta[name="date"]',
        # ç‰¹å®šæ–°èç¶²ç«™
        '.story-meta time',
        '.byline time',
        '.article-meta time',
        '.news-meta time'
    ]
    
    for selector in time_selectors:
        try:
            elements = soup.select(selector)
            
            for element in elements:
                time_text = None
                
                if selector == 'script[type="application/ld+json"]':
                    # è™•ç† JSON-LD
                    try:
                        json_content = json.loads(element.string)
                        if isinstance(json_content, list):
                            json_content = json_content[0]
                        
                        # æŸ¥æ‰¾æ—¥æœŸæ¬„ä½
                        date_fields = ['datePublished', 'publishedDate', 'dateCreated', 'dateModified']
                        for field in date_fields:
                            if field in json_content:
                                time_text = json_content[field]
                                break
                    except:
                        continue
                
                elif element.name == 'meta':
                    # Meta æ¨™ç±¤
                    time_text = element.get('content')
                
                elif element.name == 'time':
                    # Time æ¨™ç±¤
                    time_text = element.get('datetime') or element.get_text().strip()
                
                else:
                    # å…¶ä»–å…ƒç´ 
                    time_text = element.get_text().strip()
                
                if time_text:
                    # å˜—è©¦è§£ææ™‚é–“
                    parsed_time = parse_time_string(time_text)
                    if parsed_time:
                        publish_time = parsed_time
                        print(f"   â° æ‰¾åˆ°ç™¼å¸ƒæ™‚é–“: {publish_time} (ä¾†æº: {selector})")
                        return publish_time
        
        except Exception as e:
            continue
    
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå˜—è©¦ç”¨æ­£å‰‡è¡¨é”å¼åœ¨æ–‡æœ¬ä¸­å°‹æ‰¾
    try:
        text_content = soup.get_text()
        
        # å¸¸è¦‹çš„æ™‚é–“æ ¼å¼æ­£å‰‡è¡¨é”å¼
        time_patterns = [
            r'(\d{4}[-/]\d{1,2}[-/]\d{1,2}[\s\T]\d{1,2}:\d{2})',  # YYYY-MM-DD HH:MM
            r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})',  # YYYY-MM-DD
            r'(\d{1,2}[-/]\d{1,2}[-/]\d{4})',  # MM-DD-YYYY æˆ– DD-MM-YYYY
            r'ç™¼å¸ƒæ™‚é–“[ï¼š:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2}[\s\T]?\d{0,2}:?\d{0,2})',
            r'æ›´æ–°æ™‚é–“[ï¼š:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2}[\s\T]?\d{0,2}:?\d{0,2})',
            r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥[\s]*\d{0,2}:?\d{0,2})',  # ä¸­æ–‡æ ¼å¼
        ]
        
        for pattern in time_patterns:
            matches = re.findall(pattern, text_content)
            if matches:
                time_text = matches[0]
                parsed_time = parse_time_string(time_text)
                if parsed_time:
                    print(f"   â° å¾æ–‡æœ¬ä¸­æ‰¾åˆ°æ™‚é–“: {parsed_time}")
                    return parsed_time
    
    except Exception as e:
        pass
    
    print(f"   âš ï¸  ç„¡æ³•æ‰¾åˆ°ç™¼å¸ƒæ™‚é–“")
    return None

def parse_time_string(time_str):
    """è§£æå„ç¨®æ ¼å¼çš„æ™‚é–“å­—ç¬¦ä¸²"""
    if not time_str:
        return None
    
    # æ¸…ç†æ™‚é–“å­—ç¬¦ä¸²
    time_str = str(time_str).strip()
    
    # ç§»é™¤æ™‚å€ä¿¡æ¯ï¼ˆç°¡åŒ–è™•ç†ï¼‰
    time_str = re.sub(r'[+-]\d{2}:?\d{2}$', '', time_str)
    time_str = re.sub(r'[A-Z]{3,4}$', '', time_str)
    
    # å¸¸è¦‹çš„æ™‚é–“æ ¼å¼
    formats = [
        '%Y-%m-%dT%H:%M:%S',      # ISO æ ¼å¼
        '%Y-%m-%d %H:%M:%S',      # æ¨™æº–æ ¼å¼
        '%Y-%m-%d %H:%M',         # æ²’æœ‰ç§’
        '%Y-%m-%d',               # åªæœ‰æ—¥æœŸ
        '%Y/%m/%d %H:%M:%S',      # æ–œç·šåˆ†éš”
        '%Y/%m/%d %H:%M',         # æ–œç·šåˆ†éš”ï¼Œæ²’æœ‰ç§’
        '%Y/%m/%d',               # æ–œç·šåˆ†éš”ï¼Œåªæœ‰æ—¥æœŸ
        '%d/%m/%Y %H:%M:%S',      # æ­æ´²æ ¼å¼
        '%d/%m/%Y %H:%M',         # æ­æ´²æ ¼å¼ï¼Œæ²’æœ‰ç§’
        '%d/%m/%Y',               # æ­æ´²æ ¼å¼ï¼Œåªæœ‰æ—¥æœŸ
        '%m/%d/%Y %H:%M:%S',      # ç¾åœ‹æ ¼å¼
        '%m/%d/%Y %H:%M',         # ç¾åœ‹æ ¼å¼ï¼Œæ²’æœ‰ç§’
        '%m/%d/%Y',               # ç¾åœ‹æ ¼å¼ï¼Œåªæœ‰æ—¥æœŸ
    ]
    
    # è™•ç†ä¸­æ–‡æ—¥æœŸæ ¼å¼
    if 'å¹´' in time_str and 'æœˆ' in time_str and 'æ—¥' in time_str:
        try:
            # æå–æ•¸å­—
            year_match = re.search(r'(\d{4})å¹´', time_str)
            month_match = re.search(r'(\d{1,2})æœˆ', time_str)
            day_match = re.search(r'(\d{1,2})æ—¥', time_str)
            time_match = re.search(r'(\d{1,2}):(\d{2})', time_str)
            
            if year_match and month_match and day_match:
                year = int(year_match.group(1))
                month = int(month_match.group(1))
                day = int(day_match.group(1))
                
                if time_match:
                    hour = int(time_match.group(1))
                    minute = int(time_match.group(2))
                    return datetime(year, month, day, hour, minute).strftime('%Y/%m/%d %H:%M')
                else:
                    return datetime(year, month, day).strftime('%Y/%m/%d')
        except:
            pass
    
    # å˜—è©¦å„ç¨®æ ¼å¼
    for fmt in formats:
        try:
            parsed_dt = datetime.strptime(time_str, fmt)
            return parsed_dt.strftime('%Y/%m/%d %H:%M') if '%H' in fmt else parsed_dt.strftime('%Y/%m/%d')
        except:
            continue
    
    return None

def get_final_content(article_info, driver):
    """æ­¥é©Ÿ 3: è·³è½‰åˆ°åŸå§‹ç¶²ç«™ä¸¦æŠ“å–å…§å®¹ã€åœ–ç‰‡å’Œæ™‚é–“"""
    try:
        driver.get(article_info['article_url'])
        time.sleep(random.randint(4, 8))
        
        # å–å¾—è·³è½‰å¾Œçš„çœŸå¯¦ç¶²å€
        final_url = driver.current_url
        print(f"   æœ€çµ‚ç¶²å€: {final_url}")
        
        # å–å¾— HTML åŸå§‹ç¢¼ä¸¦äº¤çµ¦ BeautifulSoup
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        # æ¸…ç†å…§å®¹
        if soup.body:
            body_content = str(soup.body)
            body_content = body_content.replace("\x00", "").replace("\r", "").replace("\n", "")
            body_content = body_content.replace('"', '\\"')
            
            # ç”Ÿæˆæ–‡ç«  ID
            article_id = str(uuid.uuid4())
            
            # === æ–°å¢ï¼šæå–ç™¼å¸ƒæ™‚é–“ ===
            print(f"   â° é–‹å§‹æå–ç™¼å¸ƒæ™‚é–“...")
            publish_time = extract_publish_time(soup, final_url)
            
            # === æ–°å¢ï¼šä¸‹è¼‰åœ–ç‰‡ ===
            print(f"   ğŸ–¼ï¸  é–‹å§‹ä¸‹è¼‰åœ–ç‰‡...")
            images_info = download_images_from_soup(soup, final_url, article_id)
            
            return {
                "id": article_id,
                "story_index": article_info['story_index'],
                "story_title": article_info['story_title'],
                "story_category": article_info['story_category'],  # æ–°å¢ï¼šä¿å­˜åˆ†é¡
                "article_index": article_info['article_index'],
                "article_title": article_info['article_title'],
                "google_news_url": article_info['article_url'],
                "final_url": final_url,
                "crawl_date": dt.datetime.now().strftime("%Y/%m/%d %H:%M"),  # çˆ¬å–æ™‚é–“
                "publish_date": publish_time,  # æ–°å¢ï¼šæ–‡ç« ç™¼å¸ƒæ™‚é–“
                "content": body_content,
                "images": images_info  # æ–°å¢ï¼šåœ–ç‰‡è³‡è¨Š
            }
        
    except Exception as e:
        print(f"     âŒ ç„¡æ³•å–å¾—å…§å®¹: {e}")
    
    return None

def main():
    """ä¸»åŸ·è¡Œå‡½å¼"""
    # å®šç¾©è¦çˆ¬å–çš„æ–°èé ˜åŸŸ
    topic_sources = [
        {
            "url": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFZ4ZERBU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
            "category": "æ”¿æ²»"
        },
        {
            "url": "https://news.google.com/topics/CAAqLAgKIiZDQkFTRmdvSkwyMHZNR1ptZHpWbUVnVjZhQzFVVnhvQ1ZGY29BQVAB?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
            "category": "ç§‘å­¸èˆ‡ç§‘æŠ€"
        }
        # å¯ä»¥ç¹¼çºŒæ·»åŠ æ›´å¤šé ˜åŸŸ...
        # {
        #     "url": "å…¶ä»– Google News ä¸»é¡Œ URL",
        #     "category": "å…¶ä»–é ˜åŸŸåç¨±"
        # }
    ]
    
    # å»ºç«‹å¿…è¦çš„è³‡æ–™å¤¾
    os.makedirs("json", exist_ok=True)
    os.makedirs("images", exist_ok=True)
    
    # ç”¨æ–¼è·Ÿè¹¤å…¨å±€æ•…äº‹ç´¢å¼•
    global_story_index = 1
    all_article_links = []
    
    # === æ­¥é©Ÿ 1 & 2: éæ­·æ‰€æœ‰ä¸»é¡Œé ˜åŸŸ ===
    print("=== é–‹å§‹çˆ¬å–å¤šå€‹æ–°èé ˜åŸŸ ===")
    
    for topic in topic_sources:
        main_url = topic["url"]
        category = topic["category"]
        
        print(f"\n=== è™•ç†ä¸»é¡Œï¼š{category} ===")
        
        # æ­¥é©Ÿ 1: æ‰¾åˆ°è©²é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµ
        story_links = get_main_story_links(main_url, category)
        
        if not story_links:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ° {category} é ˜åŸŸçš„ä»»ä½•ä¸»è¦æ•…äº‹é€£çµ")
            continue
        
        # æ­¥é©Ÿ 2: å¾æ¯å€‹æ•…äº‹æŠ“å–æ–‡ç« é€£çµ
        print(f"\n=== å¾ {category} é ˜åŸŸçš„æ¯å€‹æ•…äº‹æŠ“å–æ–‡ç« é€£çµ ===")
        
        for story in story_links[:8]:  # å¯è¦–éœ€æ±‚ä¿®æ”¹æ•¸é‡
            # æ›´æ–°ç‚ºå…¨å±€ç´¢å¼•
            story['index'] = global_story_index
            
            article_links = get_article_links_from_story(story)
            all_article_links.extend(article_links)
            
            global_story_index += 1
            time.sleep(2)
    
    # å„²å­˜æ–‡ç« é€£çµï¼ˆéå¿…è¦ï¼Œå¯è¨»è§£ï¼‰
    with open("json/article_links.json", "w", encoding="utf-8") as f:
        json.dump(all_article_links, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… æ‰€æœ‰æ–‡ç« é€£çµå·²å„²å­˜åˆ° article_links.json (ç¸½å…± {len(all_article_links)} å€‹)")

    # === æ­¥é©Ÿ 3: æŠ“å–æ–‡ç« å…§å®¹å’Œåœ–ç‰‡ ===
    print("\n=== æ­¥é©Ÿ 3: æŠ“å–æ–‡ç« å…§å®¹å’Œåœ–ç‰‡ ===")
    final_articles = []

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.get("https://news.google.com/")  # å¿…é ˆå…ˆè¨ªå•ä¸€æ¬¡æ‰èƒ½åŠ  cookie
    time.sleep(2)

    # åŒ¯å…¥ cookiesï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        with open("cookies.json", "r", encoding="utf-8") as f:
            cookies = json.load(f)
        for cookie in cookies:
            if 'sameSite' in cookie:
                cookie.pop('sameSite')
            driver.add_cookie(cookie)
    except FileNotFoundError:
        print("âš ï¸  cookies.json æª”æ¡ˆä¸å­˜åœ¨ï¼Œç¹¼çºŒåŸ·è¡Œ...")

    # è™•ç†æ–‡ç« 
    for i, article in enumerate(all_article_links[:40], start=1):  # é™åˆ¶40ç¯‡æ–‡ç« 
        print(f"\nè™•ç†æ–‡ç«  {i}/{min(40, len(all_article_links))}")
        print(f"   [{article.get('story_category', 'æœªçŸ¥')}] {article['article_title']}")
        
        content = get_final_content(article, driver)
        
        if content:
            final_articles.append(content)
            print(f"âœ… æˆåŠŸæŠ“å–: [{content['story_category']}] {content['article_title']}")
            print(f"   ğŸ“· åœ–ç‰‡æ•¸é‡: {len(content['images'])}")
        else:
            print(f"âŒ ç„¡æ³•å–å¾—å…§å®¹")
            
        time.sleep(1)

    driver.quit()

    if not final_articles:
        print("âŒ æ²’æœ‰æˆåŠŸæŠ“å–åˆ°ä»»ä½•æ–‡ç« å…§å®¹")
        return

    # === ä¾ story_index åˆ†çµ„ä¸¦åŒ…å«åˆ†é¡è³‡è¨Š ===
    grouped = defaultdict(list)

    for item in final_articles:
        grouped[item["story_index"]].append({
            "id": item["id"],
            "article_index": item["article_index"],
            "article_title": item["article_title"],
            "google_news_url": item["google_news_url"],
            "final_url": item["final_url"],
            "crawl_date": item["crawl_date"],
            "publish_date": item["publish_date"],
            "content": item["content"],
            "images": item["images"]
        })

    grouped_articles = []
    for story_index in sorted(grouped.keys()):
        # å¾ final_articles ä¸­æ‰¾åˆ°å°æ‡‰çš„æ•…äº‹è³‡è¨Š
        sample_article = next((x for x in final_articles if x["story_index"] == story_index), None)
        if sample_article:
            story_title = sample_article["story_title"]
            story_category = sample_article["story_category"]
        else:
            story_title = ""
            story_category = "æœªçŸ¥"
            
        grouped_articles.append({
            "story_index": story_index,
            "story_title": story_title,
            "category": story_category,  # æ–°å¢ï¼šåˆ†é¡è³‡è¨Š
            "articles": grouped[story_index]
        })

    # === å„²å­˜åˆ†çµ„ JSON ===
    with open("json/final_news.json", "w", encoding="utf-8") as f:
        json.dump(grouped_articles, f, ensure_ascii=False, indent=2)

    # === çµ±è¨ˆè³‡è¨Š ===
    total_images = sum(len(article['images']) for article in final_articles)
    category_stats = defaultdict(int)
    for article in final_articles:
        category_stats[article['story_category']] += 1

    print(f"\nğŸ‰ å®Œæˆï¼ç¸½å…±æˆåŠŸæŠ“å– {len(final_articles)} ç¯‡æ–‡ç« ")
    print("ğŸ“Š å„é ˜åŸŸæ–‡ç« æ•¸é‡çµ±è¨ˆ:")
    for category, count in category_stats.items():
        print(f"   {category}: {count} ç¯‡")
    print(f"ğŸ“· ç¸½å…±ä¸‹è¼‰ {total_images} å¼µåœ–ç‰‡")
    print("ğŸ“ å„²å­˜æª”æ¡ˆ:")
    print("   - json/final_news.jsonï¼ˆä¾ story_index åˆ†çµ„ï¼ŒåŒ…å«é ˜åŸŸåˆ†é¡å’Œåœ–ç‰‡è³‡è¨Šï¼‰")
    print("   - json/article_links.jsonï¼ˆæ‰€æœ‰æ–‡ç« é€£çµï¼‰")
    print("   - images/ è³‡æ–™å¤¾ï¼ˆä¾æ–‡ç«  ID åˆ†é¡çš„åœ–ç‰‡ï¼‰")

if __name__ == "__main__":
    main()