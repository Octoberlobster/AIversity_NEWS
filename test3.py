import json
import time
import random
import uuid
import os
from collections import defaultdict
from datetime import datetime, timedelta
from dateutil import parser
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup

def create_robust_driver(headless: bool = False):
    """å‰µå»ºä¸€å€‹æ›´ç©©å¥çš„ WebDriver"""
    options = webdriver.ChromeOptions()

    if headless:
        options.add_argument("--headless=new")  # ç„¡é ­æ¨¡å¼
    else:
        # æœ‰è¦–çª— â†’ ä¸è¦åŠ  headless
        options.add_argument("--start-maximized")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    driver = webdriver.Chrome(options=options)
    return driver

def check_story_exists_in_supabase(story_url, category, article_datetime, article_url):
    """æ£€æŸ¥æ•…äº‹æ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼ˆå ä½ç¬¦å‡½æ•°ï¼‰"""
    # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ•°æ®åº“æ£€æŸ¥é€»è¾‘
    # ç°åœ¨è¿”å›é»˜è®¤å€¼ï¼Œè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ–‡ç« 
    return False, "process", None, "æ–°æ–‡ç« "

def get_main_story_links(main_url, category):
    """æ­¥é©Ÿ 1: å¾ä¸»é æŠ“å–æ‰€æœ‰ä¸»è¦æ•…äº‹é€£çµ"""
    driver = None
    story_links = []
    
    try:
        driver = create_robust_driver(headless=True)

        print(f"ğŸ” æ­£åœ¨æŠ“å– {category} é ˜åŸŸçš„ä¸»è¦æ•…äº‹é€£çµ...")
        driver.get(main_url)
        
        # ç­‰å¾…é é¢è¼‰å…¥ - æ‰¾åˆ°æ‰€æœ‰ c-wiz å€å¡Š
        wait = WebDriverWait(driver, 15)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'c-wiz[jsrenderer="jeGyVb"]')))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        c_wiz_blocks = soup.find_all("c-wiz", {"jsrenderer": "jeGyVb"})
        
        print(f"âœ… æ‰¾åˆ° {len(c_wiz_blocks)} å€‹ c-wiz å€å¡Š")
        
        # å¾æ¯å€‹ c-wiz å€å¡Šä¸­æ‰¾åˆ°ä¸»è¦æ•…äº‹é€£çµ
        for i, block in enumerate(c_wiz_blocks, start=1):
            try:
                story_link = block.find("a", class_="jKHa4e")  # ä¸»è¦æ•…äº‹é€£çµ
                
                if story_link:
                    href = story_link.get("href")
                    title = story_link.text.strip()
                    
                    if href:
                        # è™•ç†ç›¸å°é€£çµ
                        if href.startswith("./"):
                            full_link = "https://news.google.com" + href[1:]
                        else:
                            full_link = "https://news.google.com" + href
                        
                        # æ£€æŸ¥æ•°æ®åº“
                        should_skip, action_type, story_data, skip_reason = check_story_exists_in_supabase(
                            full_link, category, "", ""
                        )
                        
                        print(f"   è™•ç†æ•…äº‹ {i}: {href}")
                        print(f"   ğŸ“‹ æª¢æŸ¥çµæœ: {skip_reason}")
                        
                        # ç‚ºæ¯å€‹æ•…äº‹ç”Ÿæˆ UUID
                        story_id = str(uuid.uuid4())
                        
                        story_links.append({
                            "index": i,
                            "story_id": story_id,
                            "title": title,
                            "url": full_link,
                            "category": category,
                            "action_type": action_type,
                            "existing_story_data": story_data
                        })
                        
                        print(f"{i}. ğŸ“° [{category}] {title}")
                        print(f"   ğŸ†” æ•…äº‹ID: {story_id}")
                        print(f"   ğŸ”— {full_link}")
                        print(f"   ğŸ¯ è™•ç†é¡å‹: {action_type}")
                        
            except Exception as e:
                print(f"âŒ è™•ç†æ•…äº‹å€å¡Š {i} æ™‚å‡ºéŒ¯: {e}")
                continue
        
        print(f"\nğŸ“Š ç¸½å…±æ”¶é›†åˆ° {len(story_links)} å€‹ {category} é ˜åŸŸéœ€è¦è™•ç†çš„ä¸»è¦æ•…äº‹é€£çµ")
        
    except TimeoutException:
        print(f"âŒ é é¢è¼‰å…¥è¶…æ™‚: {main_url}")
    except WebDriverException as e:
        print(f"âŒ WebDriver éŒ¯èª¤: {e}")
    except Exception as e:
        print(f"âŒ æŠ“å–ä¸»è¦æ•…äº‹é€£çµæ™‚å‡ºéŒ¯: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return story_links

def get_article_links_from_story(story_info):
    """æ­¥é©Ÿ 2: é€²å…¥æ¯å€‹æ•…äº‹é é¢ï¼Œæ‰¾å‡ºæ‰€æœ‰ article ä¸‹çš„æ–‡ç« é€£çµå’Œç›¸é—œä¿¡æ¯"""
    driver = None
    article_links = []
    
    try:
        driver = create_robust_driver(headless=True)

        print(f"\nğŸ” æ­£åœ¨è™•ç†æ•…äº‹ {story_info['index']}: [{story_info['category']}] {story_info['title']}")
        print(f"   ğŸ†” æ•…äº‹ID: {story_info['story_id']}")
        
        driver.get(story_info['url'])
        time.sleep(random.randint(3, 6))
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # æ‰¾åˆ°æ‰€æœ‰ article tag
        article_elements = soup.find_all("article", class_="MQsxIb xTewfe tXImLc R7GTQ keNKEd keNKEd VkAdve GU7x0c JMJvke q4atFc")
        
        print(f"   âœ… æ‰¾åˆ° {len(article_elements)} å€‹ article å…ƒç´ ")
        
        processed_count = 0
        
        for j, article in enumerate(article_elements, start=1):
            try:
                if processed_count >= 10:
                    break  # æœ€å¤šåªæŠ“å– 15 ç¯‡æ–‡ç« 

                # åœ¨ article ä¸‹é¢æ‰¾ h4 tag
                h4_element = article.find("h4", class_="ipQwMb ekueJc RD0gLb")
                
                if h4_element:
                    # åœ¨ h4 ä¸‹é¢æ‰¾ a tag
                    link = h4_element.find("a", class_="DY5T1d RZIKme")
                    
                    if link:
                        href = link.get("href")
                        link_text = link.text.strip()
                        
                        # æ‰¾åª’é«”ä¾†æº
                        media_element = article.find("a", class_="wEwyrc")
                        media = media_element.text.strip() if media_element else "æœªçŸ¥ä¾†æº"

                        if media == "MSN" or media == "è‡ªç”±æ™‚å ±" or media == "chinatimes.com" or media == "ä¸­æ™‚é›»å­å ±" or media == "ä¸­æ™‚æ–°èç¶²" or media == "ä¸Šå ±Up Media":
                            continue

                        # æ‰¾æ™‚é–“
                        time_element = article.find(class_="WW6dff uQIVzc Sksgp slhocf")
                        if time_element and time_element.get("datetime"):
                            dt_str = time_element.get("datetime")  # e.g. "2025-08-21T07:15:00Z"
                            # è§£ææˆ datetime (å‡è¨­ä¾†æºæ˜¯ UTC æ ¼å¼ï¼ŒISO 8601)
                            dt_obj = datetime.fromisoformat(dt_str.replace("Z", "+00:00"))

                            # åŠ ä¸Š 8 å°æ™‚ï¼ˆUTC+8 â†’ å°ç£æ™‚é–“ï¼‰
                            article_datetime = dt_obj + timedelta(hours=8)
                            article_datetime = article_datetime.strftime("%Y-%m-%d %H:%M:%S")

                        else:
                            article_datetime = "æœªçŸ¥æ™‚é–“"

                        if href:
                            # è™•ç†ç›¸å°é€£çµ
                            if href.startswith("./"):
                                full_href = "https://news.google.com" + href[1:]
                            else:
                                full_href = "https://news.google.com" + href
                            
                            # é€²éšæª¢æŸ¥ï¼šåŒ…å«æ–‡ç« è³‡è¨Š
                            should_skip, action_type, story_data, skip_reason = check_story_exists_in_supabase(
                                story_info['url'], story_info['category'], article_datetime, full_href
                            )
                            
                            if should_skip and action_type == "skip":
                                print(f"     â­ï¸  è·³éæ–‡ç« : {link_text}")
                                print(f"        åŸå› : {skip_reason}")
                                continue
                            
                            article_links.append({
                                "story_id": story_info['story_id'],
                                "story_title": story_info['title'],
                                "story_category": story_info['category'],
                                "story_url": story_info['url'],
                                "article_index": processed_count + 1,
                                "article_title": link_text,
                                "article_url": full_href,
                                "media": media,
                                "article_datetime": article_datetime,
                                "action_type": action_type,
                                "existing_story_data": story_data
                            })
                            
                            processed_count += 1
                            print(f"     {processed_count}. ğŸ“„ {link_text}")
                            print(f"        ğŸ¢ åª’é«”: {media}")
                            print(f"        ğŸ“… æ™‚é–“: {article_datetime}")
                            print(f"        ğŸ¯ è™•ç†é¡å‹: {action_type}")
                            print(f"        ğŸ”— {full_href}")
                        else:
                            print(f"     {j}. âš ï¸ æ‰¾ä¸åˆ°æ–‡ç« é€£çµ")
                    else:
                        print(f"     {j}. âš ï¸ h4 å…ƒç´ ä¸­æ²’æœ‰æ‰¾åˆ°å°æ‡‰çš„ a tag")
                else:
                    print(f"     {j}. âš ï¸ article å…ƒç´ ä¸­æ²’æœ‰æ‰¾åˆ° h4 tag")
                    
            except Exception as e:
                print(f"     âŒ è™•ç†æ–‡ç« å…ƒç´  {j} æ™‚å‡ºéŒ¯: {e}")
                continue
        
    except TimeoutException:
        print(f"âŒ æ•…äº‹é é¢è¼‰å…¥è¶…æ™‚: {story_info['url']}")
    except WebDriverException as e:
        print(f"âŒ WebDriver éŒ¯èª¤: {e}")
    except Exception as e:
        print(f"âŒ è™•ç†æ•…äº‹æ™‚å‡ºéŒ¯: {e}")
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return article_links

def get_final_content(article_info, driver):
    """æ­¥é©Ÿ 3: è·³è½‰åˆ°åŸå§‹ç¶²ç«™ä¸¦æŠ“å–å…§å®¹ã€åœ–ç‰‡å’Œæ™‚é–“"""
    MAX_RETRIES = 2
    TIMEOUT = 15
    
    for attempt in range(MAX_RETRIES):
        try:
            print(f"   å˜—è©¦ç¬¬ {attempt + 1} æ¬¡è¨ªå•...")
            
            # è¨­ç½®é¡µé¢åŠ è½½è¶…æ™‚
            driver.set_page_load_timeout(TIMEOUT)
            
            # ä½¿ç”¨ try-except å¤„ç†é¡µé¢åŠ è½½è¶…æ™‚
            try:
                driver.get(article_info['article_url'])
            except TimeoutException:
                print(f"   âš ï¸ é é¢åŠ è¼‰è¶…æ™‚ï¼Œä½†ç¹¼çºŒå˜—è©¦ç²å–å…§å®¹...")
                # å³ä½¿è¶…æ—¶ï¼Œä¹Ÿå°è¯•è·å–å·²åŠ è½½çš„å†…å®¹
            except WebDriverException as e:
                print(f"   âŒ WebDriver éŒ¯èª¤: {e}")
                if attempt < MAX_RETRIES - 1:
                    print(f"   ğŸ”„ {TIMEOUT//2} ç§’å¾Œé‡è©¦...")
                    time.sleep(TIMEOUT//2)
                    continue
                else:
                    return None
            
            # ç­‰å¾…ä¸€å°æ®µæ™‚é–“è®“é é¢æ¸²æŸ“
            time.sleep(random.randint(4, 6))
            
            # å–å¾—è·³è½‰å¾Œçš„çœŸå¯¦ç¶²å€
            try:
                skip_patterns = [
                    "https://www.gamereactor.cn/video",
                    "https://wantrich.chinatimes.com",
                    "https://taongafarm.site", 
                    "https://www.cmoney.tw",
                    "https://www.cw.com.tw",
                    "https://www.msn.com/",
                    "https://cn.wsj.com/",
                    "https://about.pts.org.tw/pr/latestnews",
                    "https://www.chinatimes.com",
                    # "https://newtalk.tw",
                    "https://sports.ltn.com.tw",
                    "https://video.ltn.com.tw",
                    "https://def.ltn.com.tw",
                    "https://www.upmedia.mg",
                    "http://www.aastocks.com",
                    "https://news.futunn.com",
                    "https://ec.ltn.com.tw/",
                    "https://health.ltn.com.tw",
                    "https://www.taiwannews",
                    "https://www.ftvnews.com.tw",
                    "https://tw.nextapple.com",
                    "https://talk.ltn.com.tw",
                    "https://www.mobile01.com/",
                    "https://www.worldjournal.com/"
                ]
                final_url = driver.current_url
                print(f"   æœ€çµ‚ç¶²å€: {final_url}")
                
                if final_url.startswith("https://www.google.com/sorry/index?continue=https://news.google.com/read"):
                    driver.refresh()
                    time.sleep(random.randint(2, 4))
                    final_url = driver.current_url
                elif any(final_url.startswith(pattern) for pattern in skip_patterns):
                    print(f"   â­ï¸  è·³éé€£çµ: {final_url}")
                    return None
                
            except WebDriverException:
                print(f"   âš ï¸ ç„¡æ³•ç²å–ç•¶å‰ URLï¼Œä½¿ç”¨åŸå§‹ URL")
                final_url = article_info['article_url']
                return None
            
            # å–å¾— HTML åŸå§‹ç¢¼ä¸¦äº¤çµ¦ BeautifulSoup
            try:
                html = driver.page_source
                soup = BeautifulSoup(html, "html.parser")
            except WebDriverException:
                print(f"   âŒ ç„¡æ³•ç²å–é é¢æºç¢¼")
                if attempt < MAX_RETRIES - 1:
                    print(f"   ğŸ”„ {TIMEOUT//2} ç§’å¾Œé‡è©¦...")
                    time.sleep(TIMEOUT//2)
                    continue
                else:
                    return None

            # æ¸…ç†å…§å®¹
            content_to_clean = None

            # ç¬¬ä¸€å„ªå…ˆï¼šå°‹æ‰¾ article æ¨™ç±¤
            article_tag = soup.find('article')
            if article_tag and article_info['media'] != 'Now æ–°è':
                content_to_clean = str(article_tag)
            elif soup.find('artical'):
                article_tag = soup.find('artical')
                content_to_clean = str(article_tag)
            else:
                # ç¬¬äºŒå„ªå…ˆï¼šå°‹æ‰¾ç‰¹å®š id çš„ div æ¨™ç±¤
                target_ids = [
                    'text ivu-mt', 'content-box', 'text', 'boxTitle', 
                    'news-detail-content', 'story', 'article-content__editor', 'article-body', 
                    'artical-content', 'article_text', 'newsText'
                ]
                
                div_by_id = None
                for target_id in target_ids:
                    try:
                        div_by_id = soup.find('div', id=target_id)
                        if div_by_id:
                            break
                    except Exception as e:
                        print(f"   âš ï¸ æœå°‹ ID '{target_id}' æ™‚å‡ºéŒ¯: {e}")
                        continue
                
                if div_by_id:
                    content_to_clean = str(div_by_id)
                else:
                    # ç¬¬ä¸‰å„ªå…ˆï¼šå°‹æ‰¾ç‰¹å®š class çš„ div æ¨™ç±¤
                    target_classes = ['articleBody clearfix', 'text boxTitle','text ivu-mt', 'paragraph', 'atoms', 
                                      'news-box-text border', 'newsLeading', 'text']

                    div_by_class = None
                    for target_class in target_classes:
                        try:
                            div_by_class = soup.find('div', class_=target_class)
                            if div_by_class:
                                break
                        except Exception as e:
                            print(f"   âš ï¸ æœå°‹ class '{target_class}' æ™‚å‡ºéŒ¯: {e}")
                            continue
                    
                    if div_by_class:
                        content_to_clean = str(div_by_class)
                    else:
                        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ body
                        if soup.body:
                            content_to_clean = str(soup.body)

            # å¦‚æœæœ‰æ‰¾åˆ°å…§å®¹ï¼Œé€²è¡Œæ¸…ç†
            if content_to_clean:
                try:
                    # é‡æ–°è§£ææ‰¾åˆ°çš„å…§å®¹
                    content_soup = BeautifulSoup(content_to_clean, "html.parser")
                    
                    # æ’é™¤ç‰¹å®šçš„ div æ¨™ç±¤
                    excluded_divs = content_soup.find_all('div', class_='paragraph moreArticle')
                    for div in excluded_divs:
                        div.decompose()
                    
                    # æ’é™¤ç‰¹å®šçš„ p æ¨™ç±¤
                    excluded_p_classes = [
                        'mb-module-gap read-more-vendor break-words leading-[1.4] text-px20 lg:text-px18 lg:leading-[1.8] text-batcave __web-inspector-hide-shortcut__',
                        'mb-module-gap read-more-editor break-words leading-[1.4] text-px20 lg:text-px18 lg:leading-[1.8] text-batcave'
                    ]
                    
                    for p_class in excluded_p_classes:
                        excluded_ps = content_soup.find_all('p', class_=p_class)
                        for p in excluded_ps:
                            p.decompose()
                    
                    # æœ€çµ‚æ¸…ç†
                    body_content = str(content_soup)
                    body_content = body_content.replace("\x00", "").replace("\r", "").replace("\n", "")
                    body_content = body_content.replace('"', '\\"')
                    
                except Exception as e:
                    print(f"   âŒ å…§å®¹æ¸…ç†æ™‚å‡ºéŒ¯: {e}")
                    body_content = ""
            else:
                body_content = ""
                print(f"   âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„å…§å®¹")
                
            # ç”Ÿæˆæ–‡ç«  ID
            article_id = str(uuid.uuid4())

            # æª¢æŸ¥æ˜¯å¦è¢«å°é–
            if ("æ‚¨çš„ç¶²è·¯å·²é­åˆ°åœæ­¢è¨ªå•æœ¬ç¶²ç«™çš„æ¬Šåˆ©ã€‚" in body_content or 
                "æˆ‘å€‘çš„ç³»çµ±åµæ¸¬åˆ°æ‚¨çš„é›»è…¦ç¶²è·¯é€å‡ºçš„æµé‡æœ‰ç•°å¸¸æƒ…æ³ã€‚" in body_content):
                print(f"   âš ï¸ æ–‡ç«  {article_id} è¢«å°é–ï¼Œç„¡æ³•è¨ªå•")
                return None

            return {
                "story_id": article_info['story_id'],
                "story_title": article_info['story_title'],
                "story_category": article_info['story_category'],
                "story_url": article_info['story_url'],
                "id": article_id,
                "article_index": article_info['article_index'],
                "article_title": article_info['article_title'],
                "google_news_url": article_info['article_url'],
                "final_url": final_url,
                "media": article_info.get('media', 'æœªçŸ¥ä¾†æº'),
                "content": body_content,
                "article_datetime": article_info.get('article_datetime', 'æœªçŸ¥æ™‚é–“'),
                "action_type": article_info.get('action_type', 'process'),
                "existing_story_data": article_info.get('existing_story_data')
            }
            
        except Exception as e:
            print(f"   âŒ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—: {e}")
            if attempt < MAX_RETRIES - 1:
                print(f"   ğŸ”„ {TIMEOUT//2} ç§’å¾Œé‡è©¦...")
                time.sleep(TIMEOUT//2)
            else:
                print(f"   ğŸ’€ å·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„è©²æ–‡ç« ")
    
    return None


def find_earliest_datetime(articles):
    """æ‰¾åˆ°æ–‡ç« åˆ—è¡¨ä¸­æœ€æ—©çš„æ™‚é–“"""
    valid_datetimes = []
    
    for article in articles:
        article_datetime = article.get('article_datetime', 'æœªçŸ¥æ™‚é–“')
        if article_datetime and article_datetime != 'æœªçŸ¥æ™‚é–“':
            try:
                # å˜—è©¦è§£æ datetime å­—ç¬¦ä¸²
                parsed_dt = parser.parse(article_datetime)
                valid_datetimes.append(parsed_dt)
            except (ValueError, TypeError) as e:
                print(f"âš ï¸ è§£ææ™‚é–“å¤±æ•—: {article_datetime}, éŒ¯èª¤: {e}")
                continue
    
    if valid_datetimes:
        # æ‰¾åˆ°æœ€æ—©çš„æ™‚é–“ä¸¦æ ¼å¼åŒ–
        earliest_dt = min(valid_datetimes)
        return earliest_dt
    else:
        # å¦‚æœæ²’æœ‰æœ‰æ•ˆçš„æ™‚é–“ï¼Œä½¿ç”¨ç•¶å‰æ™‚é–“
        return datetime.now()


def group_articles_by_story_and_time(processed_articles, time_window_days=2):
    """
    æ ¹æ“šæ•…äº‹åˆ†çµ„ï¼Œç„¶å¾Œåœ¨æ¯å€‹æ•…äº‹å…§æŒ‰æ™‚é–“å°‡æ–‡ç« åˆ†çµ„
    processed_articles: å¾ get_final_content è™•ç†å¾Œçš„æ–‡ç« åˆ—è¡¨
    time_window_days: æ™‚é–“çª—å£å¤©æ•¸ï¼ˆçœŸæ­£çš„æ¯Nå¤©åˆ†çµ„ï¼‰
    """
    print(f"\n=== é–‹å§‹åŸºæ–¼æ•…äº‹å’Œæ™‚é–“åˆ†çµ„æ–‡ç« ï¼ˆæ¯å€‹æ•…äº‹å…§æŒ‰{time_window_days}å¤©åˆ†çµ„ï¼‰===")
    
    # æŒ‰æ•…äº‹IDåˆ†çµ„
    story_grouped = defaultdict(list)
    for article in processed_articles:
        story_id = article["story_id"]
        story_grouped[story_id].append(article)
    
    all_final_stories = []
    
    for story_id, articles in story_grouped.items():
        if not articles:
            continue
            
        # ç²å–æ•…äº‹åŸºæœ¬ä¿¡æ¯ï¼ˆå¾ç¬¬ä¸€ç¯‡æ–‡ç« ï¼‰
        first_article = articles[0]
        story_title = first_article["article_title"]
        story_url = first_article["story_url"]
        story_category = first_article["story_category"]
        
        print(f"\nğŸ” è™•ç†æ•…äº‹: {story_title}")
        print(f"   ğŸ†” Story ID: {story_id}")
        print(f"   ğŸ“Š åŒ…å« {len(articles)} ç¯‡æ–‡ç« ")
        
        # è§£ææ‰€æœ‰æ–‡ç« çš„æ™‚é–“
        articles_with_time = []
        for article in articles:
            article_datetime = article.get('article_datetime', 'æœªçŸ¥æ™‚é–“')
            if article_datetime and article_datetime != 'æœªçŸ¥æ™‚é–“':
                try:
                    parsed_dt = parser.parse(article_datetime)
                    articles_with_time.append({
                        'article': article,
                        'datetime': parsed_dt
                    })
                except (ValueError, TypeError) as e:
                    print(f"âš ï¸ è§£ææ™‚é–“å¤±æ•—: {article_datetime}, ä½¿ç”¨ç•¶å‰æ™‚é–“")
                    articles_with_time.append({
                        'article': article,
                        'datetime': datetime.now()
                    })
            else:
                # æ²’æœ‰æ™‚é–“çš„æ–‡ç« ä½¿ç”¨ç•¶å‰æ™‚é–“
                articles_with_time.append({
                    'article': article,
                    'datetime': datetime.now()
                })
        
        # æŒ‰æ™‚é–“æ’åº
        articles_with_time.sort(key=lambda x: x['datetime'])
        
        # åœ¨åŒä¸€æ•…äº‹å…§é€²è¡Œæ™‚é–“åˆ†çµ„ - ä¿®æ­£çš„é‚è¼¯
        time_groups = []
        current_group = []
        current_group_start_time = None
        current_group_end_time = None
        
        for item in articles_with_time:
            article_time = item['datetime']
            
            if current_group_start_time is None:
                # ç¬¬ä¸€ç¯‡æ–‡ç« ï¼Œé–‹å§‹ç¬¬ä¸€çµ„
                current_group_start_time = article_time
                current_group_end_time = article_time + timedelta(days=time_window_days)
                current_group.append(item)
                print(f"      ğŸ é–‹å§‹æ–°çµ„: {current_group_start_time.strftime('%Y/%m/%d %H:%M')} - {current_group_end_time.strftime('%Y/%m/%d %H:%M')}")
            else:
                # æª¢æŸ¥æ˜¯å¦åœ¨ç•¶å‰çµ„çš„æ™‚é–“çª—å£å…§
                if article_time < current_group_end_time:
                    # åœ¨åŒä¸€çµ„å…§
                    current_group.append(item)
                    print(f"         âœ… åŠ å…¥ç•¶å‰çµ„: {article_time.strftime('%Y/%m/%d %H:%M')}")
                else:
                    # è¶…å‡ºæ™‚é–“çª—å£ï¼Œé–‹å§‹æ–°çš„ä¸€çµ„
                    if current_group:
                        time_groups.append(current_group)
                        print(f"      ğŸ“¦ å®Œæˆçµ„åˆ¥ï¼ŒåŒ…å« {len(current_group)} ç¯‡æ–‡ç« ")
                    
                    # é–‹å§‹æ–°çµ„
                    current_group = [item]
                    current_group_start_time = article_time
                    current_group_end_time = article_time + timedelta(days=time_window_days)
                    print(f"      ğŸ é–‹å§‹æ–°çµ„: {current_group_start_time.strftime('%Y/%m/%d %H:%M')} - {current_group_end_time.strftime('%Y/%m/%d %H:%M')}")
        
        # æ·»åŠ æœ€å¾Œä¸€çµ„
        if current_group:
            time_groups.append(current_group)
            print(f"      ğŸ“¦ å®Œæˆæœ€å¾Œçµ„åˆ¥ï¼ŒåŒ…å« {len(current_group)} ç¯‡æ–‡ç« ")
        
        print(f"   ğŸ“Š åœ¨æ•…äº‹å…§åˆ†æˆ {len(time_groups)} å€‹æ™‚é–“çµ„")
        
        # ç‚ºæ¯å€‹æ™‚é–“çµ„å‰µå»ºæœ€çµ‚çš„æ•…äº‹æ•¸æ“š
        for group_idx, group in enumerate(time_groups):
            # æ‰¾åˆ°çµ„å…§æœ€æ—©å’Œæœ€æ™šçš„æ™‚é–“
            earliest_time = min(item['datetime'] for item in group)
            latest_time = max(item['datetime'] for item in group)
            
            crawl_date = earliest_time.strftime("%Y/%m/%d %H:%M")
            
            # è¨ˆç®—å¯¦éš›çš„æ™‚é–“ç¯„åœ
            if earliest_time.date() == latest_time.date():
                time_range = earliest_time.strftime('%Y/%m/%d')
            else:
                time_range = f"{earliest_time.strftime('%Y/%m/%d')} - {latest_time.strftime('%Y/%m/%d')}"
            
            # å¦‚æœä¸€å€‹æ•…äº‹è¢«åˆ†æˆå¤šå€‹æ™‚é–“çµ„ï¼Œç‚ºæ¯çµ„ç”Ÿæˆæ–°çš„æ•…äº‹ID
            if len(time_groups) > 1:
                base_story_id = story_id[:-2]  # ç§»é™¤æœ€å¾Œå…©ç¢¼
                final_story_id = f"{base_story_id}{group_idx + 1:02d}"  # æ·»åŠ å…©ä½æ•¸çš„çµ„ç´¢å¼•
                final_story_title = f"{story_title} (ç¬¬{group_idx + 1}çµ„)"
            else:
                final_story_id = story_id
                final_story_title = story_title
            
            # æº–å‚™æ–‡ç« åˆ—è¡¨
            grouped_articles = []
            for article_idx, item in enumerate(group, 1):
                article = item['article']
                grouped_articles.append({
                    "article_id": article["id"],
                    "article_title": article["article_title"],
                    "article_index": article_idx,  # é‡æ–°ç·¨è™Ÿ
                    "google_news_url": article["google_news_url"],
                    "article_url": article["final_url"],
                    "media": article["media"],
                    "content": article["content"],
                    "original_datetime": article.get("article_datetime", "æœªçŸ¥æ™‚é–“")
                })
            
            story_data = {
                "story_id": final_story_id,
                "story_title": final_story_title,
                "story_url": story_url,
                "crawl_date": crawl_date,
                "time_range": time_range,
                "category": story_category,
                "articles": grouped_articles
            }
            
            all_final_stories.append(story_data)
            
            # è¨ˆç®—å¯¦éš›å¤©æ•¸è·¨åº¦
            actual_days = (latest_time.date() - earliest_time.date()).days + 1
            
            if len(time_groups) > 1:
                print(f"   ğŸ“° æ™‚é–“çµ„ {group_idx + 1}: {time_range} (å¯¦éš›è·¨åº¦: {actual_days}å¤©)")
            else:
                print(f"   ğŸ“° å®Œæ•´æ•…äº‹: {time_range} (å¯¦éš›è·¨åº¦: {actual_days}å¤©)")
            print(f"      ğŸ†” æœ€çµ‚ Story ID: {final_story_id}")
            print(f"      ğŸ“… Crawl Date: {crawl_date}")
            print(f"      ğŸ“„ æ–‡ç« æ•¸: {len(grouped_articles)} ç¯‡")
            
            # é©—è­‰æ™‚é–“çª—å£
            if actual_days > time_window_days:
                print(f"      âš ï¸  è­¦å‘Š: å¯¦éš›è·¨åº¦ ({actual_days}å¤©) è¶…éè¨­å®šçª—å£ ({time_window_days}å¤©)")
    
    print(f"\nâœ… ç¸½å…±è™•ç†å®Œæˆ {len(all_final_stories)} å€‹æœ€çµ‚æ•…äº‹")
    return all_final_stories

def process_news_pipeline(main_url, category):
    """
    å®Œæ•´çš„æ–°èè™•ç†ç®¡é“
    """
    print(f"ğŸš€ é–‹å§‹è™•ç† {category} åˆ†é¡çš„æ–°è...")
    
    # æ­¥é©Ÿ1: ç²å–æ‰€æœ‰æ•…äº‹é€£çµ
    story_links = get_main_story_links(main_url, category)
    if not story_links:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ•…äº‹é€£çµ")
        return []
    
    # æ­¥é©Ÿ2: è™•ç†æ¯å€‹æ•…äº‹ï¼Œç²å–æ‰€æœ‰æ–‡ç« é€£çµ
    all_article_links = []
    for story_info in story_links[:4 ]:
        article_links = get_article_links_from_story(story_info)
        all_article_links.extend(article_links)
    
    if not all_article_links:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« é€£çµ")
        return []
    
    print(f"\nğŸ“Š ç¸½å…±æ”¶é›†åˆ° {len(all_article_links)} ç¯‡æ–‡ç« å¾…è™•ç†")
    
    # æ­¥é©Ÿ3: ç²å–æ¯ç¯‡æ–‡ç« çš„å®Œæ•´å…§å®¹
    final_articles = []
    driver = create_robust_driver(headless=False)  # ä½¿ç”¨æœ‰è¦–çª—æ¨¡å¼ä»¥ä¾¿æ–¼èª¿è©¦
    
    initialize_driver_with_cookies(driver)
    
    try:
        for i, article_info in enumerate(all_article_links, 1):
            print(f"\nğŸ”„ è™•ç†æ–‡ç«  {i}/{len(all_article_links)}: {article_info['article_title']}")
            
            article_content = get_final_content(article_info, driver)
            if article_content:
                final_articles.append(article_content)
                print(f"   âœ… æˆåŠŸç²å–å…§å®¹")
            else:
                print(f"   âŒ ç„¡æ³•ç²å–å…§å®¹")
            
            # éš¨æ©Ÿå»¶é²
            time.sleep(random.randint(2, 4))
            
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    # æ­¥é©Ÿ4: æŒ‰æ•…äº‹å’Œæ™‚é–“åˆ†çµ„
    final_stories = group_articles_by_story_and_time(final_articles, time_window_days=3)
    
    return final_stories

def initialize_driver_with_cookies(driver):
    """åˆå§‹åŒ– WebDriver ä¸¦è¼‰å…¥ cookies"""
    try:
        # å…ˆè¨ªå• Google News ä¸»é 
        driver.get("https://news.google.com/")
        time.sleep(2)
        
        # å˜—è©¦è¼‰å…¥ cookies
        try:
            with open("cookies.json", "r", encoding="utf-8") as f:
                cookies = json.load(f)
            
            for cookie in cookies:
                if 'sameSite' in cookie:
                    cookie.pop('sameSite')
                try:
                    driver.add_cookie(cookie)
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•æ·»åŠ  cookie: {e}")
            
            print("âœ… Cookies è¼‰å…¥å®Œæˆ")
            
        except FileNotFoundError:
            print("âš ï¸ cookies.json æª”æ¡ˆä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜èªè¨­ç½®")
    
    except Exception as e:
        print(f"âš ï¸ åˆå§‹åŒ– WebDriver cookies æ™‚å‡ºéŒ¯: {e}")

def save_stories_to_json(stories, filename):
    """
    å°‡æ•…äº‹æ•¸æ“šä¿å­˜åˆ°JSONæ–‡ä»¶
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(stories, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•¸æ“šå·²ä¿å­˜åˆ° {filename}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶æ™‚å‡ºéŒ¯: {e}")
        return False

def save_stories_to_supabase(stories):
    """
    å°‡æ•…äº‹æ•¸æ“šä¿å­˜åˆ°Supabaseæ•¸æ“šåº«
    é€™å€‹å‡½æ•¸éœ€è¦æ ¹æ“šä½ çš„Supabaseé…ç½®ä¾†å¯¦ç¾
    """
    try:
        # é€™è£¡éœ€è¦å¯¦ç¾ä½ çš„Supabaseä¿å­˜é‚è¼¯
        # ä¾‹å¦‚ï¼š
        # for story in stories:
        #     supabase.table('stories').insert(story).execute()
        
        print(f"âœ… å·²å°‡ {len(stories)} å€‹æ•…äº‹ä¿å­˜åˆ°Supabase")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ°Supabaseæ™‚å‡ºéŒ¯: {e}")
        return False

def main():
    """
    ä¸»å‡½æ•¸ - æ–°èçˆ¬èŸ²çš„å…¥å£é»
    """
    print("="*80)
    print("ğŸŒŸ Google News çˆ¬èŸ²ç¨‹åºå•Ÿå‹•")
    print("="*80)
    
    # é…ç½®éœ€è¦è™•ç†çš„æ–°èåˆ†é¡
    news_categories = {
        "Politics": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFZ4ZERBU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Taiwan News": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRFptTXpJU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "International News": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Science & Technology": "https://news.google.com/topics/CAAqLAgKIiZDQkFTRmdvSkwyMHZNR1ptZHpWbUVnVjZhQzFVVnhvQ1ZGY29BQVAB?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Lifestyle & Consumer": "https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSkwyMHZNREUwWkhONEVnVjZhQzFVVnlnQVAB?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Sports": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Entertainment": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Business & Finance": "https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0JYcG9MVlJYR2dKVVZ5Z0FQAQ?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "Health & Wellness": "https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0JYcG9MVlJYS0FBUAE?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant"
    }

    
    # å¯ä»¥é¸æ“‡è™•ç†ç‰¹å®šåˆ†é¡æˆ–å…¨éƒ¨åˆ†é¡
    selected_categories = ["Politics"]#, "Taiwan News", "International News"]  # å¯ä»¥ä¿®æ”¹é€™è£¡ä¾†é¸æ“‡è¦è™•ç†çš„åˆ†é¡
    selected_categories = list(news_categories.keys())  # è™•ç†æ‰€æœ‰åˆ†é¡
    
    all_final_stories = []
    start_time = time.time()
    
    try:
        for category in selected_categories:
            if category not in news_categories:
                print(f"âš ï¸ æœªçŸ¥çš„åˆ†é¡: {category}")
                continue
                
            category_start_time = time.time()
            print(f"\n{'='*60}")
            print(f"ğŸ¯ é–‹å§‹è™•ç†åˆ†é¡: {category}")
            print(f"{'='*60}")
            
            # è™•ç†è©²åˆ†é¡çš„æ–°è
            category_stories = process_news_pipeline(news_categories[category], category)
            
            if category_stories:
                all_final_stories.extend(category_stories)
                category_end_time = time.time()
                category_duration = category_end_time - category_start_time
                
                print(f"\nâœ… {category} åˆ†é¡è™•ç†å®Œæˆ!")
                print(f"   ğŸ“Š ç²å¾— {len(category_stories)} å€‹æ•…äº‹")
                print(f"   â±ï¸  è€—æ™‚: {category_duration:.2f} ç§’")
            else:
                print(f"\nâŒ {category} åˆ†é¡è™•ç†å¤±æ•—ï¼Œæ²’æœ‰ç²å¾—ä»»ä½•æ•…äº‹")
            
            # åˆ†é¡ä¹‹é–“çš„å»¶é²
            if category != selected_categories[-1]:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹åˆ†é¡
                print(f"\nâ³ ç­‰å¾… 30 ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹åˆ†é¡...")
                time.sleep(30)
        
        # è™•ç†å®Œæˆå¾Œçš„çµ±è¨ˆ
        total_end_time = time.time()
        total_duration = total_end_time - start_time
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ æ‰€æœ‰åˆ†é¡è™•ç†å®Œæˆ!")
        print(f"{'='*80}")
        print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"   ğŸ·ï¸  è™•ç†åˆ†é¡æ•¸: {len(selected_categories)}")
        print(f"   ğŸ“° ç¸½æ•…äº‹æ•¸: {len(all_final_stories)}")
        
        # çµ±è¨ˆæ¯å€‹åˆ†é¡çš„æ•…äº‹æ•¸
        category_counts = {}
        total_articles = 0
        for story in all_final_stories:
            category = story['category']
            category_counts[category] = category_counts.get(category, 0) + 1
            total_articles += len(story['articles'])
        
        for category, count in category_counts.items():
            print(f"   ğŸ“‚ {category}: {count} å€‹æ•…äº‹")
        
        print(f"   ğŸ“„ ç¸½æ–‡ç« æ•¸: {total_articles}")
        print(f"   â±ï¸  ç¸½è€—æ™‚: {total_duration:.2f} ç§’ ({total_duration/60:.1f} åˆ†é˜)")
        
        # ä¿å­˜æ•¸æ“š
        if all_final_stories:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            json_filename = f"json/google_news_stories_{timestamp}.json"
            if save_stories_to_json(all_final_stories, json_filename):
                print(f"ğŸ“ æœ¬åœ°JSONæ–‡ä»¶: {json_filename}")
            
            # ä¿å­˜åˆ°æ•¸æ“šåº«ï¼ˆå¦‚æœéœ€è¦ï¼‰
            try:
                # save_stories_to_supabase(all_final_stories)
                print("ğŸ’¾ æ•¸æ“šåº«ä¿å­˜: å·²è·³é (è«‹æ ¹æ“šéœ€è¦å¯¦ç¾)")
            except Exception as e:
                print(f"âŒ æ•¸æ“šåº«ä¿å­˜å¤±æ•—: {e}")
            
        else:
            print("âš ï¸ æ²’æœ‰ç²å¾—ä»»ä½•æ•…äº‹æ•¸æ“š")
    
    except KeyboardInterrupt:
        print(f"\nâš¡ ç¨‹åºè¢«ç”¨æˆ¶ä¸­æ–·")
        if all_final_stories:
            # å³ä½¿è¢«ä¸­æ–·ï¼Œä¹Ÿä¿å­˜å·²ç²å–çš„æ•¸æ“š
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_filename = f"json/google_news_stories_interrupted_{timestamp}.json"
            save_stories_to_json(all_final_stories, json_filename)
            print(f"ğŸ“ å·²ä¿å­˜ä¸­æ–·å‰çš„æ•¸æ“š: {json_filename}")
    
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºåŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        print(f"ğŸ“‹ éŒ¯èª¤è©³æƒ…:\n{traceback.format_exc()}")
    
    finally:
        print(f"\n{'='*80}")
        print(f"ğŸ‘‹ Google News çˆ¬èŸ²ç¨‹åºçµæŸ")
        print(f"{'='*80}")

if __name__ == "__main__":
    main()