import json
import os
from datetime import datetime, timedelta
from supabase import create_client, Client
from typing import List, Dict, Any, Tuple

def setup_supabase() -> Client:
    """è¨­ç½® Supabase å®¢æˆ¶ç«¯"""
    # è«‹æ›¿æ›ç‚ºæ‚¨çš„ Supabase URL å’Œ API Key
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    return supabase

def load_json_data(file_path: str) -> List[Dict[Any, Any]]:
    """è¼‰å…¥ JSON æª”æ¡ˆ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
        return data
    except FileNotFoundError:
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {file_path}")
        return []
    except json.JSONDecodeError as e:
        print(f"éŒ¯èª¤: JSON è§£æå¤±æ•— - {e}")
        return []

def parse_crawl_date(date_str: str) -> datetime:
    """è§£æ crawl_date å­—ä¸²ç‚º datetime ç‰©ä»¶"""
    try:
        # å‡è¨­æ ¼å¼ç‚º "2025/08/15 00:43"
        return datetime.strptime(date_str, "%Y/%m/%d %H:%M")
    except ValueError:
        try:
            # å˜—è©¦å…¶ä»–å¯èƒ½æ ¼å¼
            return datetime.strptime(date_str, "%Y-%m-%d %H:%M")
        except ValueError:
            print(f"è­¦å‘Š: ç„¡æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
            return datetime.now()

def check_story_exists(supabase: Client, story_url: str) -> Tuple[bool, str]:
    """æª¢æŸ¥ story_url æ˜¯å¦å·²å­˜åœ¨ï¼Œå›å‚³ (æ˜¯å¦å­˜åœ¨, æœ€æ–° crawl_date)"""
    try:
        response = (
            supabase.table("stories")
            .select("crawl_date", "story_id")
            .eq("story_url", story_url)
            .order("crawl_date", desc=True)   # å–æœ€æ–°ä¸€ç­†
            .limit(1)
            .execute()
        )
        if response.data:
            return True, response.data[0]["crawl_date"], response.data[0]["story_id"]
        return False, "", ""
    except Exception as e:
        print(f"æª¢æŸ¥æ•…äº‹æ˜¯å¦å­˜åœ¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False, ""

def should_update_story(existing_crawl_date: str, new_crawl_date: str) -> bool:
    """åˆ¤æ–·æ˜¯å¦æ‡‰è©²æ›´æ–°æ•…äº‹ï¼ˆæ–°çš„ crawl_date æ˜¯å¦æ¯”æ—¢æœ‰çš„æ™š 3 å¤©ä»¥ä¸Šï¼‰"""
    try:
        existing_date = parse_crawl_date(existing_crawl_date)
        new_date = parse_crawl_date(new_crawl_date)
        
        # è¨ˆç®—æ—¥æœŸå·®ç•°
        date_diff = new_date - existing_date
        
        # å¦‚æœæ–°æ—¥æœŸæ¯”æ—¢æœ‰æ—¥æœŸæ™š 3 å¤©ä»¥ä¸Šï¼Œå‰‡æ›´æ–°
        return date_diff.days >= 3
    except Exception as e:
        print(f"æ¯”è¼ƒæ—¥æœŸæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def upload_stories(supabase: Client, news_data: List[Dict[Any, Any]]) -> Tuple[bool, List[str]]:
    """ä¸Šå‚³æ•…äº‹è³‡æ–™åˆ° stories è¡¨ï¼Œå›å‚³ (æ˜¯å¦æˆåŠŸ, å·²ä¸Šå‚³çš„story_ids)"""
    stories_to_upload = []
    uploaded_story_ids = []
    skipped_count = 0
    updated_count = 0
    new_count = 0
    
    print("æ­£åœ¨æª¢æŸ¥æ•…äº‹è³‡æ–™...")
    
    for item in news_data:
        story_url = item.get("story_url")
        new_crawl_date = item.get("crawl_date")
        story_id = item.get("story_id")
        
        # æª¢æŸ¥ story_url æ˜¯å¦å·²å­˜åœ¨
        exists, existing_crawl_date, story_idSU = check_story_exists(supabase, story_url)

        if not exists or (story_idSU==story_id):
            # æ–°æ•…äº‹ï¼Œç›´æ¥åŠ å…¥ä¸Šå‚³åˆ—è¡¨
            story_record = {
                "story_id": story_id,
                "story_title": item.get("story_title"),
                "story_url": story_url,
                "crawl_date": new_crawl_date,
                "category": item.get("category")
            }
            stories_to_upload.append(story_record)
            uploaded_story_ids.append(story_id)
            new_count += 1
            print(f"æ–°æ•…äº‹: {item.get('story_title')[:50]}...")
    
    print(f"\nè™•ç†çµæœ:")
    print(f"  æ–°å¢: {new_count} ç­†")
    print(f"  æ›´æ–°: {updated_count} ç­†")
    print(f"  è·³é: {skipped_count} ç­†")
    print(f"  ç¸½å…±éœ€ä¸Šå‚³: {len(stories_to_upload)} ç­†")
    
    if not stories_to_upload:
        print("æ²’æœ‰éœ€è¦ä¸Šå‚³çš„æ•…äº‹è³‡æ–™")
        return True, []
    
    try:
        # ä½¿ç”¨ upsert é¿å…é‡è¤‡æ’å…¥
        response = supabase.table("stories").upsert(stories_to_upload).execute()
        print(f"æˆåŠŸä¸Šå‚³ {len(stories_to_upload)} ç­†æ•…äº‹è³‡æ–™åˆ° stories è¡¨")
        return True, uploaded_story_ids
    except Exception as e:
        print(f"ä¸Šå‚³ stories è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False, []

def upload_articles(supabase: Client, news_data: List[Dict[Any, Any]], uploaded_story_ids: List[str]) -> bool:
    """ä¸Šå‚³æ–‡ç« è³‡æ–™åˆ° cleaned_news è¡¨ï¼Œåªä¸Šå‚³å·²æˆåŠŸä¸Šå‚³çš„æ•…äº‹å°æ‡‰çš„æ–‡ç« ï¼Œé¿å…é‡è¤‡"""
    articles_data = []
    
    for item in news_data:
        story_id = item.get("story_id")
        
        # åªè™•ç†å·²æˆåŠŸä¸Šå‚³çš„æ•…äº‹
        if story_id not in uploaded_story_ids:
            continue
            
        articles = item.get("articles", [])
        
        for article in articles:
            article_url = article.get("article_url")

            # ğŸ” æª¢æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒ article_url
            try:
                existing = (
                    supabase.table("cleaned_news")
                    .select("article_id")
                    .eq("article_url", article_url)
                    .limit(1)
                    .execute()
                )
                if existing.data:
                    print(f"è·³éæ–‡ç« ï¼ˆå·²å­˜åœ¨ï¼‰: {article.get('article_title')[:50]}...")
                    continue
            except Exception as e:
                print(f"æª¢æŸ¥æ–‡ç« æ˜¯å¦å­˜åœ¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue

            article_record = {
                "article_id": article.get("article_id"),
                "article_title": article.get("article_title"),
                "article_url": article_url,
                "media": article.get("media"),
                "content": article.get("content"),
                "story_id": story_id
            }
            articles_data.append(article_record)
    
    if not articles_data:
        print("æ²’æœ‰éœ€è¦ä¸Šå‚³çš„æ–‡ç« è³‡æ–™")
        return True
    
    try:
        # åˆ†æ‰¹ä¸Šå‚³ä»¥é¿å…å–®æ¬¡è«‹æ±‚éå¤§
        batch_size = 100
        total_articles = len(articles_data)
        
        for i in range(0, total_articles, batch_size):
            batch = articles_data[i:i + batch_size]
            supabase.table("cleaned_news").upsert(batch).execute()
            print(f"å·²ä¸Šå‚³ {min(i + batch_size, total_articles)}/{total_articles} ç­†æ–‡ç« è³‡æ–™")
        
        print(f"æˆåŠŸä¸Šå‚³ {total_articles} ç­†æ–‡ç« è³‡æ–™åˆ° cleaned_news è¡¨")
        return True
    except Exception as e:
        print(f"ä¸Šå‚³ cleaned_news è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    # JSON æª”æ¡ˆè·¯å¾‘
    json_file_path = "json/processed/cleaned_final_news.json"  # è«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘
    
    print("é–‹å§‹è™•ç†æ–°èè³‡æ–™ä¸Šå‚³...")
    
    # 1. è¨­ç½® Supabase å®¢æˆ¶ç«¯
    print("è¨­ç½® Supabase é€£æ¥...")
    supabase = setup_supabase()
    
    # 2. è¼‰å…¥ JSON è³‡æ–™
    print(f"è¼‰å…¥ JSON æª”æ¡ˆ: {json_file_path}")
    news_data = load_json_data(json_file_path)
    
    if not news_data:
        print("æ²’æœ‰è³‡æ–™å¯ä¸Šå‚³ï¼Œç¨‹å¼çµæŸ")
        return
    
    print(f"è¼‰å…¥äº† {len(news_data)} ç­†æ•…äº‹è³‡æ–™")
    
    # 3. ä¸Šå‚³ stories è³‡æ–™
    print("\né–‹å§‹ä¸Šå‚³ stories è³‡æ–™...")
    stories_success, uploaded_story_ids = upload_stories(supabase, news_data)
    
    if not stories_success:
        print("stories è³‡æ–™ä¸Šå‚³å¤±æ•—ï¼Œåœæ­¢ç¨‹å¼")
        return
    
    if not uploaded_story_ids:
        print("æ²’æœ‰æ–°çš„æˆ–éœ€è¦æ›´æ–°çš„æ•…äº‹ï¼Œç¨‹å¼çµæŸ")
        return
    
    # 4. ä¸Šå‚³ articles è³‡æ–™
    print("\né–‹å§‹ä¸Šå‚³ articles è³‡æ–™...")
    articles_success = upload_articles(supabase, news_data, uploaded_story_ids)
    
    if articles_success:
        print("\nâœ… æ‰€æœ‰è³‡æ–™ä¸Šå‚³å®Œæˆï¼")
    else:
        print("\nâŒ articles è³‡æ–™ä¸Šå‚³å¤±æ•—")

if __name__ == "__main__":
    # åŸ·è¡Œä¸»ç¨‹å¼
    main()